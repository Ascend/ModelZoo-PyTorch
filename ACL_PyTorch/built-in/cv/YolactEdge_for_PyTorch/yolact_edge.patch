From a2560ae1e4367706c09763ca3b879959df69ecdc Mon Sep 17 00:00:00 2001
From: sunyuqi3
Date: Fri, 13 Jan 2023 14:56:49 +0800
Subject: [PATCH] patch

---
 yolact_edge/yolact.py | 42 +++++++++++++++++++++++++-----------------
 1 file changed, 25 insertions(+), 17 deletions(-)

diff --git a/yolact_edge/yolact.py b/yolact_edge/yolact.py
index b6efdf7..2de113e 100644
--- a/yolact_edge/yolact.py
+++ b/yolact_edge/yolact.py
@@ -33,10 +33,11 @@ except:
 
 # This is required for Pytorch 1.0.1 on Windows to initialize Cuda on some driver versions.
 # See the bug report here: https://github.com/pytorch/pytorch/issues/17108
-torch.cuda.current_device()
+# torch.cuda.current_device()
 
 # As of March 10, 2019, Pytorch DataParallel still doesn't support JIT Script Modules
-use_jit = False if use_torch2trt else torch.cuda.device_count() <= 1
+# use_jit = False if use_torch2trt else torch.cuda.device_count() <= 1
+use_jit = False
 
 ScriptModuleWrapper = torch.jit.ScriptModule if use_jit else nn.Module
 script_method_wrapper = torch.jit.script_method if use_jit else lambda fn, _rcn=None: fn
@@ -1089,6 +1090,9 @@ class Yolact(nn.Module):
     def __init__(self, training=True):
         super().__init__()
 
+        self.extras = {"backbone": "full", "interrupt": False,
+                       "moving_statistics": {"aligned_feats": []}}
+
         self.backbone = construct_backbone(cfg.backbone)
 
         self.training = training
@@ -1608,19 +1612,19 @@ class Yolact(nn.Module):
         x = torch.ones((1, lateral_channels * 2, 69, 69)).cuda()
         self.trt_load_if("flow_net", trt_fn, [x], int8_mode, parent=self.flow_net, batch_size=batch_size)
 
-    def forward(self, x, extras=None):
+    def forward(self, x):
         """ The input should be of size [batch_size, 3, img_h, img_w] """
 
         if cfg.flow.train_flow:
-            return self.forward_flow(extras)
+            return self.forward_flow(self.extras)
 
         outs_wrapper = {}
 
         with timer.env('backbone'):
-            if cfg.flow is None or extras is None or extras["backbone"] == "full":
+            if cfg.flow is None or self.extras is None or self.extras["backbone"] == "full":
                 outs = self.backbone(x)
 
-            elif extras is not None and extras["backbone"] == "partial":
+            elif self.extras is not None and self.extras["backbone"] == "partial":
                 if hasattr(self, 'partial_backbone'):
                     outs = self.partial_backbone(x)
                 else:
@@ -1631,22 +1635,22 @@ class Yolact(nn.Module):
 
         if cfg.flow is not None:
             with timer.env('fpn'):
-                assert type(extras) == dict
-                if extras["backbone"] == "full":
+                assert type(self.extras) == dict
+                if self.extras["backbone"] == "full":
                     outs = [outs[i] for i in cfg.backbone.selected_layers]
                     outs_fpn_phase_1_wrapper = self.fpn_phase_1(*outs)
                     outs_phase_1, lats_phase_1 = outs_fpn_phase_1_wrapper[:len(outs)], outs_fpn_phase_1_wrapper[len(outs):]
                     lateral = lats_phase_1[0].detach()
-                    moving_statistics = extras["moving_statistics"]
+                    moving_statistics = self.extras["moving_statistics"]
 
-                    if extras.get("keep_statistics", False):
+                    if self.extras.get("keep_statistics", False):
                         outs_wrapper["feats"] = [out.detach() for out in outs_phase_1]
                         outs_wrapper["lateral"] = lateral
 
                     outs_wrapper["outs_phase_1"] = [out.detach() for out in outs_phase_1]
                 else:
-                    assert extras["moving_statistics"] is not None
-                    moving_statistics = extras["moving_statistics"]
+                    assert self.extras["moving_statistics"] is not None
+                    moving_statistics = self.extras["moving_statistics"]
                     outs_phase_1 = moving_statistics["feats"].copy()
 
                     if cfg.flow.warp_mode != 'take':
@@ -1699,7 +1703,7 @@ class Yolact(nn.Module):
                     outs_wrapper["outs_phase_1"] = outs_phase_1.copy()
 
                 outs = self.fpn_phase_2(*outs_phase_1)
-                if extras["backbone"] == "partial":
+                if self.extras["backbone"] == "partial":
                     outs_wrapper["outs_phase_2"] = [out for out in outs]
                 else:
                     outs_wrapper["outs_phase_2"] = [out.detach() for out in outs]
@@ -1709,7 +1713,7 @@ class Yolact(nn.Module):
                 outs = [outs[i] for i in cfg.backbone.selected_layers]
                 outs = self.fpn(outs)
 
-        if extras is not None and extras.get("interrupt", None):
+        if self.extras is not None and self.extras.get("interrupt", None):
             return outs_wrapper
 
         proto_out = None
@@ -1740,6 +1744,9 @@ class Yolact(nn.Module):
                     bias_shape[-1] = 1
                     proto_out = torch.cat([proto_out, torch.ones(*bias_shape)], -1)
 
+            return outs, proto_out
+        
+    def postprocess(self, outs, proto_out):
         with timer.env('pred_heads'):
             pred_outs = { 'loc': [], 'conf': [], 'mask': [], 'priors': [] }
 
@@ -1779,7 +1786,7 @@ class Yolact(nn.Module):
             if cfg.use_semantic_segmentation_loss:
                 pred_outs['segm'] = self.semantic_seg_conv(outs[0])
 
-            outs_wrapper["pred_outs"] = pred_outs
+            # outs_wrapper["pred_outs"] = pred_outs
         else:
             if cfg.use_sigmoid_focal_loss:
                 # Note: even though conf[0] exists, this mode doesn't train it so don't use it
@@ -1792,8 +1799,9 @@ class Yolact(nn.Module):
             else:
                 pred_outs['conf'] = F.softmax(pred_outs['conf'], -1)
 
-            outs_wrapper["pred_outs"] = self.detect(pred_outs)
-        return outs_wrapper
+            # outs_wrapper["pred_outs"] = self.detect(pred_outs)
+            pred_outs = self.detect(pred_outs)
+        return pred_outs
 
 
 # Some testing code
-- 
2.39.0.windows.2

