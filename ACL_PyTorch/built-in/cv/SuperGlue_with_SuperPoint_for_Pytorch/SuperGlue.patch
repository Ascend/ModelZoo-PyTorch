Binary files SuperGluePretrainedNetwork/.git/index and SuperGluePretrainedNetwork_fix/.git/index differ
diff -uNr SuperGluePretrainedNetwork/models/superglue.py SuperGluePretrainedNetwork_fix/models/superglue.py
--- SuperGluePretrainedNetwork/models/superglue.py	2023-03-21 19:40:02.770550552 +0800
+++ SuperGluePretrainedNetwork_fix/models/superglue.py	2023-03-21 19:55:54.406562211 +0800
@@ -48,6 +48,45 @@
 from torch import nn
 
 
+def einsum1(eq, x0, y0):
+    '''
+    bdhn,bdhm->bhnm
+    '''
+    b, d, h, n = x0.shape
+    _, _, _, m = y0.shape
+    x = x0.reshape(b, d, h, n, 1)
+    y = y0.reshape(b, d, h, 1, m)
+    z = x * y
+    z = z.sum(dim=1)
+    return z
+
+
+def einsum2(eq, x0, y0):
+    '''
+    'bhnm,bdhm->bdhn'
+    '''
+    b, h, n, m = x0.shape
+    _, d, _, _ = y0.shape
+    x = x0.reshape(b, 1, h, n, m)
+    y = y0.reshape(b, d, h, 1, m)
+    z = x * y
+    z = z.sum(dim=-1)
+    return z
+
+
+def einsum3(eq, x0, y0):
+    '''
+    'bdn,bdm->bnm'
+    '''
+    b, d, n = x0.shape
+    _, _, m = y0.shape
+    x = x0.reshape(b, d, n, 1)
+    y = y0.reshape(b, d, 1, m)
+    z = x * y
+    z = z.sum(dim=1)
+    return z
+
+
 def MLP(channels: List[int], do_bn: bool = True) -> nn.Module:
     """ Multi-layer perceptron """
     n = len(channels)
@@ -86,9 +125,9 @@
 
 def attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> Tuple[torch.Tensor,torch.Tensor]:
     dim = query.shape[1]
-    scores = torch.einsum('bdhn,bdhm->bhnm', query, key) / dim**.5
+    scores = einsum1('bdhn,bdhm->bhnm', query, key) / dim**.5
     prob = torch.nn.functional.softmax(scores, dim=-1)
-    return torch.einsum('bhnm,bdhm->bdhn', prob, value), prob
+    return einsum2('bhnm,bdhm->bdhn', prob, value), prob
 
 
 class MultiHeadedAttention(nn.Module):
@@ -201,6 +240,7 @@
         'GNN_layers': ['self', 'cross'] * 9,
         'sinkhorn_iterations': 100,
         'match_threshold': 0.2,
+        'image_size': [1600, 1200]
     }
 
     def __init__(self, config):
@@ -226,11 +266,14 @@
         self.load_state_dict(torch.load(str(path)))
         print('Loaded SuperGlue model (\"{}\" weights)'.format(
             self.config['weights']))
+        
+        assert(len(self.config['image_size']) == 2)
+        self.image_size = self.config['image_size']
 
     def forward(self, data):
         """Run SuperGlue on a pair of keypoints and descriptors"""
-        desc0, desc1 = data['descriptors0'], data['descriptors1']
-        kpts0, kpts1 = data['keypoints0'], data['keypoints1']
+        desc0, desc1 = data[2], data[5]
+        kpts0, kpts1 = data[0], data[3]
 
         if kpts0.shape[1] == 0 or kpts1.shape[1] == 0:  # no keypoints
             shape0, shape1 = kpts0.shape[:-1], kpts1.shape[:-1]
@@ -242,12 +285,13 @@
             }
 
         # Keypoint normalization.
-        kpts0 = normalize_keypoints(kpts0, data['image0'].shape)
-        kpts1 = normalize_keypoints(kpts1, data['image1'].shape)
+        h, w = self.image_size
+        kpts0 = normalize_keypoints(kpts0, [1, 1, h, w])
+        kpts1 = normalize_keypoints(kpts1, [1, 1, h, w])
 
         # Keypoint MLP encoder.
-        desc0 = desc0 + self.kenc(kpts0, data['scores0'])
-        desc1 = desc1 + self.kenc(kpts1, data['scores1'])
+        desc0 = desc0 + self.kenc(kpts0, data[1])
+        desc1 = desc1 + self.kenc(kpts1, data[4])
 
         # Multi-layer Transformer network.
         desc0, desc1 = self.gnn(desc0, desc1)
@@ -256,7 +300,7 @@
         mdesc0, mdesc1 = self.final_proj(desc0), self.final_proj(desc1)
 
         # Compute matching descriptor distance.
-        scores = torch.einsum('bdn,bdm->bnm', mdesc0, mdesc1)
+        scores = einsum3('bdn,bdm->bnm', mdesc0, mdesc1)
         scores = scores / self.config['descriptor_dim']**.5
 
         # Run the optimal transport.
diff -uNr SuperGluePretrainedNetwork/models/superpoint.py SuperGluePretrainedNetwork_fix/models/superpoint.py
--- SuperGluePretrainedNetwork/models/superpoint.py	2023-03-21 19:40:02.770550552 +0800
+++ SuperGluePretrainedNetwork_fix/models/superpoint.py	2023-03-21 19:57:21.546563279 +0800
@@ -77,6 +77,92 @@
     return keypoints[indices], scores
 
 
+def bilinear_grid_sample(im, grid, align_corners=False):
+    """Given an input and a flow-field grid, computes the output using input
+    values and pixel locations from grid. Supported only bilinear interpolation
+    method to sample the input pixels.
+
+    Args:
+        im (torch.Tensor): Input feature map, shape (N, C, H, W)
+        grid (torch.Tensor): Point coordinates, shape (N, Hg, Wg, 2)
+        align_corners {bool}: If set to True, the extrema (-1 and 1) are
+            considered as referring to the center points of the input's
+            corner pixels. If set to False, they are instead considered as
+            referring to the corner points of the input's corner pixels,
+            making the sampling more resolution agnostic.
+    Returns:
+        torch.Tensor: A tensor with sampled points, shape (N, C, Hg, Wg)
+    """
+    n, c, h, w = im.shape
+    gn, gh, gw, _ = grid.shape
+    assert n == gn
+
+    x = grid[:, :, :, 0]
+    y = grid[:, :, :, 1]
+
+    if align_corners:
+        x = ((x + 1) / 2) * (w - 1)
+        y = ((y + 1) / 2) * (h - 1)
+    else:
+        x = ((x + 1) * w - 1) / 2
+        y = ((y + 1) * h - 1) / 2
+
+    x = x.view(n, -1)
+    y = y.view(n, -1)
+
+    x0 = torch.floor(x).long()
+    y0 = torch.floor(y).long()
+    x1 = x0 + 1
+    y1 = y0 + 1
+
+    wa = ((x1 - x) * (y1 - y)).unsqueeze(1)
+    wb = ((x1 - x) * (y - y0)).unsqueeze(1)
+    wc = ((x - x0) * (y1 - y)).unsqueeze(1)
+    wd = ((x - x0) * (y - y0)).unsqueeze(1)
+
+    # Apply default for grid_sample function zero padding
+    im_padded = torch.cat([im, torch.zeros(im.size(0), im.size(1), 1, im.size(3))], dim=2)
+    im_padded = torch.cat([im_padded, torch.zeros(im.size(0), im_padded.size(1), im_padded.size(2), 1)], dim=3)
+    im_padded = torch.cat([torch.zeros(im.size(0), im_padded.size(1), im_padded.size(2), 1), im_padded], dim=3)
+    im_padded = torch.cat([torch.zeros(im.size(0), im_padded.size(1), 1, im_padded.size(3)), im_padded], dim=2)
+    padded_h = h + 2
+    padded_w = w + 2
+    # save points positions after padding
+    x0, x1, y0, y1 = x0 + 1, x1 + 1, y0 + 1, y1 + 1
+
+    # Clip coordinates to padded image size
+    x0 = torch.where(x0 < 0, torch.tensor(0), x0)
+    x0 = torch.where(x0 > padded_w - 1, torch.tensor(padded_w - 1), x0)
+    x1 = torch.where(x1 < 0, torch.tensor(0), x1)
+    x1 = torch.where(x1 > padded_w - 1, torch.tensor(padded_w - 1), x1)
+    y0 = torch.where(y0 < 0, torch.tensor(0), y0)
+    y0 = torch.where(y0 > padded_h - 1, torch.tensor(padded_h - 1), y0)
+    y1 = torch.where(y1 < 0, torch.tensor(0), y1)
+    y1 = torch.where(y1 > padded_h - 1, torch.tensor(padded_h - 1), y1)
+
+    im_padded = im_padded.view(n, c, -1)
+
+    x0_y0_tmp = (x0 + y0 * padded_w).unsqueeze(1).to(dtype=torch.int32)
+    s = torch.zeros(x0_y0_tmp.size(0), c, x0_y0_tmp.size(2))
+    x0_y0 = x0_y0_tmp.expand_as(s).to(dtype=torch.int64)
+    x0_y1_tmp = (x0 + y1 * padded_w).unsqueeze(1).to(dtype=torch.int32)
+    s = torch.zeros(x0_y1_tmp.size(0), c, x0_y1_tmp.size(2))
+    x0_y1 = x0_y1_tmp.expand_as(s).to(dtype=torch.int64)
+    x1_y0_tmp = (x1 + y0 * padded_w).unsqueeze(1).to(dtype=torch.int32)
+    s = torch.zeros(x1_y0_tmp.size(0), c, x1_y0_tmp.size(2))
+    x1_y0 = x1_y0_tmp.expand_as(s).to(dtype=torch.int64)
+    x1_y1_tmp = (x1 + y1 * padded_w).unsqueeze(1).to(dtype=torch.int32)
+    s = torch.zeros(x1_y1_tmp.size(0), c, x1_y1_tmp.size(2))
+    x1_y1 = x1_y1_tmp.expand_as(s).to(dtype=torch.int64)
+
+    Ia = torch.gather(im_padded, 2, x0_y0)
+    Ib = torch.gather(im_padded, 2, x0_y1)
+    Ic = torch.gather(im_padded, 2, x1_y0)
+    Id = torch.gather(im_padded, 2, x1_y1)
+
+    return (Ia * wa + Ib * wb + Ic * wc + Id * wd).reshape(n, c, gh, gw)
+
+
 def sample_descriptors(keypoints, descriptors, s: int = 8):
     """ Interpolate descriptors at keypoint locations """
     b, c, h, w = descriptors.shape
@@ -84,9 +170,8 @@
     keypoints /= torch.tensor([(w*s - s/2 - 0.5), (h*s - s/2 - 0.5)],
                               ).to(keypoints)[None]
     keypoints = keypoints*2 - 1  # normalize to (-1, 1)
-    args = {'align_corners': True} if torch.__version__ >= '1.3' else {}
-    descriptors = torch.nn.functional.grid_sample(
-        descriptors, keypoints.view(b, 1, -1, 2), mode='bilinear', **args)
+    descriptors = bilinear_grid_sample(
+        descriptors, keypoints.view(b, 1, -1, 2), align_corners=True)
     descriptors = torch.nn.functional.normalize(
         descriptors.reshape(b, c, -1), p=2, dim=1)
     return descriptors
@@ -142,10 +227,10 @@
 
         print('Loaded SuperPoint model')
 
-    def forward(self, data):
+    def forward(self, x):
         """ Compute keypoints, scores, descriptors for image """
         # Shared Encoder
-        x = self.relu(self.conv1a(data['image']))
+        x = self.relu(self.conv1a(x))
         x = self.relu(self.conv1b(x))
         x = self.pool(x)
         x = self.relu(self.conv2a(x))
@@ -162,9 +247,9 @@
         scores = self.convPb(cPa)
         scores = torch.nn.functional.softmax(scores, 1)[:, :-1]
         b, _, h, w = scores.shape
-        scores = scores.permute(0, 2, 3, 1).reshape(b, h, w, 8, 8)
-        scores = scores.permute(0, 1, 3, 2, 4).reshape(b, h*8, w*8)
+        scores = torch.nn.functional.pixel_shuffle(scores, 8)
         scores = simple_nms(scores, self.config['nms_radius'])
+        scores = scores.squeeze(1)
 
         # Extract keypoints
         keypoints = [
