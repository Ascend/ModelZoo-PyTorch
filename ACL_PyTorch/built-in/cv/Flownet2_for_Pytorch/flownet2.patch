diff --git a/models.py b/models.py
index 8457d2f..34380a6 100755
--- a/models.py
+++ b/models.py
@@ -6,8 +6,10 @@ import math
 import numpy as np
 
 try:
-    from networks.resample2d_package.resample2d import Resample2d
-    from networks.channelnorm_package.channelnorm import ChannelNorm
+    # from networks.resample2d_package.resample2d import Resample2d
+    # from networks.channelnorm_package.channelnorm import ChannelNorm
+    from networks.resample2d_onnx import Resample2d
+    from networks.channelnorm_onnx import ChannelNorm
 
     from networks import FlowNetC
     from networks import FlowNetS
@@ -16,8 +18,10 @@ try:
 
     from networks.submodules import *
 except:
-    from .networks.resample2d_package.resample2d import Resample2d
-    from .networks.channelnorm_package.channelnorm import ChannelNorm
+    # from .networks.resample2d_package.resample2d import Resample2d
+    # from .networks.channelnorm_package.channelnorm import ChannelNorm
+    from .networks.resample2d_onnx import Resample2d
+    from .networks.channelnorm_onnx import ChannelNorm
 
     from .networks import FlowNetC
     from .networks import FlowNetS
@@ -33,7 +37,7 @@ class FlowNet2(nn.Module):
         super(FlowNet2,self).__init__()
         self.batchNorm = batchNorm
         self.div_flow = div_flow
-        self.rgb_max = args.rgb_max
+        # self.rgb_max = args.rgb_max
         self.args = args
 
         self.channelnorm = ChannelNorm()
@@ -117,12 +121,12 @@ class FlowNet2(nn.Module):
             weight.data[i,i,:,:] = torch.from_numpy(bilinear)
         return 
 
-    def forward(self, inputs):
-        rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))
+    def forward(self, x1, x2):
+        # rgb_mean = inputs.contiguous().view(inputs.size()[:2]+(-1,)).mean(dim=-1).view(inputs.size()[:2] + (1,1,1,))
         
-        x = (inputs - rgb_mean) / self.rgb_max
-        x1 = x[:,:,0,:,:]
-        x2 = x[:,:,1,:,:]
+        # x = (inputs - rgb_mean) / self.rgb_max
+        # x1 = x[:,:,0,:,:]
+        # x2 = x[:,:,1,:,:]
         x = torch.cat((x1,x2), dim = 1)
 
         # flownetc
diff --git a/networks/FlowNetC.py b/networks/FlowNetC.py
index 61e117a..32e130d 100755
--- a/networks/FlowNetC.py
+++ b/networks/FlowNetC.py
@@ -5,7 +5,8 @@ from torch.nn import init
 import math
 import numpy as np
 
-from .correlation_package.correlation import Correlation
+# from .correlation_package.correlation import Correlation
+from .correlation_onnx import Correlation
 
 from .submodules import *
 'Parameter count , 39,175,298 '
diff --git a/networks/channelnorm_onnx.py b/networks/channelnorm_onnx.py
new file mode 100644
index 0000000..d490c8a
--- /dev/null
+++ b/networks/channelnorm_onnx.py
@@ -0,0 +1,13 @@
+import torch
+import torch.nn as nn
+
+
+class ChannelNorm(nn.Module):
+    def __init__(self, dim=1):
+        super(ChannelNorm, self).__init__()
+        self.dim = 1
+
+    def forward(self, data):
+        sq = data.mul(data)
+        r_sum = torch.sum(sq, dim=self.dim, keepdim=True)
+        return torch.sqrt(r_sum)
diff --git a/networks/correlation_onnx.py b/networks/correlation_onnx.py
new file mode 100644
index 0000000..4cb01ae
--- /dev/null
+++ b/networks/correlation_onnx.py
@@ -0,0 +1,28 @@
+import math
+import torch
+import torch.nn as nn
+
+
+class Correlation(nn.Module):
+    def __init__(self, pad_size=4, kernel_size=1, max_displacement=4, stride1=1, stride2=1, corr_multiply=1):
+        assert kernel_size == 1
+        assert pad_size == max_displacement
+        super().__init__()
+        self.stride1 = stride1
+        self.stride2 = stride2
+        self.max_hdisp = max_displacement
+        self.padlayer = nn.ConstantPad2d(pad_size, 0)
+        self.offset_size = math.ceil((2 * self.max_hdisp + 1) / self.stride2) * \
+            math.ceil((2 * self.max_hdisp + 1) / self.stride2)
+        # adhoc fix size for input: 56, 128
+        self.hei = 56
+        self.wid = 128
+
+    def forward(self, in1, in2):
+        in2_pad = self.padlayer(in2)
+        offsety, offsetx = torch.meshgrid([torch.arange(0, 2 * self.max_hdisp + 1, step=self.stride2),
+                                           torch.arange(0, 2 * self.max_hdisp + 1, step=self.stride2)])
+        output = torch.cat([
+            torch.mean(in1 * in2_pad[:, :, dy:dy+self.hei, dx:dx+self.wid], 1, keepdim=True)
+            for dx, dy in zip(offsetx.reshape(self.offset_size), offsety.reshape(self.offset_size))], 1)
+        return output
diff --git a/networks/resample2d_onnx.py b/networks/resample2d_onnx.py
new file mode 100644
index 0000000..46074b9
--- /dev/null
+++ b/networks/resample2d_onnx.py
@@ -0,0 +1,91 @@
+import torch
+import torch.nn as nn
+
+
+class Resample2d(nn.Module):
+    def forward(self, input1, input2):
+        _, d, _, _ = input1.size()
+        b, _, h, w = input2.size()
+        y, x = torch.meshgrid(torch.arange(h), torch.arange(w))
+
+
+        if b == 1:
+            # optimize for bs1
+            input1 = input1.squeeze().reshape(3, h*w).transpose(0, 1)
+            input2 = input2.squeeze()
+
+            fx = input2[0, :, :]
+            fy = input2[1, :, :]
+
+            x2 = x.float() + fx
+            y2 = y.float() + fy
+
+            ix2_L = x2
+            iy2_T = y2
+            ix2_L = torch.clamp(ix2_L, 0, w-1)
+            iy2_T = torch.clamp(iy2_T, 0, h-1)
+            ix2_R = torch.clamp(ix2_L + 1, 0, w-1)
+            iy2_B = torch.clamp(iy2_T + 1, 0, h-1)
+
+            alpha = x2 - torch.floor(ix2_L)
+            beta = y2 - torch.floor(iy2_T)
+
+            ix2_L = ix2_L.long()
+            iy2_T = iy2_T.long()
+            ix2_R = ix2_R.long()
+            iy2_B = iy2_B.long()
+
+            # TL = input1[0, :, iy2_T, ix2_L]
+            # TR = input1[0, :, iy2_T, ix2_R]
+            # BL = input1[0, :, iy2_B, ix2_L]
+            # BR = input1[0, :, iy2_B, ix2_R]
+            # optimize for onnx
+            TL = input1[iy2_T * w + ix2_L].reshape(h, w, 3).permute(2, 0, 1)
+            TR = input1[iy2_T * w + ix2_R].reshape(h, w, 3).permute(2, 0, 1)
+            BL = input1[iy2_B * w + ix2_L].reshape(h, w, 3).permute(2, 0, 1)
+            BR = input1[iy2_B * w + ix2_R].reshape(h, w, 3).permute(2, 0, 1)
+
+            # Interpolation
+            warped_data = (1-alpha) * (1-beta) * TL + \
+                alpha * (1-beta) * TR + \
+                (1-alpha) * beta * BL + \
+                alpha * beta * BR
+
+            return warped_data.unsqueeze(axis=0)
+        else:
+            dims = torch.arange(b).unsqueeze(dim=1).unsqueeze(dim=2)
+            dims = dims.repeat(1, h, w)
+
+            fx = input2[:, 0, :, :]
+            fy = input2[:, 1, :, :]
+
+            x2 = x.float() + fx
+            y2 = y.float() + fy
+
+            ix2_L = x2
+            iy2_T = y2
+            ix2_L = torch.clamp(ix2_L, 0, w-1)
+            iy2_T = torch.clamp(iy2_T, 0, h-1)
+            ix2_R = torch.clamp(ix2_L + 1, 0, w-1)
+            iy2_B = torch.clamp(iy2_T + 1, 0, h-1)
+
+            alpha = x2 - torch.floor(ix2_L)
+            beta = y2 - torch.floor(iy2_T)
+
+            ix2_L = ix2_L.long()
+            iy2_T = iy2_T.long()
+            ix2_R = ix2_R.long()
+            iy2_B = iy2_B.long()
+
+            TL = input1.permute(1, 0, 2, 3)[:, dims, iy2_T, ix2_L]
+            TR = input1.permute(1, 0, 2, 3)[:, dims, iy2_T, ix2_R]
+            BL = input1.permute(1, 0, 2, 3)[:, dims, iy2_B, ix2_L]
+            BR = input1.permute(1, 0, 2, 3)[:, dims, iy2_B, ix2_R]
+
+            # Interpolation
+            warped_data = (1-alpha) * (1-beta) * TL + \
+                alpha * (1-beta) * TR + \
+                (1-alpha) * beta * BL + \
+                alpha * beta * BR
+
+            return warped_data.permute(1, 0, 2, 3)
