diff --git a/detectron2/layers/__init__.py b/detectron2/layers/__init__.py
index c8bd1fb..d821e8b 100644
--- a/detectron2/layers/__init__.py
+++ b/detectron2/layers/__init__.py
@@ -2,7 +2,8 @@
 from .batch_norm import FrozenBatchNorm2d, get_norm, NaiveSyncBatchNorm
 from .deform_conv import DeformConv, ModulatedDeformConv
 from .mask_ops import paste_masks_in_image
-from .nms import batched_nms, batched_nms_rotated, nms, nms_rotated
+#from .nms import batched_nms, batched_nms_rotated, nms, nms_rotated
+from .nms import batched_nms, batch_nms_op, batched_nms_rotated, nms, nms_rotated
 from .roi_align import ROIAlign, roi_align
 from .roi_align_rotated import ROIAlignRotated, roi_align_rotated
 from .shape_spec import ShapeSpec
diff --git a/detectron2/layers/nms.py b/detectron2/layers/nms.py
index ac14d45..ff2e166 100644
--- a/detectron2/layers/nms.py
+++ b/detectron2/layers/nms.py
@@ -16,6 +16,58 @@ else:
     nms_rotated_func = torch.ops.detectron2.nms_rotated
 
 
+class BatchNMSOp(torch.autograd.Function):
+    @staticmethod
+    def forward(ctx, bboxes, scores, score_threshold, iou_threshold, max_size_per_class, max_total_size):
+        """
+        boxes (torch.Tensor): boxes in shape (batch, N, C, 4).
+        scores (torch.Tensor): scores in shape (batch, N, C).
+        return:
+            nmsed_boxes: (1, N, 4)
+            nmsed_scores: (1, N)
+            nmsed_classes: (1, N)
+            nmsed_num: (1,)
+        """
+
+        # Phony implementation for onnx export
+        nmsed_boxes = bboxes[:, :max_total_size, 0, :]
+        nmsed_scores = scores[:, :max_total_size, 0]
+        nmsed_classes = torch.arange(max_total_size, dtype=torch.long)
+        nmsed_num = torch.Tensor([max_total_size])
+
+        return nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_num
+
+    @staticmethod
+    def symbolic(g, bboxes, scores, score_thr, iou_thr, max_size_p_class, max_t_size):
+        nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_num = g.op('BatchMultiClassNMS',
+            bboxes, scores, score_threshold_f=score_thr, iou_threshold_f=iou_thr,
+            max_size_per_class_i=max_size_p_class, max_total_size_i=max_t_size, outputs=4)
+        return nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_num
+
+def batch_nms_op(bboxes, scores, score_threshold, iou_threshold, max_size_per_class, max_total_size):
+    """
+    boxes (torch.Tensor): boxes in shape (N, 4).
+    scores (torch.Tensor): scores in shape (N, ).
+    #boxes_all:[N,80];score_all:[N,80,4]
+    """
+
+    if bboxes.dtype == torch.float32:
+        bboxes = bboxes.reshape(1, bboxes.shape[0].numpy(), -1, 4).half()
+        scores = scores.reshape(1, scores.shape[0].numpy(), -1).half()
+    else:
+        bboxes = bboxes.reshape(1, bboxes.shape[0].numpy(), -1, 4)
+        scores = scores.reshape(1, scores.shape[0].numpy(), -1)
+
+    nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_num = BatchNMSOp.apply(bboxes, scores,
+        score_threshold, iou_threshold, max_size_per_class, max_total_size)
+    nmsed_boxes = nmsed_boxes.float()
+    nmsed_scores = nmsed_scores.float()
+    nmsed_classes = nmsed_classes.long()
+    dets = torch.cat((nmsed_boxes.reshape((-1, 4)), nmsed_scores.reshape((-1, 1))), -1)
+    labels = nmsed_classes.reshape(-1,1)
+    return dets, labels,nmsed_num
+
+
 def batched_nms(
     boxes: torch.Tensor, scores: torch.Tensor, idxs: torch.Tensor, iou_threshold: float
 ):
diff --git a/detectron2/modeling/box_regression.py b/detectron2/modeling/box_regression.py
index 12be000..3c7ad23 100644
--- a/detectron2/modeling/box_regression.py
+++ b/detectron2/modeling/box_regression.py
@@ -87,20 +87,30 @@ class Box2BoxTransform(object):
         deltas = deltas.float()  # ensure fp32 for decoding precision
         boxes = boxes.to(deltas.dtype)
 
-        widths = boxes[:, 2] - boxes[:, 0]
-        heights = boxes[:, 3] - boxes[:, 1]
-        ctr_x = boxes[:, 0] + 0.5 * widths
-        ctr_y = boxes[:, 1] + 0.5 * heights
+        boxes_prof = boxes.permute(1, 0)
+        widths = boxes_prof[2, :] - boxes_prof[0, :]
+        heights = boxes_prof[3, :] - boxes_prof[1, :]
+        ctr_x = boxes_prof[0, :] + 0.5 * widths
+        ctr_y = boxes_prof[1, :] + 0.5 * heights
 
         wx, wy, ww, wh = self.weights
-        dx = deltas[:, 0::4] / wx
-        dy = deltas[:, 1::4] / wy
-        dw = deltas[:, 2::4] / ww
-        dh = deltas[:, 3::4] / wh
+
+        denorm_deltas = deltas
+        if denorm_deltas.shape[1] > 4:
+            denorm_deltas = denorm_deltas.view(-1, 80, 4)
+            dx = denorm_deltas[:, :, 0:1:].view(-1, 80) / wx
+            dy = denorm_deltas[:, :, 1:2:].view(-1, 80) / wy
+            dw = denorm_deltas[:, :, 2:3:].view(-1, 80) / ww
+            dh = denorm_deltas[:, :, 3:4:].view(-1, 80) / wh
+        else:
+            dx = denorm_deltas[:, 0:1:] / wx
+            dy = denorm_deltas[:, 1:2:] / wy
+            dw = denorm_deltas[:, 2:3:] / ww
+            dh = denorm_deltas[:, 3:4:] / wh
 
         # Prevent sending too large values into torch.exp()
-        dw = torch.clamp(dw, max=self.scale_clamp)
-        dh = torch.clamp(dh, max=self.scale_clamp)
+        dw = torch.clamp(dw, min=-float('inf'), max=self.scale_clamp)
+        dh = torch.clamp(dh, min=-float('inf'), max=self.scale_clamp)
 
         pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]
         pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]
@@ -112,6 +122,7 @@ class Box2BoxTransform(object):
         x2 = pred_ctr_x + 0.5 * pred_w
         y2 = pred_ctr_y + 0.5 * pred_h
         pred_boxes = torch.stack((x1, y1, x2, y2), dim=-1)
+        
         return pred_boxes.reshape(deltas.shape)
 
 
diff --git a/detectron2/modeling/meta_arch/retinanet.py b/detectron2/modeling/meta_arch/retinanet.py
index 81992a3..54621ac 100644
--- a/detectron2/modeling/meta_arch/retinanet.py
+++ b/detectron2/modeling/meta_arch/retinanet.py
@@ -10,7 +10,7 @@ from torch.nn import functional as F
 
 from detectron2.config import configurable
 from detectron2.data.detection_utils import convert_image_to_rgb
-from detectron2.layers import ShapeSpec, batched_nms, cat, get_norm, nonzero_tuple
+from detectron2.layers import ShapeSpec, batched_nms, cat, get_norm,batch_nms_op
 from detectron2.structures import Boxes, ImageList, Instances, pairwise_iou
 from detectron2.utils.events import get_event_storage
 
@@ -229,7 +229,7 @@ class RetinaNet(nn.Module):
         vis_name = f"Top: GT bounding boxes; Bottom: {max_boxes} Highest Scoring Results"
         storage.put_image(vis_name, vis_img)
 
-    def forward(self, batched_inputs: List[Dict[str, Tensor]]):
+    def forward(self, batched_inputs: Tuple[Dict[str, Tensor]]):
         """
         Args:
             batched_inputs: a list, batched outputs of :class:`DatasetMapper` .
@@ -249,9 +249,8 @@ class RetinaNet(nn.Module):
             in :doc:`/tutorials/models`.
         """
         images = self.preprocess_image(batched_inputs)
-        features = self.backbone(images.tensor)
+        features = self.backbone(images)
         features = [features[f] for f in self.head_in_features]
-
         anchors = self.anchor_generator(features)
         pred_logits, pred_anchor_deltas = self.head(features)
         # Transpose the Hi*Wi*A dimension to the middle:
@@ -276,18 +275,9 @@ class RetinaNet(nn.Module):
 
             return losses
         else:
-            results = self.inference(anchors, pred_logits, pred_anchor_deltas, images.image_sizes)
-            if torch.jit.is_scripting():
-                return results
-            processed_results = []
-            for results_per_image, input_per_image, image_size in zip(
-                results, batched_inputs, images.image_sizes
-            ):
-                height = input_per_image.get("height", image_size[0])
-                width = input_per_image.get("width", image_size[1])
-                r = detector_postprocess(results_per_image, height, width)
-                processed_results.append({"instances": r})
-            return processed_results
+            results = self.inference(anchors, pred_logits, pred_anchor_deltas, [(images.shape[1], images.shape[2])])
+
+        return results
 
     def losses(self, anchors, pred_logits, gt_labels, pred_anchor_deltas, gt_boxes):
         """
@@ -415,6 +405,7 @@ class RetinaNet(nn.Module):
                 anchors, pred_logits_per_image, deltas_per_image, image_size
             )
             results.append(results_per_image)
+
         return results
 
     def inference_single_image(
@@ -441,57 +432,40 @@ class RetinaNet(nn.Module):
         """
         boxes_all = []
         scores_all = []
-        class_idxs_all = []
-
         # Iterate over every feature level
+        max_size_perclass=200
         for box_cls_i, box_reg_i, anchors_i in zip(box_cls, box_delta, anchors):
             # (HxWxAxK,)
-            predicted_prob = box_cls_i.flatten().sigmoid_()
-
-            # Apply two filtering below to make NMS faster.
-            # 1. Keep boxes with confidence score higher than threshold
-            keep_idxs = predicted_prob > self.test_score_thresh
-            predicted_prob = predicted_prob[keep_idxs]
-            topk_idxs = nonzero_tuple(keep_idxs)[0]
-
-            # 2. Keep top k top scoring boxes only
-            num_topk = min(self.test_topk_candidates, topk_idxs.size(0))
-            # torch.sort is actually faster than .topk (at least on GPUs)
-            predicted_prob, idxs = predicted_prob.sort(descending=True)
-            predicted_prob = predicted_prob[:num_topk]
-            topk_idxs = topk_idxs[idxs[:num_topk]]
-
-            anchor_idxs = topk_idxs // self.num_classes
-            classes_idxs = topk_idxs % self.num_classes
-
-            box_reg_i = box_reg_i[anchor_idxs]
-            anchors_i = anchors_i[anchor_idxs]
-            # predict boxes
-            predicted_boxes = self.box2box_transform.apply_deltas(box_reg_i, anchors_i.tensor)
-
+            predicted_prob = box_cls_i.sigmoid_()
+            predicted_prob, indices =torch.topk(predicted_prob,max_size_perclass,dim=0)#[top,80]
+            indices = indices.flatten()
+            box_reg_i = box_reg_i[indices]#[top*80,4]
+            anchors_i = anchors_i[indices]#[top*80,4]
+            predicted_boxes = self.box2box_transform.apply_deltas(box_reg_i, anchors_i.tensor)\
+                .reshape(-1,self.num_classes,4)#[top,80,4]
             boxes_all.append(predicted_boxes)
             scores_all.append(predicted_prob)
-            class_idxs_all.append(classes_idxs)
 
-        boxes_all, scores_all, class_idxs_all = [
-            cat(x) for x in [boxes_all, scores_all, class_idxs_all]
+        boxes_all, scores_all = [
+            cat(x) for x in [boxes_all, scores_all]
         ]
-        keep = batched_nms(boxes_all, scores_all, class_idxs_all, self.test_nms_thresh)
-        keep = keep[: self.max_detections_per_image]
+
+        # boxes_all:[N,80];score_all:[N,80,4]
+        dets, labels,_=batch_nms_op(boxes_all, scores_all, self.test_score_thresh, self.test_nms_thresh,max_size_per_class=max_size_perclass,max_total_size=self.max_detections_per_image)
 
         result = Instances(image_size)
-        result.pred_boxes = Boxes(boxes_all[keep])
-        result.scores = scores_all[keep]
-        result.pred_classes = class_idxs_all[keep]
+        result.pred_boxes = Boxes(dets[:, :4])
+        result.scores = dets[:, 4]
+        result.pred_classes = labels
+
+
         return result
 
-    def preprocess_image(self, batched_inputs: List[Dict[str, Tensor]]):
+    def preprocess_image(self, batched_inputs: Tuple[Dict[str, Tensor]]):
         """
         Normalize, pad and batch the input images.
         """
-        images = [x["image"].to(self.device) for x in batched_inputs]
-        images = [(x - self.pixel_mean) / self.pixel_std for x in images]
-        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
+        images = batched_inputs[0]["image"].to(self.device)
         return images
 
 
diff --git a/tools/deploy/export_model.py b/tools/deploy/export_model.py
index 6fb8cee..38fad9d 100755
--- a/tools/deploy/export_model.py
+++ b/tools/deploy/export_model.py
@@ -5,7 +5,7 @@ import os
 from typing import Dict, List, Tuple
 import torch
 from torch import Tensor, nn
-
+import numpy as np
 import detectron2.data.transforms as T
 from detectron2.checkpoint import DetectionCheckpointer
 from detectron2.config import get_cfg
@@ -107,7 +107,7 @@ def export_scripting(torch_model):
 # experimental. API not yet final
 def export_tracing(torch_model, inputs):
     assert TORCH_VERSION >= (1, 8)
-    image = inputs[0]["image"]
+    image = torch.Tensor(1, 3, 1344, 1344)
     inputs = [{"image": image}]  # remove other unused keys
 
     if isinstance(torch_model, GeneralizedRCNN):
@@ -130,7 +130,11 @@ def export_tracing(torch_model, inputs):
     elif args.format == "onnx":
         # NOTE onnx export currently failing in pytorch
         with PathManager.open(os.path.join(args.output, "model.onnx"), "wb") as f:
-            torch.onnx.export(traceable_model, (image,), f)
+            input_names = ["input0"]
+            output_names = ["output0", "output1", "output2", "output3"]
+            dynamic_axes = {'input0': [0], "output0": [0], "output1": [0], "output2": [0]}
+            torch.onnx.export(traceable_model, (image,), f, input_names=input_names, output_names=output_names,
+                              dynamic_axes=dynamic_axes, opset_version=11)
     logger.info("Inputs schema: " + str(traceable_model.inputs_schema))
     logger.info("Outputs schema: " + str(traceable_model.outputs_schema))
 
@@ -153,7 +157,6 @@ def export_tracing(torch_model, inputs):
 
 
 def get_sample_inputs(args):
-
     if args.sample_image is None:
         # get a first batch from dataset
         data_loader = build_detection_test_loader(cfg, cfg.DATASETS.TEST[0])
@@ -178,6 +181,7 @@ def get_sample_inputs(args):
 
 
 if __name__ == "__main__":
+
     parser = argparse.ArgumentParser(description="Export a model for deployment.")
     parser.add_argument(
         "--format",
