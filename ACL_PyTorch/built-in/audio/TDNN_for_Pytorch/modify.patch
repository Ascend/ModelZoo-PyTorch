diff --git a/speechbrain/nnet/CNN.py b/speechbrain/nnet/CNN.py
index 1745846..8750680 100644
--- a/speechbrain/nnet/CNN.py
+++ b/speechbrain/nnet/CNN.py
@@ -346,7 +346,7 @@ class Conv1d(nn.Module):
         padding="same",
         groups=1,
         bias=True,
-        padding_mode="reflect",
+        padding_mode="constant",
         skip_transpose=False,
     ):
         super().__init__()
diff --git a/speechbrain/pretrained/interfaces.py b/speechbrain/pretrained/interfaces.py
index e2521ec..ead6a06 100644
--- a/speechbrain/pretrained/interfaces.py
+++ b/speechbrain/pretrained/interfaces.py
@@ -1,3 +1,17 @@
+# Copyright 2021 Huawei Technologies Co., Ltd
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
 """Defines interfaces for simple inference with pretrained models
 
 Authors:
@@ -85,7 +99,10 @@ class Pretrained:
                     setattr(self, arg, default)
 
         # Put modules on the right device, accessible with dot notation
-        self.modules = torch.nn.ModuleDict(modules).to(self.device)
+        self.modules = torch.nn.ModuleDict(modules)
+        for mod in self.modules:
+            self.modules[mod].to(self.device)
+
         for mod in self.MODULES_NEEDED:
             if mod not in modules:
                 raise ValueError(f"Need modules['{mod}']")
@@ -93,7 +110,7 @@ class Pretrained:
         # Check MODULES_NEEDED and HPARAMS_NEEDED and
         # make hyperparams available with dot notation
         if self.HPARAMS_NEEDED and hparams is None:
-            raise ValueError(f"Need to provide hparams dict.")
+            raise ValueError("Need to provide hparams dict.")
         if hparams is not None:
             # Also first check that all required params are found:
             for hp in self.HPARAMS_NEEDED:
@@ -190,6 +207,7 @@ class Pretrained:
         hparams_file="hyperparams.yaml",
         overrides={},
         savedir=None,
+        use_auth_token=False,
         **kwargs,
     ):
         """Fetch and load based from outside source based on HyperPyYAML file
@@ -215,12 +233,17 @@ class Pretrained:
             Any changes to make to the hparams file when it is loaded.
         savedir : str or Path
             Where to put the pretraining material. If not given, will use
-            ./pretrained_checkpoints/<class-name>-hash(source).
+            ./pretrained_models/<class-name>-hash(source).
+        use_auth_token : bool (default: False)
+            If true Hugginface's auth_token will be used to load private models from the HuggingFace Hub,
+            default is False because majority of models are public.
         """
         if savedir is None:
             clsname = cls.__name__
-            savedir = f"./pretrained_checkpoints/{clsname}-{hash(source)}"
-        hparams_local_path = fetch(hparams_file, source, savedir)
+            savedir = f"./pretrained_models/{clsname}-{hash(source)}"
+        hparams_local_path = fetch(
+            hparams_file, source, savedir, use_auth_token
+        )
 
         # Load the modules:
         with open(hparams_local_path) as fin:
@@ -257,7 +280,7 @@ class EndToEndSLU(Pretrained):
     "{'intent': 'SimpleMath', 'slots': {'number1': 37.67, 'number2': 75.7, 'op': ' minus '}}"
     """
 
-    HPARAMS_NEEDED = ["tokenizer", "asr_model"]
+    HPARAMS_NEEDED = ["tokenizer", "asr_model_source"]
     MODULES_NEEDED = [
         "slu_enc",
         "beam_searcher",
@@ -266,6 +289,10 @@ class EndToEndSLU(Pretrained):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.tokenizer = self.hparams.tokenizer
+        self.asr_model = EncoderDecoderASR.from_hparams(
+            source=self.hparams.asr_model_source,
+            run_opts={"device": self.device},
+        )
 
     def decode_file(self, path):
         """Maps the given audio file to a string representing the
@@ -282,6 +309,7 @@ class EndToEndSLU(Pretrained):
             The predicted semantics.
         """
         waveform = self.load_audio(path)
+        waveform = waveform.to(self.device)
         # Fake a batch:
         batch = waveform.unsqueeze(0)
         rel_length = torch.tensor([1.0])
@@ -310,10 +338,10 @@ class EndToEndSLU(Pretrained):
         wavs = wavs.float()
         wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)
         with torch.no_grad():
-            ASR_encoder_out = self.hparams.asr_model.encode_batch(
+            ASR_encoder_out = self.asr_model.encode_batch(
                 wavs.detach(), wav_lens
             )
-        encoder_out = self.hparams.slu_enc(ASR_encoder_out)
+        encoder_out = self.modules.slu_enc(ASR_encoder_out)
         return encoder_out
 
     def decode_batch(self, wavs, wav_lens):
@@ -338,7 +366,7 @@ class EndToEndSLU(Pretrained):
             Each predicted token id.
         """
         with torch.no_grad():
-            wav_lens = wav_lens.to(self.device)
+            wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)
             encoder_out = self.encode_batch(wavs, wav_lens)
             predicted_tokens, scores = self.modules.beam_searcher(
                 encoder_out, wav_lens
@@ -512,6 +540,23 @@ class EncoderClassifier(Pretrained):
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
+    
+    def extract_feats(self, wavs, wav_lens=None):
+        # wav to feats
+        wavs = wavs.to('cpu').float()
+        if wav_lens is None:
+            wav_lens = torch.ones(wavs.shape[0], device='cpu')
+        
+        feats = self.modules.compute_features(wavs)
+        feats = self.modules.mean_var_norm(feats, wav_lens)
+
+        return feats
+    
+    def feats_classify(self, feats, wav_lens=None):
+        emb = self.modules.embedding_model(feats, wav_lens)
+        out_prob = self.modules.classifier(emb).squeeze(1)
+
+        return out_prob
 
     def encode_batch(self, wavs, wav_lens=None, normalize=False):
         """Encodes the input audio into a single vector embedding.
@@ -595,7 +640,36 @@ class EncoderClassifier(Pretrained):
         out_prob = self.modules.classifier(emb).squeeze(1)
         score, index = torch.max(out_prob, dim=-1)
         text_lab = self.hparams.label_encoder.decode_torch(index)
+        return out_prob, score, index, text_lab
 
+    def classify_file(self, path):
+        """Classifies the given audiofile into the given set of labels.
+
+        Arguments
+        ---------
+        path : str
+            Path to audio file to classify.
+
+        Returns
+        -------
+        out_prob
+            The log posterior probabilities of each class ([batch, N_class])
+        score:
+            It is the value of the log-posterior for the best class ([batch,])
+        index
+            The indexes of the best class ([batch,])
+        text_lab:
+            List with the text labels corresponding to the indexes.
+            (label encoder should be provided).
+        """
+        waveform = self.load_audio(path)
+        # Fake a batch:
+        batch = waveform.unsqueeze(0)
+        rel_length = torch.tensor([1.0])
+        emb = self.encode_batch(batch, rel_length)
+        out_prob = self.modules.classifier(emb).squeeze(1)
+        score, index = torch.max(out_prob, dim=-1)
+        text_lab = self.hparams.label_encoder.decode_torch(index)
         return out_prob, score, index, text_lab
 
 
@@ -732,6 +806,7 @@ class SepformerSeparation(Pretrained):
         """
 
         # Separation
+        mix = mix.to(self.device)
         mix_w = self.modules.encoder(mix)
         est_mask = self.modules.masknet(mix_w)
         mix_w = torch.stack([mix_w] * self.hparams.num_spks)
@@ -774,6 +849,7 @@ class SepformerSeparation(Pretrained):
         path = fetch(fl, source=source, savedir=savedir)
 
         batch, fs_file = torchaudio.load(path)
+        batch = batch.to(self.device)
         fs_model = self.hparams.sample_rate
 
         # resample the data if needed
@@ -846,6 +922,7 @@ class SpectralMaskEnhancement(Pretrained):
         torch.tensor
             A batch of enhanced waveforms of the same shape as input.
         """
+        noisy = noisy.to(self.device)
         noisy_features = self.compute_features(noisy)
 
         # Perform masking-based enhancement, multiplying output with input.
@@ -869,6 +946,7 @@ class SpectralMaskEnhancement(Pretrained):
             If provided, writes enhanced data to this file.
         """
         noisy = self.load_audio(filename)
+        noisy = noisy.to(self.device)
 
         # Fake a batch:
         batch = noisy.unsqueeze(0)
diff --git a/templates/speaker_id/custom_model.py b/templates/speaker_id/custom_model.py
index 9a78a37..3a67eae 100644
--- a/templates/speaker_id/custom_model.py
+++ b/templates/speaker_id/custom_model.py
@@ -76,9 +76,10 @@ class Xvector(torch.nn.Module):
                         out_channels=out_channels,
                         kernel_size=tdnn_kernel_sizes[block_index],
                         dilation=tdnn_dilations[block_index],
+                        skip_transpose=True,
                     ),
                     activation(),
-                    BatchNorm1d(input_size=out_channels),
+                    BatchNorm1d(input_size=out_channels,skip_transpose=True),
                 ]
             )
             in_channels = tdnn_channels[block_index]
@@ -105,8 +106,12 @@ class Xvector(torch.nn.Module):
         ---------
         x : torch.Tensor
         """
+        x = x.transpose(1, -1)
 
         for layer in self.blocks:
+            if type(layer) == type(StatisticsPooling()):
+                x = x.transpose(1, -1)
+            
             try:
                 x = layer(x, lengths=lens)
             except TypeError:
diff --git a/templates/speaker_id/mini_librispeech_prepare.py b/templates/speaker_id/mini_librispeech_prepare.py
index c22add8..7a777df 100644
--- a/templates/speaker_id/mini_librispeech_prepare.py
+++ b/templates/speaker_id/mini_librispeech_prepare.py
@@ -171,7 +171,7 @@ def split_sets(wav_list, split_ratio):
     dictionary containing train, valid, and test splits.
     """
     # Random shuffle of the list
-    random.shuffle(wav_list)
+    # random.shuffle(wav_list)
     tot_split = sum(split_ratio)
     tot_snts = len(wav_list)
     data_split = {}
