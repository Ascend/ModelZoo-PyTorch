diff --git a/espnet_onnx/asr/abs_asr_model.py b/espnet_onnx/asr/abs_asr_model.py
index d55e2c4..1fecbeb 100644
--- a/espnet_onnx/asr/abs_asr_model.py
+++ b/espnet_onnx/asr/abs_asr_model.py
@@ -11,6 +11,7 @@ from espnet_onnx.asr.scorer.length_bonus import LengthBonus
 from espnet_onnx.asr.scorer.interface import BatchScorerInterface
 from espnet_onnx.asr.beam_search.beam_search import BeamSearch
 from espnet_onnx.asr.beam_search.batch_beam_search import BatchBeamSearch
+from espnet_onnx.asr.beam_search.multi_batch_beam_search import MultiBatchBeamSearch
 from espnet_onnx.asr.beam_search.beam_search_transducer import BeamSearchTransducer
 
 
@@ -20,7 +21,7 @@ class AbsASRModel(AbsModel):
             # check if quantized model config is defined.
             raise RuntimeError(
                 'Configuration for quantized model is not defined.')
-        
+
     def _build_beam_search(self, scorers, weights):
         if self.config.transducer.use_transducer_decoder:
             self.beam_search = BeamSearchTransducer(
@@ -41,35 +42,49 @@ class AbsASRModel(AbsModel):
                 if not isinstance(v, BatchScorerInterface)
             ]
             if len(non_batch) == 0:
-                self.beam_search.__class__ = BatchBeamSearch
-                logging.info("BatchBeamSearch implementation is selected.")
+                if not self.enable_multibatch:
+                    self.beam_search.__class__ = BatchBeamSearch
+                    logging.info("BatchBeamSearch implementation is selected.")
+                else:
+                    self.beam_search.__class__ = MultiBatchBeamSearch
+                    logging.info("MultiBatchBeamSearch implementation is selected.")
             else:
                 logging.warning(
                     f"As non-batch scorers {non_batch} are found, "
                     f"fall back to non-batch implementation."
                 )
-    
-    def _build_model(self, providers, use_quantized):
-        self.encoder = get_encoder(self.config.encoder, providers, use_quantized)
-        decoder = get_decoder(self.config.decoder, providers, use_quantized)
+
+    def _build_model(self, providers, use_quantized, device_id, only_use_encoder=False, only_use_decoder=False):
+        if only_use_encoder:
+            self.encoder = get_encoder(self.config.encoder, providers, use_quantized, device_id)
+            return self.encoder
+        decoder = get_decoder(self.config.decoder, providers, use_quantized, device_id)
         scorers = {'decoder': decoder}
         weights = {}
+        ctc = None
+        joint_network = None
         if not self.config.transducer.use_transducer_decoder:
-            ctc = CTCPrefixScorer(self.config.ctc, self.config.token.eos, providers, use_quantized)
-            scorers.update(
-                ctc=ctc,
-                length_bonus=LengthBonus(len(self.config.token.list))
-            )
-            weights.update(
-                decoder=self.config.weights.decoder,
-                ctc=self.config.weights.ctc,
-                length_bonus=self.config.weights.length_bonus,
-            )
+            use_ctc = self.config.ctc.use_ctc if 'use_ctc' in self.config.ctc.keys() else True
+            if use_ctc:
+                ctc = CTCPrefixScorer(self.config.ctc, self.config.token.eos, providers, use_quantized, device_id)
+                scorers.update(
+                    ctc=ctc,
+                    length_bonus=LengthBonus(len(self.config.token.list))
+                )
+                weights.update(
+                    decoder=self.config.weights.decoder,
+                    ctc=self.config.weights.ctc,
+                    length_bonus=self.config.weights.length_bonus,
+                )
+            else:
+                weights.update(
+                    decoder=self.config.weights.decoder,
+                )
         else:
-            joint_network = JointNetwork(self.config.joint_network, providers, use_quantized)
+            joint_network = JointNetwork(self.config.joint_network, providers, use_quantized, device_id)
             scorers.update(joint_network=joint_network)
-            
-        lm = get_lm(self.config, providers, use_quantized)
+
+        lm = get_lm(self.config, providers, use_quantized, device_id)
         if lm is not None:
             scorers.update(lm=lm)
             weights.update(lm=self.config.weights.lm)
@@ -79,3 +94,6 @@ class AbsASRModel(AbsModel):
         self._build_token_converter()
         self.scorers = scorers
         self.weights = weights
+        if only_use_decoder:
+            return decoder, ctc, joint_network, lm
+        return self.encoder, decoder, ctc, joint_network, lm
diff --git a/espnet_onnx/asr/asr_model.py b/espnet_onnx/asr/asr_model.py
index 93e7de3..3386ff8 100644
--- a/espnet_onnx/asr/asr_model.py
+++ b/espnet_onnx/asr/asr_model.py
@@ -22,22 +22,34 @@ class Speech2Text(AbsASRModel):
                  model_dir: Union[Path, str] = None,
                  providers: List[str] = ['CPUExecutionProvider'],
                  use_quantized: bool = False,
+                 device_id: int = 0,
+                 only_use_encoder: bool = False,
+                 only_use_decoder: bool = False,
+                 enable_multibatch: bool = False
                  ):
         assert check_argument_types()
         self._check_argument(tag_name, model_dir)
         self._load_config()
-        
+
         # check onnxruntime version and providers
-        self._check_ort_version(providers)
-        
+        # add npu, we don,t check
+        #self._check_ort_version(providers)
         # check if model is exported for streaming.
+        self.only_use_encoder = only_use_encoder
+        self.only_use_decoder = only_use_decoder
+        self.enable_multibatch = enable_multibatch
         if self.config.encoder.enc_type == 'ContextualXformerEncoder':
             raise RuntimeError('Onnx model is built for streaming. Use StreamingSpeech2Text instead.')
 
         # check quantize and optimize model
         self._check_flags(use_quantized)
-        self._build_model(providers, use_quantized)
-        
+        if not only_use_encoder and not only_use_decoder:
+            self.encoder_m, self.decoder_m, self.ctc_m, self.joint_network_m, self.lm_m = self._build_model(providers, use_quantized, device_id)
+        elif only_use_encoder:
+            self.encoder_m = self._build_model(providers, use_quantized, device_id, only_use_encoder=True)
+        elif only_use_decoder:
+            self.decoder_m, self.ctc_m, self.joint_network_m, self.lm_m = self._build_model(providers, use_quantized, device_id, only_use_decoder=True)
+
         if self.config.transducer.use_transducer_decoder:
             self.start_idx = 1
             self.last_idx = None
@@ -45,7 +57,6 @@ class Speech2Text(AbsASRModel):
             self.start_idx = 1
             self.last_idx = -1
 
-
     def __call__(self, speech: np.ndarray) -> List[
         Tuple[
             Optional[str],
@@ -60,24 +71,37 @@ class Speech2Text(AbsASRModel):
         Returns:
             text, token, token_int, hyp
         """
-        assert check_argument_types()
+        # assert check_argument_types()
 
         # check dtype
-        if speech.dtype != np.float32:
-            speech = speech.astype(np.float32)
+        # if speech.dtype != np.float32:
+        #     speech = speech.astype(np.float32)
+
+        if not self.only_use_decoder:
+            if isinstance(speech, np.ndarray):
+                speech = speech[np.newaxis, :]
+                lengths = np.array([speech.shape[1]]).astype("int64")
+                enc, _ = self.encoder(speech=speech, speech_length=lengths)
+            else:
+                # support for multibatch
+                enc, _ = self.encoder(speech)
 
-        # data: (Nsamples,) -> (1, Nsamples)
-        speech = speech[np.newaxis, :]
-        # lengths: (1,)
-        lengths = np.array([speech.shape[1]]).astype(np.int64)
+        if self.only_use_encoder:
+            return enc
 
-        # b. Forward Encoder
-        enc, _ = self.encoder(speech=speech, speech_length=lengths)
-        if isinstance(enc, tuple):
-            enc = enc[0]
-        assert len(enc) == 1, len(enc)
+        if not self.only_use_decoder:
+            if isinstance(enc, tuple):
+                enc = enc[0]
+            assert len(enc) == 1, len(enc)
 
-        nbest_hyps = self.beam_search(enc[0])[:1]
+            nbest_hyps = self.beam_search(enc[0])[:1]
+        else:
+            enc = speech
+            init_batch = enc.shape[0]
+            nbest_hyps_list = self.beam_search(enc)
+            nbest_hyps = []
+            for batch_idx in range(init_batch):
+                nbest_hyps.append(nbest_hyps_list[batch_idx][0])
 
         results = []
         for hyp in nbest_hyps:
@@ -86,7 +110,7 @@ class Speech2Text(AbsASRModel):
                 token_int = list(hyp.yseq[self.start_idx : self.last_idx])
             else:
                 token_int = list(hyp.yseq[self.start_idx:])
-                
+
             # remove blank symbol id, which is assumed to be 0
             token_int = list([int(i) for i in filter(lambda x: x != 0, token_int)])
 
diff --git a/espnet_onnx/asr/beam_search/batch_beam_search.py b/espnet_onnx/asr/beam_search/batch_beam_search.py
index 87ee562..4a6716b 100644
--- a/espnet_onnx/asr/beam_search/batch_beam_search.py
+++ b/espnet_onnx/asr/beam_search/batch_beam_search.py
@@ -76,7 +76,7 @@ class BatchBeamSearch(BeamSearch):
         ]
 
     def batch_beam(
-        self, weighted_scores: np.ndarray, ids: np.ndarray
+            self, weighted_scores: np.ndarray, ids: np.ndarray,
     ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
         """Batch-compute topk full token ids and partial token ids.
         Args:
@@ -102,6 +102,7 @@ class BatchBeamSearch(BeamSearch):
         Returns:
             Hypothesis: The initial hypothesis.
         """
+        batch_num = x.shape[0]
         init_states = dict()
         init_scores = dict()
         for k, d in self.scorers.items():
@@ -115,7 +116,7 @@ class BatchBeamSearch(BeamSearch):
                     states=init_states,
                     yseq=np.array([self.sos], dtype=np.int64),
                 )
-            ]
+            ] * batch_num
         )
 
     def score_full(
@@ -252,11 +253,12 @@ class BatchBeamSearch(BeamSearch):
         return self.batchfy(best_hyps)
 
     def post_process(
-        self,
-        i: int,
-        maxlen: int,
-        running_hyps: BatchHypothesis,
-        ended_hyps: List[Hypothesis],
+            self,
+            i: int,
+            maxlen: int,
+            running_hyps: BatchHypothesis,
+            ended_hyps: List[Hypothesis],
+            keep_ori_hyps: bool = False
     ) -> BatchHypothesis:
         """Perform post-processing of beam search iterations.
         Args:
@@ -264,6 +266,7 @@ class BatchBeamSearch(BeamSearch):
             maxlen (int): The maximum length of tokens in beam search.
             running_hyps (BatchHypothesis): The running hypotheses in beam search.
             ended_hyps (List[Hypothesis]): The ended hypotheses in beam search.
+            keep_ori_hyps (Bool): whether to keep ori hyps
         Returns:
             BatchHypothesis: The new running hypotheses.
         """
@@ -308,4 +311,6 @@ class BatchBeamSearch(BeamSearch):
             hyp = self._select(running_hyps, b)
             ended_hyps.append(hyp)
         remained_ids = np.transpose(np.nonzero(is_eos == 0)).reshape(-1)
+        if keep_ori_hyps:
+            return self._batch_select(running_hyps, remained_ids), running_hyps
         return self._batch_select(running_hyps, remained_ids)
diff --git a/espnet_onnx/asr/model/decoder.py b/espnet_onnx/asr/model/decoder.py
index 9090203..e1c5966 100644
--- a/espnet_onnx/asr/model/decoder.py
+++ b/espnet_onnx/asr/model/decoder.py
@@ -6,10 +6,10 @@ from espnet_onnx.asr.model.decoders.xformer import XformerDecoder
 from espnet_onnx.asr.model.decoders.transducer import TransducerDecoder
 
 
-def get_decoder(config: Config, providers: List[str], use_quantized: bool = False):
+def get_decoder(config: Config, providers: List[str], use_quantized: bool = False, device_id: int = 0):
     if config.dec_type == 'RNNDecoder':
         return RNNDecoder(config, providers, use_quantized)
     elif config.dec_type == 'TransducerDecoder':
-        return TransducerDecoder(config, providers, use_quantized)
+        return TransducerDecoder(config, providers, use_quantized, device_id)
     else:
-        return XformerDecoder(config, providers, use_quantized)
+        return XformerDecoder(config, providers, use_quantized, device_id)
diff --git a/espnet_onnx/asr/model/decoders/transducer.py b/espnet_onnx/asr/model/decoders/transducer.py
index ca8240a..8af8702 100644
--- a/espnet_onnx/asr/model/decoders/transducer.py
+++ b/espnet_onnx/asr/model/decoders/transducer.py
@@ -15,14 +15,15 @@ from espnet_onnx.asr.beam_search.hyps import (
     ExtendedHypothesis
 )
 from espnet_onnx.utils.config import Config
-
+from pyacl.acl_infer import AclNet
 
 class TransducerDecoder(BatchScorerInterface):
     def __init__(
         self,
         config: Config,
         providers: List[str],
-        use_quantized: bool = False
+        use_quantized: bool = False,
+        device_id: int = 0
     ):
         """Onnx support for espnet2.asr.decoder.transformer_decoder
 
@@ -30,16 +31,30 @@ class TransducerDecoder(BatchScorerInterface):
             config (Config):
             use_quantized (bool): Flag to use quantized model
         """
-        if use_quantized:
-            self.decoder = onnxruntime.InferenceSession(
-                config.quantized_model_path,
-                providers=providers
-            )
+        if providers[0] == 'NPUExecutionProvider':
+            input_size = config.input_size
+            if not isinstance(input_size, int):
+                input_size = input_size.split(',')
+                input_size = list(map(int, input_size))
+            output_size = config.output_size
+            if not isinstance(output_size, int):
+                output_size = output_size.split(',')
+                output_size = list(map(int, output_size))
+            self.decoder = AclNet(model_path=config.model_path, output_data_shape=output_size, \
+                input_data_shape=input_size, device_id=device_id)
+            self.providers = 'NPU'
         else:
-            self.decoder = onnxruntime.InferenceSession(
-                config.model_path,
-                providers=providers
-            )
+            self.providers = 'XPU'
+            if use_quantized:
+                self.decoder = onnxruntime.InferenceSession(
+                    config.quantized_model_path,
+                    providers=providers
+                )
+            else:
+                self.decoder = onnxruntime.InferenceSession(
+                    config.model_path,
+                    providers=providers
+                )
         self.n_layers = config.n_layers
         self.odim = config.odim
         self.dtype = config.dtype
@@ -70,8 +85,13 @@ class TransducerDecoder(BatchScorerInterface):
             dec_out, dec_state = cache[str_labels]
         else:
             input_dict = self.get_input_dict(label, hyp.dec_state)
-            dec_out, * \
-                next_states = self.decoder.run(self.output_names, input_dict)
+            if self.providers == "NPU":
+                
+                outputs, exe_t = self.decoder(list(input_dict.values()))
+                dec_out, *next_states = outputs
+            else:
+                dec_out, * \
+                    next_states = self.decoder.run(self.output_names, input_dict)
             dec_state = self.split(next_states)
             cache[str_labels] = (dec_out, dec_state)
 
diff --git a/espnet_onnx/asr/model/decoders/xformer.py b/espnet_onnx/asr/model/decoders/xformer.py
index b8ccd20..274d484 100644
--- a/espnet_onnx/asr/model/decoders/xformer.py
+++ b/espnet_onnx/asr/model/decoders/xformer.py
@@ -11,7 +11,7 @@ import onnxruntime
 
 from espnet_onnx.asr.scorer.interface import BatchScorerInterface
 from espnet_onnx.utils.config import Config
-
+from ais_bench.infer.interface import InferSession
 
 class XformerDecoder(BatchScorerInterface):
     def __init__(
@@ -19,6 +19,7 @@ class XformerDecoder(BatchScorerInterface):
         config: Config,
         providers: List[str],
         use_quantized: bool = False,
+        device_id: int = 0
     ):
         """Onnx support for espnet2.asr.decoder.transformer_decoder
 
@@ -26,23 +27,28 @@ class XformerDecoder(BatchScorerInterface):
             config (Config):
             use_quantized (bool): Flag to use quantized model
         """
-        if use_quantized:
-            self.decoder = onnxruntime.InferenceSession(
-                config.quantized_model_path,
-                providers=providers,
-            )
+        if providers[0] == "NPUExecutionProvider":
+            self.decoder = InferSession(device_id, config.model_path)
+            self.providers = 'NPU'
         else:
-            self.decoder = onnxruntime.InferenceSession(
-                config.model_path,
-                providers=providers,
-            )
-        self.config = config
-        self.n_layers = config.n_layers
-        self.odim = config.odim
+            self.providers = 'XPU'
+            if use_quantized:
+                self.decoder = onnxruntime.InferenceSession(
+                    config.quantized_model_path,
+                    providers=providers,
+                )
+            else:
+                self.decoder = onnxruntime.InferenceSession(
+                    config.model_path,
+                    providers=providers,
+                )
         self.in_caches = [d.name for d in self.decoder.get_inputs()
                           if 'cache' in d.name]
         self.out_caches = [d.name for d in self.decoder.get_outputs()
                            if 'cache' in d.name]
+        self.config = config
+        self.n_layers = config.n_layers
+        self.odim = config.odim
 
     def batch_score(
         self, ys: np.ndarray, states: List[Any], xs: np.ndarray
@@ -65,7 +71,7 @@ class XformerDecoder(BatchScorerInterface):
         n_batch = len(ys)
         if states[0] is None:
             batch_state = [
-                np.zeros((1, 1, self.odim), dtype=np.float32)
+                np.zeros((n_batch, 1, self.odim), dtype=np.float32)
                 for _ in range(self.n_layers)
             ]
         else:
@@ -77,11 +83,16 @@ class XformerDecoder(BatchScorerInterface):
 
         # batch decoding
         input_dict = self.get_input_dict(ys, xs, batch_state)
-        
-        logp, *states = self.decoder.run(
-            ['y'] + self.out_caches,
-            input_dict
-        )
+        if self.providers == "NPU":
+            outputs = self.decoder.infer(list(input_dict.values()),
+                                         mode="dymshape",
+                                         custom_sizes=self.config.output_size)
+            logp, *states = outputs
+        else:
+            logp, *states = self.decoder.run(
+                ['y'] + self.out_caches,
+                input_dict
+            )
 
         if type(self.n_layers) == 1:
             states = [states]
@@ -93,7 +104,10 @@ class XformerDecoder(BatchScorerInterface):
         return logp, state_list
 
     def get_input_dict(self, ys, xs, state):
-        in_names = [d.name for d in self.decoder.get_inputs() if 'cache' not in d.name]
+        if self.providers == "NPU":
+            in_names = [d.name for d in self.decoder.get_inputs() if 'cache' not in d.name]
+        else:
+            in_names = [d.name for d in self.decoder.get_inputs() if 'cache' not in d.name]
         ret = {}
         if 'tgt' in in_names: ret.update(tgt=ys.astype(np.int64))
         if 'memory' in in_names: ret.update(memory=xs)
diff --git a/espnet_onnx/asr/model/encoder.py b/espnet_onnx/asr/model/encoder.py
index 3a2cb40..0854ad8 100644
--- a/espnet_onnx/asr/model/encoder.py
+++ b/espnet_onnx/asr/model/encoder.py
@@ -4,8 +4,8 @@ from espnet_onnx.asr.model.encoders.encoder import Encoder
 from espnet_onnx.asr.model.encoders.streaming import StreamingEncoder
 
 
-def get_encoder(config: Config, providers: List[str], use_quantized: bool = False):
+def get_encoder(config: Config, providers: List[str], use_quantized: bool = False, device_id: int = 0):
     if config.enc_type == 'ContextualXformerEncoder':
         return StreamingEncoder(config, providers, use_quantized)
     else:
-        return Encoder(config, providers, use_quantized)
+        return Encoder(config, providers, use_quantized, device_id)
diff --git a/espnet_onnx/asr/model/encoders/encoder.py b/espnet_onnx/asr/model/encoders/encoder.py
index bb52644..a95fd35 100644
--- a/espnet_onnx/asr/model/encoders/encoder.py
+++ b/espnet_onnx/asr/model/encoders/encoder.py
@@ -13,28 +13,37 @@ from espnet_onnx.utils.function import (
     mask_fill
 )
 from espnet_onnx.utils.config import Config
+from ais_bench.infer.interface import InferSession
 
 
 class Encoder:
     def __init__(
-        self,
-        encoder_config: Config,
-        providers: List[str],
-        use_quantized: bool = False,
+            self,
+            encoder_config: Config,
+            providers: List[str],
+            use_quantized: bool = False,
+            device_id: int = 0,
+            rank_mode: bool = True
     ):
+        self.rank_mode = rank_mode
         self.config = encoder_config
         # Note that id model was optimized and quantized,
         # then the quantized model should be optimized.
-        if use_quantized:
-            self.encoder = onnxruntime.InferenceSession(
-                self.config.quantized_model_path,
-                providers=providers
-            )
+        if providers[0] == 'NPUExecutionProvider':
+            self.encoder = InferSession(device_id, self.config.model_path)
+            self.providers = 'NPU'
         else:
-            self.encoder = onnxruntime.InferenceSession(
-                self.config.model_path,
-                providers=providers
-            )
+            self.providers = 'XPU'
+            if use_quantized:
+                self.encoder = onnxruntime.InferenceSession(
+                    self.config.quantized_model_path,
+                    providers=providers
+                )
+            else:
+                self.encoder = onnxruntime.InferenceSession(
+                    self.config.model_path,
+                    providers=providers
+                )
 
         self.frontend = Frontend(self.config.frontend, providers, use_quantized)
         if self.config.do_normalize:
@@ -49,26 +58,30 @@ class Encoder:
         # if self.config.do_postencoder:
         #     self.postencoder = Postencoder(self.config.postencoder)
 
+
     def __call__(
-        self, speech: np.ndarray, speech_length: np.ndarray
+        self, speech, speech_length = None
     ) -> Tuple[np.ndarray, np.ndarray]:
         """Frontend + Encoder. Note that this method is used by asr_inference.py
         Args:
             speech: (Batch, Length, ...)
             speech_lengths: (Batch, )
         """
-        # 1. Extract feature
-        feats, feat_length = self.frontend(speech, speech_length)
+        if isinstance(speech, list):
+            # 1. Extract feature
+            feats, feat_length = self.frontend(speech, speech_length)
 
-        # 2. normalize with global MVN
-        if self.config.do_normalize:
-            feats, feat_length = self.normalize(feats, feat_length)
+            # 2. normalize with global MVN
+            if self.config.do_normalize:
+                feats, feat_length = self.normalize(feats, feat_length)
 
-        # 3. forward encoder
-        encoder_out, encoder_out_lens = \
-            self.forward_encoder(feats, feat_length)
-        encoder_out = self.mask_output(encoder_out, encoder_out_lens)
+            # 3. forward encoder
+            encoder_out, encoder_out_lens = \
+                self.forward_encoder(feats, feat_length)
 
+        else:
+            encoder_out, encoder_out_lens = self.forward_encoder(speech)
+        encoder_out = self.mask_output(encoder_out, encoder_out_lens)
         # if self.config.do_postencoder:
         #     encoder_out, encoder_out_lens = self.postencoder(
         #         encoder_out, encoder_out_lens
@@ -81,12 +94,32 @@ class Encoder:
             feats = mask_fill(feats, make_pad_mask(feat_length, feats, 1), 0.0)
         return feats, feat_length
 
-    def forward_encoder(self, feats, feat_length):
-        encoder_out, encoder_out_lens = \
-            self.encoder.run(["encoder_out", "encoder_out_lens"], {
-                "feats": feats
-            })
-        
+    def forward_encoder(self, datas, feat_length=None):
+        if self.providers == "NPU":
+            if len(datas) == 2:
+                # dynamic batch mode
+                feats, feat_length = datas
+                outputs = self.encoder.infer([feats],
+                                             mode="dymshape",
+                                             custom_sizes=self.config.output_size)
+                encoder_out, encoder_out_lens = outputs
+            else:
+                # multi batch mode
+                feats, feat_length, mask, pos_mask, conv_mask, encoder_out_lens = datas
+                if self.rank_mode:
+                    mode = 'dymdims'
+                else:
+                    mode = 'dymshape'
+                outputs = self.encoder.infer([feats, mask, pos_mask, conv_mask],
+                                             mode=mode,
+                                             custom_sizes=self.config.output_size)
+                encoder_out = outputs[0]
+        else:
+            encoder_out, encoder_out_lens = \
+                self.encoder.run(["encoder_out", "encoder_out_lens"], {
+                    "feats": feats
+                })
+
         if self.config.enc_type == 'RNNEncoder':
             encoder_out = mask_fill(encoder_out, make_pad_mask(
                 feat_length, encoder_out, 1), 0.0)
diff --git a/espnet_onnx/asr/model/joint_network.py b/espnet_onnx/asr/model/joint_network.py
index ce17fd3..11e5ac4 100644
--- a/espnet_onnx/asr/model/joint_network.py
+++ b/espnet_onnx/asr/model/joint_network.py
@@ -1,6 +1,7 @@
 from typing import List
 
 import onnxruntime
+from pyacl.acl_infer import AclNet
 
 
 class JointNetwork:
@@ -8,22 +9,41 @@ class JointNetwork:
         self,
         config,
         providers: List[str],
-        use_quantized=False
+        use_quantized=False,
+        device_id=0
     ):
-        if use_quantized:
-            self.joint_session = onnxruntime.InferenceSession(
-                config.quantized_model_path,
-                providers=providers
-            )
+        if providers[0] == 'NPUExecutionProvider':
+            input_size = config.input_size
+            if not isinstance(input_size, int):
+                input_size = input_size.split(',')
+                input_size = list(map(int, input_size))
+            output_size = config.output_size
+            if not isinstance(output_size, int):
+                output_size = output_size.split(',')
+                output_size = list(map(int, output_size))
+            self.joint_session = AclNet(model_path=config.model_path, output_data_shape=output_size, \
+                input_data_shape=input_size, device_id=device_id)
+            self.providers = 'NPU'
         else:
-            self.joint_session = onnxruntime.InferenceSession(
-                config.model_path,
-                providers=providers
-            )
+            self.providers = 'XPU'
+            if use_quantized:
+                self.joint_session = onnxruntime.InferenceSession(
+                    config.quantized_model_path,
+                    providers=providers
+                )
+            else:
+                self.joint_session = onnxruntime.InferenceSession(
+                    config.model_path,
+                    providers=providers
+                )
 
     def __call__(self, enc_out, dec_out):
         input_dict = {
             'enc_out': enc_out,
             'dec_out': dec_out
         }
-        return self.joint_session.run(['joint_out'], input_dict)[0]
+        if self.providers == 'NPU':
+            outputs, exe_t = self.joint_session([enc_out, dec_out])
+            return outputs[0]
+        else:
+            return self.joint_session.run(['joint_out'], input_dict)[0]
diff --git a/espnet_onnx/asr/model/lm.py b/espnet_onnx/asr/model/lm.py
index 8d38733..8db2646 100644
--- a/espnet_onnx/asr/model/lm.py
+++ b/espnet_onnx/asr/model/lm.py
@@ -4,10 +4,10 @@ from espnet_onnx.asr.model.lms.seqrnn_lm import SequentialRNNLM
 from espnet_onnx.asr.model.lms.transformer_lm import TransformerLM
 
 
-def get_lm(config: Config, providers: List[str], use_quantized: bool = False):
+def get_lm(config: Config, providers: List[str], use_quantized: bool = False, device_id: int = 0):
     if config.lm.use_lm:
         if config.lm.lm_type == 'SequentialRNNLM':
-            return SequentialRNNLM(config.lm, providers, use_quantized)
+            return SequentialRNNLM(config.lm, providers, use_quantized, device_id)
         elif config.lm.lm_type == 'TransformerLM':
-            return TransformerLM(config.lm, providers, use_quantized)
+            return TransformerLM(config.lm, providers, use_quantized, device_id)
     return None
diff --git a/espnet_onnx/asr/model/lms/seqrnn_lm.py b/espnet_onnx/asr/model/lms/seqrnn_lm.py
index aea2583..a39271d 100644
--- a/espnet_onnx/asr/model/lms/seqrnn_lm.py
+++ b/espnet_onnx/asr/model/lms/seqrnn_lm.py
@@ -10,7 +10,7 @@ from scipy.special import log_softmax
 
 from espnet_onnx.asr.scorer.interface import BatchScorerInterface
 from espnet_onnx.utils.config import Config
-
+from pyacl.acl_infer import AclNet
 
 class SequentialRNNLM(BatchScorerInterface):
     """Sequential RNNLM.
@@ -24,16 +24,30 @@ class SequentialRNNLM(BatchScorerInterface):
         providers: List[str],
         use_quantized: bool = False
     ):
-        if use_quantized:
-            self.lm_session = onnxruntime.InferenceSession(
-                config.quantized_model_path,
-                providers=providers
-            )
+        if providers[0] == 'NPUExecutionProvider':
+            input_size = config.input_size
+            if not isinstance(input_size, int):
+                input_size = input_size.split(',')
+                input_size = list(map(int, input_size))
+            output_size = config.output_size
+            if not isinstance(output_size, int):
+                output_size = output_size.split(',')
+                output_size = list(map(int, output_size))
+            self.lm_session = AclNet(model_path=config.model_path, output_data_shape=output_size, \
+                input_data_shape=input_size, device_id=device_id)
+            self.providers = 'NPU'
         else:
-            self.lm_session = onnxruntime.InferenceSession(
-                config.model_path,
-                providers=providers
-            )
+            self.providers = 'XPU'
+            if use_quantized:
+                self.lm_session = onnxruntime.InferenceSession(
+                    config.quantized_model_path,
+                    providers=providers
+                )
+            else:
+                self.lm_session = onnxruntime.InferenceSession(
+                    config.model_path,
+                    providers=providers
+                )
         self.enc_output_names = ['y'] \
             + [d.name for d in self.lm_session.get_outputs() if 'hidden' in d.name]
         self.enc_in_cache_names = [
@@ -81,13 +95,19 @@ class SequentialRNNLM(BatchScorerInterface):
         if state is None:
             state = self.create_cache()
 
-        input_dic.update({
-            k: v for k, v in zip(self.enc_in_cache_names, state)
-        })
-        decoded, *new_state = self.lm_session.run(
-            self.enc_output_names,
-            input_dic
-        )
+        if self.providers == "NPU":
+            state = list(state)
+            state.insert(0, input_dic['x'])
+            outputs, exe_t= self.lm_session(state)
+            decoded, *new_state = outputs
+        else:
+            input_dic.update({
+                k: v for k, v in zip(self.enc_in_cache_names, state)
+            })
+            decoded, *new_state = self.lm_session.run(
+                self.enc_output_names,
+                input_dic
+            )
         logp = log_softmax(decoded, axis=-1).reshape(-1)
         return logp, new_state
 
@@ -119,13 +139,19 @@ class SequentialRNNLM(BatchScorerInterface):
             states = np.concatenate([states[:, None] for s in states], axis=1)
 
         input_dic = {'x': ys[:, -1:].astype(np.int64)}
-        input_dic.update({
-            k: v for k, v in zip(self.enc_in_cache_names, states)
-        })
-        decoded, *new_states = self.lm_session.run(
-            self.enc_output_names,
-            input_dic
-        )
+        if self.providers == "NPU":
+            states = list(states)
+            states.insert(0, input_dic['x'])
+            outputs, exe_t = self.lm_session(states)
+            decoded, *new_states = outputs
+        else:
+            input_dic.update({
+                k: v for k, v in zip(self.enc_in_cache_names, states)
+            })
+            decoded, *new_states = self.lm_session.run(
+                self.enc_output_names,
+                input_dic
+            )
         decoded = decoded.squeeze(1)
         logp = log_softmax(decoded, axis=-1)
         # state: Change to batch first
diff --git a/espnet_onnx/asr/model/lms/transformer_lm.py b/espnet_onnx/asr/model/lms/transformer_lm.py
index a5bc200..6e41f81 100644
--- a/espnet_onnx/asr/model/lms/transformer_lm.py
+++ b/espnet_onnx/asr/model/lms/transformer_lm.py
@@ -8,25 +8,32 @@ import onnxruntime
 from scipy.special import log_softmax
 
 from espnet_onnx.asr.scorer.interface import BatchScorerInterface
-
+from ais_bench.infer.interface import InferSession
 
 class TransformerLM(BatchScorerInterface):
     def __init__(
         self,
         config,
         providers: List[str],
-        use_quantized: bool =False,
+        use_quantized: bool = False,
+        device_id: int = 0
     ):
-        if use_quantized:
-            self.lm_session = onnxruntime.InferenceSession(
-                config.quantized_model_path,
-                providers=providers
-            )
+        self.config = config
+        if providers[0] == 'NPUExecutionProvider':
+            self.lm_session = InferSession(device_id, config.model_path)
+            self.providers = 'NPU'
         else:
-            self.lm_session = onnxruntime.InferenceSession(
-                config.model_path,
-                providers=providers
-            )
+            self.providers = 'XPU'
+            if use_quantized:
+                self.lm_session = onnxruntime.InferenceSession(
+                    config.quantized_model_path,
+                    providers=providers
+                )
+            else:
+                self.lm_session = onnxruntime.InferenceSession(
+                    config.model_path,
+                    providers=providers
+                )
             
         self.enc_output_names = ['y'] \
             + [d.name for d in self.lm_session.get_outputs() if 'cache' in d.name]
@@ -35,7 +42,6 @@ class TransformerLM(BatchScorerInterface):
 
         self.nlayers = config.nlayers
         self.odim = config.odim
-
     def score(
         self, y: np.ndarray, state: Any, x: np.ndarray
     ) -> Tuple[np.ndarray, Any]:
@@ -57,17 +63,23 @@ class TransformerLM(BatchScorerInterface):
 
         if state is None:
             state = [
-                np.zeros((1, 1, self.odim), dtype=np.float32)
+                np.zeros((y.shape[0], 1, self.odim), dtype=np.float32)
                 for _ in range(self.nlayers)
             ]
 
         input_dic.update({
             k: v for k, v in zip(self.enc_in_cache_names, state)
         })
-        decoded, *new_state = self.lm_session.run(
-            self.enc_output_names,
-            input_dic
-        )
+        if self.providers == "NPU":
+            outputs = self.lm_session.infer(list(input_dic.values()),
+                                            mode="dymshape",
+                                            custom_sizes=self.config.output_size)
+            decoded, *new_state = outputs
+        else:
+            decoded, *new_state = self.lm_session.run(
+                self.enc_output_names,
+                input_dic
+            )
 
         if self.nlayers == 1:
             new_state = [new_state]
@@ -75,6 +87,7 @@ class TransformerLM(BatchScorerInterface):
         logp = log_softmax(decoded, axis=-1).squeeze(0)
         return logp, new_state
 
+
     def batch_score(
         self, ys: np.ndarray, states: List[Any], xs: np.ndarray
     ) -> Tuple[np.ndarray, List[Any]]:
@@ -98,7 +111,7 @@ class TransformerLM(BatchScorerInterface):
         is_first_iteration = False
         if states[0] is None:
             batch_state = [
-                np.zeros((1, 1, self.odim), dtype=np.float32)
+                np.zeros((ys.shape[0], 1, self.odim), dtype=np.float32)
                 for _ in range(self.nlayers)
             ]
             is_first_iteration = True
@@ -113,10 +126,16 @@ class TransformerLM(BatchScorerInterface):
         input_dic.update({
             k: v for k, v in zip(self.enc_in_cache_names, batch_state)
         })
-        decoded, *new_state = self.lm_session.run(
-            self.enc_output_names,
-            input_dic
-        )
+        if self.providers == "NPU":
+            outputs = self.lm_session.infer(list(input_dic.values()),
+                                            mode="dymshape",
+                                            custom_sizes=self.config.output_size)
+            decoded, *new_state = outputs
+        else:
+            decoded, *new_state = self.lm_session.run(
+                self.enc_output_names,
+                input_dic
+            )
         logp = log_softmax(decoded, axis=-1)
 
         # if first iteration, remove the first row
diff --git a/espnet_onnx/asr/scorer/ctc_prefix_scorer.py b/espnet_onnx/asr/scorer/ctc_prefix_scorer.py
index c6d094e..71046e2 100644
--- a/espnet_onnx/asr/scorer/ctc_prefix_scorer.py
+++ b/espnet_onnx/asr/scorer/ctc_prefix_scorer.py
@@ -10,6 +10,7 @@ from scipy.special import (
 
 from espnet_onnx.utils.config import Config
 from .interface import BatchPartialScorerInterface
+from ais_bench.infer.interface import InferSession
 
 
 class CTCPrefixScore:
@@ -105,24 +106,30 @@ class CTCPrefixScore:
 class CTCPrefixScorer(BatchPartialScorerInterface):
     """Decoder interface wrapper for CTCPrefixScore."""
 
-    def __init__(self, ctc: Config, eos: int, providers: List[str], use_quantized: bool = False):
+    def __init__(self, ctc: Config, eos: int, providers: List[str], use_quantized: bool = False, device_id: int = 0):
         """Initialize class.
         Args:
             ctc (np.ndarray): The CTC implementation.
                 For example, :class:`espnet.nets.pytorch_backend.ctc.CTC`
             eos (int): The end-of-sequence id.
         """
+        self.config = ctc
         assert check_argument_types()
-        if use_quantized:
-            self.ctc = onnxruntime.InferenceSession(
-                ctc.quantized_model_path,
-                providers=providers
-            )
+        if providers[0] == 'NPUExecutionProvider':
+            self.ctc = InferSession(device_id, ctc.model_path)
+            self.providers = 'NPU'
         else:
-            self.ctc = onnxruntime.InferenceSession(
-                ctc.model_path,
-                providers=providers
-            )
+            self.providers = 'XPU'
+            if use_quantized:
+                self.ctc = onnxruntime.InferenceSession(
+                    ctc.quantized_model_path,
+                    providers=providers
+                )
+            else:
+                self.ctc = onnxruntime.InferenceSession(
+                    ctc.model_path,
+                    providers=providers
+                )
         self.eos = eos
         self.impl = None
 
@@ -132,7 +139,13 @@ class CTCPrefixScorer(BatchPartialScorerInterface):
             x (np.ndarray): The encoded feature tensor
         Returns: initial state
         """
-        x = self.ctc.run(["ctc_out"], {"x": x[None, :]})[0]
+        if self.providers == "NPU":
+            x = self.ctc.infer([x[None, :].astype("float16")],
+                               mode="dymshape",
+                               custom_sizes=self.config.output_size)
+            x = x[0]
+        else:
+            x = self.ctc.run(["ctc_out"], {"x": x[None, :]})[0]
         logp = np.squeeze(x, axis=0)
         # TODO(karita): use CTCPrefixScoreTH
         self.impl = CTCPrefixScore(logp, 0, self.eos, np)
@@ -185,11 +198,35 @@ class CTCPrefixScorer(BatchPartialScorerInterface):
             x (np.ndarray): The encoded feature tensor
         Returns: initial state
         """
-        logp = self.ctc.run(["ctc_out"], {"x": x[None, :]})[0]
+        if self.providers == "NPU":
+            logp = self.ctc.infer([x[None, :]],
+                               mode="dymshape",
+                               custom_sizes=self.config.output_size)
+            logp = logp[0]
+        else:
+            logp = self.ctc.run(["ctc_out"], {"x": x[None, :]})[0]
         xlen = np.array([logp.shape[1]])
         self.impl = CTCPrefixScoreTH(logp, xlen, 0, self.eos)
         return None
 
+    def multi_batch_init_state(self, x: np.ndarray):
+        """Get an initial state for decoding, supported for multi batch.
+        Args:
+            x (np.ndarray): The encoded feature tensor
+        Returns: initial state
+        """
+        if self.providers == "NPU":
+            logp = self.ctc.infer([x.astype("float16")],
+                                  mode="dymshape",
+                                  custom_sizes=self.config.output_size)
+            logp = logp[0]
+        else:
+            logp = self.ctc.run(["ctc_out"], {"x": x})[0]
+        xlen = np.sum((x[:, :, 0] != 0), axis=-1)
+        xlen = np.expand_dims(xlen, axis=-1)
+        self.impl = CTCPrefixScoreTH(logp, np.squeeze(xlen), 0, self.eos)
+        return None
+
     def batch_score_partial(self, y, ids, state, x):
         """Score new token.
         Args:
@@ -212,7 +249,8 @@ class CTCPrefixScorer(BatchPartialScorerInterface):
         else:
             batch_state = None
         return self.impl(y, batch_state, ids)
-    
+
+
     def extend_prob(self, x: np.ndarray):
         """Extend probs for decoding.
 
@@ -223,7 +261,13 @@ class CTCPrefixScorer(BatchPartialScorerInterface):
             x (np.ndarray): The encoded feature tensor
 
         """
-        x = self.ctc.run(["ctc_out"], {"x": x[None, :]})[0]
+        if self.providers == "NPU":
+            x = self.ctc.infer([x[None, :].astype("float16")],
+                               mode="dymshape",
+                               custom_sizes=self.config.output_size)
+            x = x[0]
+        else:
+            x = self.ctc.run(["ctc_out"], {"x": x[None, :]})[0]
         logp = log_softmax(x, axis=-1)
         self.impl.extend_prob(logp)
 
@@ -299,7 +343,7 @@ class CTCPrefixScoreTH:
         self.idx_b = np.arange(self.batch)
         self.idx_bo = (self.idx_b * self.odim)[:, None]
 
-    def __call__(self, y, state, scoring_ids=None, att_w=None):
+    def __call__(self, y, state, scoring_ids=None, att_w=None, batch_idx=None):
         """Compute CTC prefix scores for next labels
         :param list y: prefix label sequences
         :param tuple state: previous CTC state
@@ -412,8 +456,12 @@ class CTCPrefixScoreTH:
                 axis=0,
             )
 
-        for si in range(n_bh):
-            log_psi[si, self.eos] = r_sum[self.end_frames[si // n_hyps], si]
+        if batch_idx is None:
+            for si in range(n_bh):
+                log_psi[si, self.eos] = r_sum[self.end_frames[si // n_hyps], si]
+        else:
+            for si in range(n_bh):
+                log_psi[si, self.eos] = r_sum[self.end_frames[batch_idx][si // n_hyps], si]
 
         # exclude blank probs
         log_psi[:, self.blank] = self.logzero
diff --git a/espnet_onnx/export/asr/export_asr.py b/espnet_onnx/export/asr/export_asr.py
index 187d687..ca537a8 100644
--- a/espnet_onnx/export/asr/export_asr.py
+++ b/espnet_onnx/export/asr/export_asr.py
@@ -226,7 +226,7 @@ class ASRModelExport:
             dummy_input,
             os.path.join(path, f'{model.model_name}.onnx'),
             verbose=verbose,
-            opset_version=15,
+            opset_version=11,
             input_names=model.get_input_names(),
             output_names=model.get_output_names(),
             dynamic_axes=model.get_dynamic_axes()
diff --git a/espnet_onnx/utils/torch_function.py b/espnet_onnx/utils/torch_function.py
index 7130ce6..9064822 100644
--- a/espnet_onnx/utils/torch_function.py
+++ b/espnet_onnx/utils/torch_function.py
@@ -53,6 +53,28 @@ def normalize(input: torch.Tensor, p: float = 2.0, dim: int = 1, out: Optional[t
         denom = input.norm(p, dim, keepdim=True).expand_as(input)
         return torch.div(input, denom, out=out)
 
+def triu_onnx(x, diagonal=0):
+    m = x.shape[0]
+    n = x.shape[1]
+    arange = torch.arange(n, device=x.device)
+    mask = arange.expand(m, n)
+    mask_maker = torch.arange(m, device=x.device).unsqueeze(-1)
+    if diagonal:
+        mask_maker = mask_maker + diagonal
+    mask = mask >= mask_maker
+    return mask * x
+
+def tril_onnx(x, diagonal=0):
+    m = x.shape[0]
+    n = x.shape[1]
+    arange = torch.arange(n, device=x.device)
+    mask = arange.expand(m, n)
+    mask_maker = torch.arange(m, device=x.device).unsqueeze(-1)
+    if diagonal:
+        mask_maker = mask_maker + diagonal
+    mask = mask <= mask_maker
+    return mask * x
 
 def subsequent_mask(size: torch.Tensor):
-    return torch.ones(size, size).tril()
+    #return torch.ones(size, size).tril()
+    return tril_onnx(torch.ones(size, size))
