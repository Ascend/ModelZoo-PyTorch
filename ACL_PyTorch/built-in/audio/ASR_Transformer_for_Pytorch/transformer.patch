diff --git a/speechbrain/core.py b/speechbrain/core.py
index c6b830578..79a9ff37e 100644
--- a/speechbrain/core.py
+++ b/speechbrain/core.py
@@ -288,7 +288,35 @@ def parse_arguments(arg_list=None):
         help="Enable colored progress-bar in tqdm. If this is "
         "false, tqdm shall use default colors.",
     )
-
+    parser.add_argument(
+        "--mode",
+        default='infer',
+        help="run mode, export onnx or infer",
+    )
+    parser.add_argument(
+        "--npu_rank",
+        default='1',
+        type=int,
+        help="choose the device id",
+    )
+    parser.add_argument(
+        "--encoder_file",
+        default='encoder.om',
+        type=str,
+        help="the path of om",
+    )
+    parser.add_argument(
+        "--decoder_file",
+        default='decoder.om',
+        type=str,
+        help="the path of om",
+    )   
+    parser.add_argument(
+        "--ctc_enable",
+        default='True',
+        type=bool,
+        help="enable CTC",
+    )   
     # Accept extra args to override yaml
     run_opts, overrides = parser.parse_known_args(arg_list)
 
@@ -1053,7 +1081,8 @@ class Brain:
 
         out = self.compute_forward(batch, stage=stage)
         loss = self.compute_objectives(out, batch, stage=stage)
-        return loss.detach().cpu()
+        # return loss.detach().cpu()
+        return loss
 
     def _fit_train(self, train_set, epoch, enable):
         # Training stage
@@ -1357,7 +1386,7 @@ class Brain:
             ):
                 self.step += 1
                 loss = self.evaluate_batch(batch, stage=Stage.TEST)
-                avg_test_loss = self.update_average(loss, avg_test_loss)
+                # avg_test_loss = self.update_average(loss, avg_test_loss)
 
                 # Profile only if desired (steps allow the profiler to know when all is warmed up)
                 if self.profiler is not None:
@@ -1373,7 +1402,8 @@ class Brain:
                 self.on_stage_end, args=[Stage.TEST, avg_test_loss, None]
             )
         self.step = 0
-        return avg_test_loss
+        # return avg_test_loss
+        return None
 
     def update_average(self, loss, avg_loss):
         """Update running average of the loss.
diff --git a/speechbrain/dataio/dataio.py b/speechbrain/dataio/dataio.py
index 4d55bbdb8..96dde5ce6 100644
--- a/speechbrain/dataio/dataio.py
+++ b/speechbrain/dataio/dataio.py
@@ -730,10 +730,10 @@ def length_to_mask(length, max_len=None, dtype=None, device=None):
     assert len(length.shape) == 1
 
     if max_len is None:
-        max_len = length.max().long().item()  # using arange to generate mask
+        max_len = torch.max(length, dim=0).values.long()  # using arange to generate mask
     mask = torch.arange(
         max_len, device=length.device, dtype=length.dtype
-    ).expand(len(length), max_len) < length.unsqueeze(1)
+    ).expand(length.shape[0], max_len) < length.unsqueeze(1)
 
     if dtype is None:
         dtype = length.dtype
diff --git a/speechbrain/decoders/seq2seq.py b/speechbrain/decoders/seq2seq.py
index da339c363..ba876d031 100644
--- a/speechbrain/decoders/seq2seq.py
+++ b/speechbrain/decoders/seq2seq.py
@@ -685,8 +685,10 @@ class S2SBeamSearcher(S2SBaseSearcher):
 
         return topk_hyps, topk_scores, topk_lengths, topk_log_probs
 
-    def forward(self, enc_states, wav_len):  # noqa: C901
+    def forward(self, decoder_sess_om, enc_states, wav_len, ctc=True):  # noqa: C901
         """Applies beamsearch and returns the predicted tokens."""
+        if not ctc:
+          self.ctc_weight = 0
         enc_lens = torch.round(enc_states.shape[1] * wav_len).int()
         device = enc_states.device
         batch_size = enc_states.shape[0]
@@ -766,7 +768,7 @@ class S2SBeamSearcher(S2SBaseSearcher):
                 break
 
             log_probs, memory, attn = self.forward_step(
-                inp_tokens, memory, enc_states, enc_lens
+                decoder_sess_om, inp_tokens, memory, enc_states, enc_lens
             )
             log_probs = self.att_weight * log_probs
 
@@ -819,7 +821,7 @@ class S2SBeamSearcher(S2SBaseSearcher):
                     g, ctc_memory, ctc_candidates, attn
                 )
                 log_probs = log_probs + self.ctc_weight * ctc_log_probs
-
+            
             scores = sequence_scores.unsqueeze(1).expand(-1, vocab_size)
             scores = scores + log_probs
 
@@ -1345,10 +1347,10 @@ class S2STransformerBeamSearch(S2SBeamSearcher):
         memory = torch.index_select(memory, dim=0, index=index)
         return memory
 
-    def forward_step(self, inp_tokens, memory, enc_states, enc_lens):
+    def forward_step(self, decoder_sess_om, inp_tokens, memory, enc_states, enc_lens):
         """Performs a step in the implemented beamsearcher."""
         memory = _update_mem(inp_tokens, memory)
-        pred, attn = self.model.decode(memory, enc_states)
+        pred, attn = self.model.decode(decoder_sess_om, memory, enc_states)
         prob_dist = self.softmax(self.fc(pred) / self.temperature)
         return prob_dist[:, -1, :], memory, attn
 
diff --git a/speechbrain/lobes/models/convolution.py b/speechbrain/lobes/models/convolution.py
index 7f5432708..35d27efe1 100644
--- a/speechbrain/lobes/models/convolution.py
+++ b/speechbrain/lobes/models/convolution.py
@@ -6,7 +6,7 @@ Authors
 import torch
 from speechbrain.nnet.CNN import Conv2d
 from speechbrain.nnet.containers import Sequential
-from speechbrain.nnet.normalization import LayerNorm
+from speechbrain.nnet.normalization import LayerNorm, BatchNorm2d
 
 
 class ConvolutionFrontEnd(Sequential):
@@ -57,7 +57,7 @@ class ConvolutionFrontEnd(Sequential):
         residuals=[True, True, True],
         conv_module=Conv2d,
         activation=torch.nn.LeakyReLU,
-        norm=LayerNorm,
+        norm=BatchNorm2d,
         dropout=0.1,
         conv_bias=True,
         padding="same",
diff --git a/speechbrain/lobes/models/transformer/Transformer.py b/speechbrain/lobes/models/transformer/Transformer.py
index 3913493bc..472967ec8 100644
--- a/speechbrain/lobes/models/transformer/Transformer.py
+++ b/speechbrain/lobes/models/transformer/Transformer.py
@@ -451,6 +451,7 @@ class TransformerEncoder(nn.Module):
         src_mask: Optional[torch.Tensor] = None,
         src_key_padding_mask: Optional[torch.Tensor] = None,
         pos_embs: Optional[torch.Tensor] = None,
+        attn_results: Optional[bool] = True
     ):
         """
         Arguments
@@ -483,8 +484,9 @@ class TransformerEncoder(nn.Module):
 
                 attention_lst.append(attention)
         output = self.norm(output)
-        return output, attention_lst
-
+        if attn_results:
+            return output, attention_lst
+        return output
 
 class TransformerDecoderLayer(nn.Module):
     """This class implements the self-attention decoder layer.
@@ -726,6 +728,7 @@ class TransformerDecoder(nn.Module):
         memory_key_padding_mask=None,
         pos_embs_tgt=None,
         pos_embs_src=None,
+        attn_results=True
     ):
         """
         Arguments
@@ -760,8 +763,9 @@ class TransformerDecoder(nn.Module):
             multihead_attns.append(multihead_attn)
         output = self.norm(output)
 
-        return output, self_attns, multihead_attns
-
+        if attn_results:
+            return output, self_attns, multihead_attns
+        return output
 
 class NormalizedEmbedding(nn.Module):
     """This class implements the normalized embedding layer for the transformer.
@@ -828,6 +832,17 @@ def get_key_padding_mask(padded_input, pad_idx):
 
     return key_padded_mask.detach()
 
+def triu_onnx(x, diagonal=0):
+    assert len(x.shape) == 2
+    m, l = x.shape
+    mask = torch.arange(l, device=x.device).expand(m, l)
+    # print('Transformer::839:mask', mask.shape)
+    arange = torch.arange(m, device=x.device)
+    arange = arange.unsqueeze(-1)
+    if diagonal:
+        arange = arange + diagonal
+    mask = mask >= arange
+    return x.masked_fill(mask==0, 0)
 
 def get_lookahead_mask(padded_input):
     """Creates a binary mask for each sequence which maskes future frames.
@@ -847,7 +862,7 @@ def get_lookahead_mask(padded_input):
     """
     seq_len = padded_input.shape[1]
     mask = (
-        torch.triu(torch.ones((seq_len, seq_len), device=padded_input.device))
+        triu_onnx(torch.ones((seq_len, seq_len), device=padded_input.device))
         == 1
     ).transpose(0, 1)
     mask = (
diff --git a/speechbrain/lobes/models/transformer/TransformerASR.py b/speechbrain/lobes/models/transformer/TransformerASR.py
index 4eddcba46..56de8c604 100644
--- a/speechbrain/lobes/models/transformer/TransformerASR.py
+++ b/speechbrain/lobes/models/transformer/TransformerASR.py
@@ -18,6 +18,11 @@ from speechbrain.lobes.models.transformer.Transformer import (
 from speechbrain.nnet.activations import Swish
 from speechbrain.dataio.dataio import length_to_mask
 
+import time
+import onnxruntime as rt
+from ais_bench.infer.interface import InferSession
+from sklearn.metrics.pairwise import cosine_similarity
+
 
 class TransformerASR(TransformerInterface):
     """This is an implementation of transformer model for ASR.
@@ -139,7 +144,7 @@ class TransformerASR(TransformerInterface):
 
         # reset parameters using xavier_normal_
         self._init_params()
-
+        
     def forward(self, src, tgt, wav_len=None, pad_idx=0):
         """
         Arguments
@@ -181,32 +186,32 @@ class TransformerASR(TransformerInterface):
             pos_embs=pos_embs_encoder,
         )
 
-        tgt = self.custom_tgt_module(tgt)
-
-        # Add positional encoding to the target before feeding the decoder.
-        if self.attention_type == "RelPosMHAXL":
-            # use standard sinusoidal pos encoding in decoder
-            tgt = tgt + self.positional_encoding_decoder(tgt)
-            pos_embs_encoder = None  # self.positional_encoding(src)
-            pos_embs_target = None
-        elif self.positional_encoding_type == "fixed_abs_sine":
-            tgt = tgt + self.positional_encoding(tgt)
-            pos_embs_target = None
-            pos_embs_encoder = None
-
-        decoder_out, _, _ = self.decoder(
-            tgt=tgt,
-            memory=encoder_out,
-            memory_mask=src_mask,
-            tgt_mask=tgt_mask,
-            tgt_key_padding_mask=tgt_key_padding_mask,
-            memory_key_padding_mask=src_key_padding_mask,
-            pos_embs_tgt=pos_embs_target,
-            pos_embs_src=pos_embs_encoder,
-        )
-
-        return encoder_out, decoder_out
-
+        # tgt = self.custom_tgt_module(tgt)
+
+        # # Add positional encoding to the target before feeding the decoder.
+        # if self.attention_type == "RelPosMHAXL":
+        #     # use standard sinusoidal pos encoding in decoder
+        #     tgt = tgt + self.positional_encoding_decoder(tgt)
+        #     pos_embs_encoder = None  # self.positional_encoding(src)
+        #     pos_embs_target = None
+        # elif self.positional_encoding_type == "fixed_abs_sine":
+        #     tgt = tgt + self.positional_encoding(tgt)
+        #     pos_embs_target = None
+        #     pos_embs_encoder = None
+
+        # decoder_out, _, _ = self.decoder(
+        #     tgt=tgt,
+        #     memory=encoder_out,
+        #     memory_mask=src_mask,
+        #     tgt_mask=tgt_mask,
+        #     tgt_key_padding_mask=tgt_key_padding_mask,
+        #     memory_key_padding_mask=src_key_padding_mask,
+        #     pos_embs_tgt=pos_embs_target,
+        #     pos_embs_src=pos_embs_encoder,
+        # )
+        
+        return encoder_out
+        
     def make_masks(self, src, tgt, wav_len=None, pad_idx=0):
         """This method generates the masks for training the transformer model.
 
@@ -231,7 +236,7 @@ class TransformerASR(TransformerInterface):
         return src_key_padding_mask, tgt_key_padding_mask, src_mask, tgt_mask
 
     @torch.no_grad()
-    def decode(self, tgt, encoder_out, enc_len=None):
+    def decode(self, decoder_sess_om, tgt, encoder_out, enc_len=None):
         """This method implements a decoding step for the transformer model.
 
         Arguments
@@ -259,15 +264,27 @@ class TransformerASR(TransformerInterface):
             pos_embs_target = None
             pos_embs_encoder = None
 
-        prediction, self_attns, multihead_attns = self.decoder(
-            tgt,
-            encoder_out,
-            tgt_mask=tgt_mask,
-            memory_key_padding_mask=src_key_padding_mask,
-            pos_embs_tgt=pos_embs_target,
-            pos_embs_src=pos_embs_encoder,
-        )
-        return prediction, multihead_attns[-1]
+        decoder_start = time.time()
+        # prediction, self_attns, multihead_attns = self.decoder(
+        #     tgt,
+        #     encoder_out,
+        #     tgt_mask=tgt_mask,
+        #     memory_key_padding_mask=src_key_padding_mask,
+        #     pos_embs_tgt=pos_embs_target,
+        #     pos_embs_src=pos_embs_encoder,
+        # )
+
+        tgt_np = tgt.cpu().numpy()
+        encoder_out_np = encoder_out.cpu().numpy()
+        decoder_mask_np = tgt_mask.cpu().numpy()
+
+        # om infer
+        outputSizes = 10000000
+        result = decoder_sess_om.infer([tgt_np, encoder_out_np, decoder_mask_np], 'dymshape', custom_sizes=outputSizes)
+        prediction = torch.from_numpy(result[0])
+
+        # return prediction, multihead_attns[-1]
+        return prediction, None
 
     def encode(self, src, wav_len=None):
         """
