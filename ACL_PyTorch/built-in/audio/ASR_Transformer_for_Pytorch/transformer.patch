diff --git a/recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py b/recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py
index d462abf3d..6c7deccd2 100644
--- a/recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py
+++ b/recipes/CommonVoice/ASR/CTC/train_with_wav2vec.py
@@ -37,7 +37,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core1.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the output probabilities."""
 
diff --git a/recipes/CommonVoice/ASR/seq2seq/train.py b/recipes/CommonVoice/ASR/seq2seq/train.py
index 5444e2d8e..3f78935d2 100644
--- a/recipes/CommonVoice/ASR/seq2seq/train.py
+++ b/recipes/CommonVoice/ASR/seq2seq/train.py
@@ -31,7 +31,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the output probabilities."""
 
diff --git a/recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py b/recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py
index 0c20b17bb..80bdb2b52 100644
--- a/recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py
+++ b/recipes/CommonVoice/ASR/seq2seq/train_with_wav2vec.py
@@ -39,7 +39,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core1.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the output probabilities."""
 
diff --git a/recipes/CommonVoice/ASR/transformer/train.py b/recipes/CommonVoice/ASR/transformer/train.py
index f2a4f42c9..93c98c8bb 100644
--- a/recipes/CommonVoice/ASR/transformer/train.py
+++ b/recipes/CommonVoice/ASR/transformer/train.py
@@ -39,7 +39,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the output probabilities."""
         batch = batch.to(self.device)
diff --git a/recipes/CommonVoice/self-supervised-learning/wav2vec2/train_hf_wav2vec2.py b/recipes/CommonVoice/self-supervised-learning/wav2vec2/train_hf_wav2vec2.py
index e98ff8a20..b15c161b4 100644
--- a/recipes/CommonVoice/self-supervised-learning/wav2vec2/train_hf_wav2vec2.py
+++ b/recipes/CommonVoice/self-supervised-learning/wav2vec2/train_hf_wav2vec2.py
@@ -40,7 +40,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class W2VBrain(sb.core.Brain):
+class W2VBrain(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the w2v2 loss."""
 
diff --git a/recipes/DVoice/ASR/CTC/train_with_wav2vec2.py b/recipes/DVoice/ASR/CTC/train_with_wav2vec2.py
index ae1dd39bd..104314232 100644
--- a/recipes/DVoice/ASR/CTC/train_with_wav2vec2.py
+++ b/recipes/DVoice/ASR/CTC/train_with_wav2vec2.py
@@ -37,7 +37,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the output probabilities."""
 
diff --git a/recipes/ESC50/classification/train_classifier.py b/recipes/ESC50/classification/train_classifier.py
index 476320035..4543e0fae 100755
--- a/recipes/ESC50/classification/train_classifier.py
+++ b/recipes/ESC50/classification/train_classifier.py
@@ -26,7 +26,7 @@ import numpy as np
 from confusion_matrix_fig import create_cm_fig
 
 
-class ESC50Brain(sb.core.Brain):
+class ESC50Brain(sb.core_mult.Brain):
     """Class for classifier training"
     """
 
diff --git a/recipes/ESC50/interpret/train_l2i.py b/recipes/ESC50/interpret/train_l2i.py
index fb78121b6..6ddc64348 100644
--- a/recipes/ESC50/interpret/train_l2i.py
+++ b/recipes/ESC50/interpret/train_l2i.py
@@ -98,7 +98,7 @@ def dataio_prep(hparams):
     return datasets, label_encoder
 
 
-class InterpreterESC50Brain(sb.core.Brain):
+class InterpreterESC50Brain(sb.core_mult.Brain):
     """Class for sound class embedding training" """
 
     def interpret_computation_steps(self, wavs):
diff --git a/recipes/ESC50/interpret/train_nmf.py b/recipes/ESC50/interpret/train_nmf.py
index 9da5ae403..075c9ee11 100644
--- a/recipes/ESC50/interpret/train_nmf.py
+++ b/recipes/ESC50/interpret/train_nmf.py
@@ -19,7 +19,7 @@ from esc50_prepare import prepare_esc50
 from train_l2i import dataio_prep
 
 
-class NMFBrain(sb.core.Brain):
+class NMFBrain(sb.core_mult.Brain):
     """
     The SpeechBrain class to train Non-Negative Factorization with Amortized Inference
     """
diff --git a/recipes/ESC50/interpret/train_piq.py b/recipes/ESC50/interpret/train_piq.py
index b4eb44f57..5a761302e 100644
--- a/recipes/ESC50/interpret/train_piq.py
+++ b/recipes/ESC50/interpret/train_piq.py
@@ -22,7 +22,7 @@ import matplotlib.pyplot as plt
 eps = 1e-10
 
 
-class InterpreterESC50Brain(sb.core.Brain):
+class InterpreterESC50Brain(sb.core_mult.Brain):
     """Class for sound class embedding training" """
 
     def invert_stft_with_phase(self, X_int, X_stft_phase):
diff --git a/recipes/Fisher-Callhome-Spanish/ST/transformer/train.py b/recipes/Fisher-Callhome-Spanish/ST/transformer/train.py
index 2c71cb6df..e1ff8c93d 100644
--- a/recipes/Fisher-Callhome-Spanish/ST/transformer/train.py
+++ b/recipes/Fisher-Callhome-Spanish/ST/transformer/train.py
@@ -25,7 +25,7 @@ logger = logging.getLogger(__name__)
 en_detoeknizer = MosesDetokenizer(lang="en")
 
 
-class ST(sb.core.Brain):
+class ST(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         batch = batch.to(self.device)
 
diff --git a/recipes/Google-speech-commands/train.py b/recipes/Google-speech-commands/train.py
index d4f8e8ede..bd3300f11 100644
--- a/recipes/Google-speech-commands/train.py
+++ b/recipes/Google-speech-commands/train.py
@@ -26,7 +26,7 @@ import speechbrain.nnet.CNN
 from speechbrain.utils.distributed import run_on_main
 
 
-class SpeakerBrain(sb.core.Brain):
+class SpeakerBrain(sb.core_mult.Brain):
     """Class for GSC training"
     """
 
@@ -228,7 +228,7 @@ if __name__ == "__main__":
         hparams = load_hyperpyyaml(fin, overrides)
 
     # Create experiment directory
-    sb.core.create_experiment_directory(
+    sb.core_mult.create_experiment_directory(
         experiment_directory=hparams["output_folder"],
         hyperparams_to_save=hparams_file,
         overrides=overrides,
diff --git a/recipes/IWSLT22_lowresource/train.py b/recipes/IWSLT22_lowresource/train.py
index 5108b7d2c..da05a708e 100644
--- a/recipes/IWSLT22_lowresource/train.py
+++ b/recipes/IWSLT22_lowresource/train.py
@@ -16,7 +16,7 @@ from sacremoses import MosesDetokenizer
 
 
 # Define training procedure
-class ST(sb.core.Brain):
+class ST(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the output probabilities."""
 
diff --git a/recipes/KsponSpeech/ASR/transformer/train.py b/recipes/KsponSpeech/ASR/transformer/train.py
index 71f9640a4..1c0de2cd9 100644
--- a/recipes/KsponSpeech/ASR/transformer/train.py
+++ b/recipes/KsponSpeech/ASR/transformer/train.py
@@ -49,7 +49,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches
         to the output probabilities."""
diff --git a/recipes/KsponSpeech/LM/train.py b/recipes/KsponSpeech/LM/train.py
index 366b1f29b..5d2d82d8e 100644
--- a/recipes/KsponSpeech/LM/train.py
+++ b/recipes/KsponSpeech/LM/train.py
@@ -25,7 +25,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class LM(sb.core.Brain):
+class LM(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the sentence batches
         to the output probabilities."""
diff --git a/recipes/LJSpeech/TTS/vocoder/hifi_gan/train.py b/recipes/LJSpeech/TTS/vocoder/hifi_gan/train.py
index 6c10c64f6..965d5c187 100644
--- a/recipes/LJSpeech/TTS/vocoder/hifi_gan/train.py
+++ b/recipes/LJSpeech/TTS/vocoder/hifi_gan/train.py
@@ -78,10 +78,10 @@ class HifiGanBrain(sb.Brain):
         batch = batch.to(self.device)
         y, _ = batch.sig
 
-        outputs = self.compute_forward(batch, sb.core.Stage.TRAIN)
+        outputs = self.compute_forward(batch, sb.core_mult.Stage.TRAIN)
         (y_g_hat, scores_fake, feats_fake, scores_real, feats_real) = outputs
         # calculate discriminator loss with the latest updated generator
-        loss_d = self.compute_objectives(outputs, batch, sb.core.Stage.TRAIN)[
+        loss_d = self.compute_objectives(outputs, batch, sb.core_mult.Stage.TRAIN)[
             "D_loss"
         ]
         # First train the discriminator
@@ -93,7 +93,7 @@ class HifiGanBrain(sb.Brain):
         scores_fake, feats_fake = self.modules.discriminator(y_g_hat)
         scores_real, feats_real = self.modules.discriminator(y)
         outputs = (y_g_hat, scores_fake, feats_fake, scores_real, feats_real)
-        loss_g = self.compute_objectives(outputs, batch, sb.core.Stage.TRAIN)[
+        loss_g = self.compute_objectives(outputs, batch, sb.core_mult.Stage.TRAIN)[
             "G_loss"
         ]
         # Then train the generator
diff --git a/recipes/LibriParty/generate_dataset/create_custom_dataset.py b/recipes/LibriParty/generate_dataset/create_custom_dataset.py
index 2ff70daf1..2a40a4f1c 100644
--- a/recipes/LibriParty/generate_dataset/create_custom_dataset.py
+++ b/recipes/LibriParty/generate_dataset/create_custom_dataset.py
@@ -21,7 +21,7 @@ from pathlib import Path
 from tqdm import tqdm
 
 # Load hyperparameters file with command-line overrides
-params_file, run_opts, overrides = sb.core.parse_arguments(sys.argv[1:])
+params_file, run_opts, overrides = sb.core_mult.parse_arguments(sys.argv[1:])
 with open(params_file) as fin:
     params = load_hyperpyyaml(fin, overrides)
 
diff --git a/recipes/LibriParty/generate_dataset/get_dataset_from_metadata.py b/recipes/LibriParty/generate_dataset/get_dataset_from_metadata.py
index 0c9663170..df3e49065 100644
--- a/recipes/LibriParty/generate_dataset/get_dataset_from_metadata.py
+++ b/recipes/LibriParty/generate_dataset/get_dataset_from_metadata.py
@@ -21,7 +21,7 @@ URL_METADATA = (
 )
 
 # Load hyperparameters file with command-line overrides
-params_file, run_opts, overrides = sb.core.parse_arguments(sys.argv[1:])
+params_file, run_opts, overrides = sb.core_mult.parse_arguments(sys.argv[1:])
 with open(params_file) as fin:
     params = load_hyperpyyaml(fin, overrides)
 
diff --git a/recipes/LibriSpeech/ASR/transformer/train.py b/recipes/LibriSpeech/ASR/transformer/train.py
index 001854edb..62037ad54 100644
--- a/recipes/LibriSpeech/ASR/transformer/train.py
+++ b/recipes/LibriSpeech/ASR/transformer/train.py
@@ -47,7 +47,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the waveform batches to the output probabilities."""
         batch = batch.to(self.device)
diff --git a/recipes/LibriSpeech/G2P/train_lm.py b/recipes/LibriSpeech/G2P/train_lm.py
index 2bc0acd99..c153026cd 100644
--- a/recipes/LibriSpeech/G2P/train_lm.py
+++ b/recipes/LibriSpeech/G2P/train_lm.py
@@ -47,7 +47,7 @@ logger = logging.getLogger(__name__)
 
 
 # Brain class for language model training
-class LM(sb.core.Brain):
+class LM(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Predicts the next word given the previous ones.
 
diff --git a/recipes/LibriSpeech/LM/train.py b/recipes/LibriSpeech/LM/train.py
index 3b3b846ca..6cb0ffa52 100644
--- a/recipes/LibriSpeech/LM/train.py
+++ b/recipes/LibriSpeech/LM/train.py
@@ -24,7 +24,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class LM(sb.core.Brain):
+class LM(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the sentence batches to the output probabilities."""
         batch = batch.to(self.device)
diff --git a/recipes/LibriSpeech/self-supervised-learning/wav2vec2/train_sb_wav2vec2.py b/recipes/LibriSpeech/self-supervised-learning/wav2vec2/train_sb_wav2vec2.py
index d4f97de8e..b31bcfe46 100644
--- a/recipes/LibriSpeech/self-supervised-learning/wav2vec2/train_sb_wav2vec2.py
+++ b/recipes/LibriSpeech/self-supervised-learning/wav2vec2/train_sb_wav2vec2.py
@@ -33,7 +33,7 @@ from speechbrain.lobes.models.wav2vec import sample_negatives
 logger = logging.getLogger(__name__)
 
 
-class W2V2Brain(sb.core.Brain):
+class W2V2Brain(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Computes forward pass through wav2vec model and returns encoded and
         target embeddings as well as other metrics of interest.
diff --git a/recipes/LibriTTS/vocoder/hifigan/train.py b/recipes/LibriTTS/vocoder/hifigan/train.py
index 5bc6309f6..b21d9be4c 100644
--- a/recipes/LibriTTS/vocoder/hifigan/train.py
+++ b/recipes/LibriTTS/vocoder/hifigan/train.py
@@ -78,10 +78,10 @@ class HifiGanBrain(sb.Brain):
         batch = batch.to(self.device)
         y, _ = batch.sig
 
-        outputs = self.compute_forward(batch, sb.core.Stage.TRAIN)
+        outputs = self.compute_forward(batch, sb.core_mult.Stage.TRAIN)
         (y_g_hat, scores_fake, feats_fake, scores_real, feats_real) = outputs
         # calculate discriminator loss with the latest updated generator
-        loss_d = self.compute_objectives(outputs, batch, sb.core.Stage.TRAIN)[
+        loss_d = self.compute_objectives(outputs, batch, sb.core_mult.Stage.TRAIN)[
             "D_loss"
         ]
         # First train the discriminator
@@ -93,7 +93,7 @@ class HifiGanBrain(sb.Brain):
         scores_fake, feats_fake = self.modules.discriminator(y_g_hat)
         scores_real, feats_real = self.modules.discriminator(y)
         outputs = (y_g_hat, scores_fake, feats_fake, scores_real, feats_real)
-        loss_g = self.compute_objectives(outputs, batch, sb.core.Stage.TRAIN)[
+        loss_g = self.compute_objectives(outputs, batch, sb.core_mult.Stage.TRAIN)[
             "G_loss"
         ]
         # Then train the generator
diff --git a/recipes/MEDIA/ASR/CTC/train_hf_wav2vec.py b/recipes/MEDIA/ASR/CTC/train_hf_wav2vec.py
index 3d512e4c6..6e30bfa9b 100644
--- a/recipes/MEDIA/ASR/CTC/train_hf_wav2vec.py
+++ b/recipes/MEDIA/ASR/CTC/train_hf_wav2vec.py
@@ -33,7 +33,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure.
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from waveform to output probabilities."""
 
diff --git a/recipes/MEDIA/SLU/CTC/train_hf_wav2vec.py b/recipes/MEDIA/SLU/CTC/train_hf_wav2vec.py
index 05a67032d..69c272460 100644
--- a/recipes/MEDIA/SLU/CTC/train_hf_wav2vec.py
+++ b/recipes/MEDIA/SLU/CTC/train_hf_wav2vec.py
@@ -33,7 +33,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure.
-class SLU(sb.core.Brain):
+class SLU(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from waveform to output probabilities."""
 
diff --git a/recipes/Switchboard/ASR/CTC/train_with_wav2vec.py b/recipes/Switchboard/ASR/CTC/train_with_wav2vec.py
index 9aa4f3a06..cbbe23d1e 100644
--- a/recipes/Switchboard/ASR/CTC/train_with_wav2vec.py
+++ b/recipes/Switchboard/ASR/CTC/train_with_wav2vec.py
@@ -41,7 +41,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def __init__(
         self,
         modules=None,
diff --git a/recipes/Switchboard/ASR/transformer/train.py b/recipes/Switchboard/ASR/transformer/train.py
index 1f9921709..c4fda0706 100644
--- a/recipes/Switchboard/ASR/transformer/train.py
+++ b/recipes/Switchboard/ASR/transformer/train.py
@@ -50,7 +50,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class ASR(sb.core.Brain):
+class ASR(sb.core_mult.Brain):
     def __init__(
         self,
         modules=None,
diff --git a/recipes/Switchboard/LM/train.py b/recipes/Switchboard/LM/train.py
index 5b6b9a944..654014e28 100644
--- a/recipes/Switchboard/LM/train.py
+++ b/recipes/Switchboard/LM/train.py
@@ -21,7 +21,7 @@ logger = logging.getLogger(__name__)
 
 
 # Define training procedure
-class LM(sb.core.Brain):
+class LM(sb.core_mult.Brain):
     def compute_forward(self, batch, stage):
         """Forward computations from the sentence batches to the output probabilities."""
         batch = batch.to(self.device)
diff --git a/recipes/UrbanSound8k/SoundClassification/train.py b/recipes/UrbanSound8k/SoundClassification/train.py
index 806e667d8..885deeb72 100755
--- a/recipes/UrbanSound8k/SoundClassification/train.py
+++ b/recipes/UrbanSound8k/SoundClassification/train.py
@@ -31,7 +31,7 @@ import numpy as np
 from confusion_matrix_fig import create_cm_fig
 
 
-class UrbanSound8kBrain(sb.core.Brain):
+class UrbanSound8kBrain(sb.core_mult.Brain):
     """Class for sound class embedding training"
     """
 
diff --git a/recipes/VoxCeleb/SpeakerRec/speaker_verification_cosine.py b/recipes/VoxCeleb/SpeakerRec/speaker_verification_cosine.py
index d74e56fc1..9106a0bcc 100755
--- a/recipes/VoxCeleb/SpeakerRec/speaker_verification_cosine.py
+++ b/recipes/VoxCeleb/SpeakerRec/speaker_verification_cosine.py
@@ -215,7 +215,7 @@ if __name__ == "__main__":
     sys.path.append(os.path.dirname(current_dir))
 
     # Load hyperparameters file with command-line overrides
-    params_file, run_opts, overrides = sb.core.parse_arguments(sys.argv[1:])
+    params_file, run_opts, overrides = sb.core_mult.parse_arguments(sys.argv[1:])
     with open(params_file) as fin:
         params = load_hyperpyyaml(fin, overrides)
 
@@ -228,7 +228,7 @@ if __name__ == "__main__":
     from voxceleb_prepare import prepare_voxceleb  # noqa E402
 
     # Create experiment directory
-    sb.core.create_experiment_directory(
+    sb.core_mult.create_experiment_directory(
         experiment_directory=params["output_folder"],
         hyperparams_to_save=params_file,
         overrides=overrides,
diff --git a/recipes/VoxCeleb/SpeakerRec/speaker_verification_plda.py b/recipes/VoxCeleb/SpeakerRec/speaker_verification_plda.py
index fbe89b090..e4c50d4f7 100755
--- a/recipes/VoxCeleb/SpeakerRec/speaker_verification_plda.py
+++ b/recipes/VoxCeleb/SpeakerRec/speaker_verification_plda.py
@@ -216,7 +216,7 @@ if __name__ == "__main__":
     sys.path.append(os.path.dirname(current_dir))
 
     # Load hyperparameters file with command-line overrides
-    params_file, run_opts, overrides = sb.core.parse_arguments(sys.argv[1:])
+    params_file, run_opts, overrides = sb.core_mult.parse_arguments(sys.argv[1:])
     with open(params_file) as fin:
         params = load_hyperpyyaml(fin, overrides)
 
@@ -229,7 +229,7 @@ if __name__ == "__main__":
     from voxceleb_prepare import prepare_voxceleb  # noqa E402
 
     # Create experiment directory
-    sb.core.create_experiment_directory(
+    sb.core_mult.create_experiment_directory(
         experiment_directory=params["output_folder"],
         hyperparams_to_save=params_file,
         overrides=overrides,
diff --git a/recipes/VoxCeleb/SpeakerRec/train_speaker_embeddings.py b/recipes/VoxCeleb/SpeakerRec/train_speaker_embeddings.py
index 34a9e4f79..6db80cadc 100755
--- a/recipes/VoxCeleb/SpeakerRec/train_speaker_embeddings.py
+++ b/recipes/VoxCeleb/SpeakerRec/train_speaker_embeddings.py
@@ -25,7 +25,7 @@ from hyperpyyaml import load_hyperpyyaml
 from speechbrain.utils.distributed import run_on_main
 
 
-class SpeakerBrain(sb.core.Brain):
+class SpeakerBrain(sb.core_mult.Brain):
     """Class for speaker embedding training"
     """
 
@@ -233,7 +233,7 @@ if __name__ == "__main__":
     train_data, valid_data, label_encoder = dataio_prep(hparams)
 
     # Create experiment directory
-    sb.core.create_experiment_directory(
+    sb.core_mult.create_experiment_directory(
         experiment_directory=hparams["output_folder"],
         hyperparams_to_save=hparams_file,
         overrides=overrides,
diff --git a/recipes/VoxLingua107/lang_id/train.py b/recipes/VoxLingua107/lang_id/train.py
index b261c0f9c..b67048773 100644
--- a/recipes/VoxLingua107/lang_id/train.py
+++ b/recipes/VoxLingua107/lang_id/train.py
@@ -34,7 +34,7 @@ from speechbrain.dataio.batch import PaddedBatch
 logger = logging.getLogger(__name__)
 
 
-class LanguageBrain(sb.core.Brain):
+class LanguageBrain(sb.core_mult.Brain):
     """Class for language ID training"
     """
 
@@ -253,7 +253,7 @@ if __name__ == "__main__":
     )
 
     # Create experiment directory
-    sb.core.create_experiment_directory(
+    sb.core_mult.create_experiment_directory(
         experiment_directory=hparams["output_folder"],
         hyperparams_to_save=hparams_file,
         overrides=overrides,
diff --git a/speechbrain/core.py b/speechbrain/core.py
index 0d5dc38e9..6d52c13ca 100644
--- a/speechbrain/core.py
+++ b/speechbrain/core.py
@@ -7,6 +7,11 @@ Authors
  * Aku Rouhe 2021
  * Andreas Nautsch 2022
 """
+from multiprocessing import Queue
+
+from torch.multiprocessing import Process, Manager
+#from acllite.acllite_model import AclLiteModel
+#from acllite.acllite_resource import AclLiteResource
 
 import os
 import sys
@@ -38,7 +43,7 @@ from speechbrain.dataio.dataloader import LoopedLoader
 from speechbrain.dataio.dataloader import SaveableDataLoader
 from speechbrain.dataio.sampler import DistributedSamplerWrapper
 from speechbrain.dataio.sampler import ReproducibleRandomSampler
-
+from ais_bench.infer.interface import InferSession
 logger = logging.getLogger(__name__)
 DEFAULT_LOG_CONFIG = os.path.dirname(os.path.abspath(__file__))
 DEFAULT_LOG_CONFIG = os.path.join(DEFAULT_LOG_CONFIG, "log-config.yaml")
@@ -293,7 +298,35 @@ def parse_arguments(arg_list=None):
         help="Enable colored progress-bar in tqdm. If this is "
         "false, tqdm shall use default colors.",
     )
-
+    parser.add_argument(
+        "--mode",
+        default='infer',
+        help="run mode, export onnx or infer",
+    )
+    parser.add_argument(
+        "--npu_rank",
+        default='1',
+        type=int,
+        help="choose the device id",
+    )
+    parser.add_argument(
+        "--encoder_file",
+        default='encoder.om',
+        type=str,
+        help="the path of om",
+    )
+    parser.add_argument(
+        "--decoder_file",
+        default='decoder.om',
+        type=str,
+        help="the path of om",
+    )   
+    parser.add_argument(
+        "--ctc_enable",
+        default='True',
+        type=bool,
+        help="enable CTC",
+    )   
     # Accept extra args to override yaml
     run_opts, overrides = parser.parse_known_args(arg_list)
 
@@ -1033,8 +1066,8 @@ class Brain:
             )
 
         return True
-
-    def evaluate_batch(self, batch, stage):
+#batch, Stage.TEST, device_id, encoder_model, decoder_model, out_que
+    def evaluate_batch(self, batch, stage, device_id, encoder_model, decoder_model, out_que):
         """Evaluate one batch, override for different procedure than train.
 
         The default implementation depends on two methods being defined
@@ -1058,7 +1091,8 @@ class Brain:
 
         out = self.compute_forward(batch, stage=stage)
         loss = self.compute_objectives(out, batch, stage=stage)
-        return loss.detach().cpu()
+        # return loss.detach().cpu()
+        return loss
 
     def _fit_train(self, train_set, epoch, enable):
         # Training stage
@@ -1303,7 +1337,33 @@ class Brain:
                 if any(p.requires_grad for p in module.parameters()):
                     module = DP(module)
                     self.modules[name] = module
-
+    def execute(self, device_id, in_que, out_que, encoder_path, decoder_path):
+        t0 = time.time()
+        self.encoder_sess_om = InferSession(device_id, encoder_path)
+        self.decoder_sess_om = InferSession(device_id, decoder_path)
+        print("load model cost: %.4f" % (time.time() - t0))
+        t0 = time.time()
+        while(1):
+            if in_que.empty() == True:
+                break
+            batch = in_que.get()
+            num = in_que.qsize()
+            print(f'device_{device_id}: remain_qsize_{num}' )
+            loss = self.evaluate_batch(batch, Stage.TEST, device_id, self.encoder_sess_om, self.decoder_sess_om, out_que)
+
+    def assign_task(self, device_id, input_que, output_que, encoder_om, decoder_om):    
+        proc_list = [
+            Process(target=self.execute, args=(device_id, input_que, output_que, encoder_om, decoder_om)),
+        ]
+        return proc_list
+    def creat_quees(self):
+        device_list = [0,1]
+
+        n = len(device_list)
+        input_qs = [Manager().Queue() for _ in range(n)]
+        output_qs = Manager().Queue()
+        return input_qs, output_qs, device_list
+    
     def evaluate(
         self,
         test_set,
@@ -1311,6 +1371,8 @@ class Brain:
         min_key=None,
         progressbar=None,
         test_loader_kwargs={},
+        encoder_path='',
+        decopder_path=''
     ):
         """Iterate test_set and evaluate brain performance. By default, loads
         the best-performing checkpoint (as recorded using the checkpointer).
@@ -1353,6 +1415,9 @@ class Brain:
         self.on_stage_start(Stage.TEST, epoch=None)
         self.modules.eval()
         avg_test_loss = 0.0
+
+        input_qs, output_qs, device_list  = self.creat_quees()
+        torch.set_num_threads(1)
         with torch.no_grad():
             for batch in tqdm(
                 test_set,
@@ -1360,9 +1425,13 @@ class Brain:
                 disable=not progressbar,
                 colour=self.tqdm_barcolor["test"],
             ):
+                
+                if self.step % 2  == 0:
+                    input_qs[0].put(batch)
+                else:
+                    input_qs[1].put(batch)
                 self.step += 1
-                loss = self.evaluate_batch(batch, stage=Stage.TEST)
-                avg_test_loss = self.update_average(loss, avg_test_loss)
+                
 
                 # Profile only if desired (steps allow the profiler to know when all is warmed up)
                 if self.profiler is not None:
@@ -1372,13 +1441,22 @@ class Brain:
                 # Debug mode only runs a few batches
                 if self.debug and self.step == self.debug_batches:
                     break
+            proc_list0 = self.assign_task(0, input_qs[0], output_qs, encoder_path, decopder_path)
+            proc_list1 = self.assign_task(1, input_qs[1], output_qs, encoder_path, decopder_path)
+            proc_list = proc_list0 + proc_list1
+            for proc in proc_list:
+                proc.start()
+            for proc in proc_list:
+                proc.join()
+
 
             # Only run evaluation "on_stage_end" on main process
             run_on_main(
                 self.on_stage_end, args=[Stage.TEST, avg_test_loss, None]
             )
         self.step = 0
-        return avg_test_loss
+        # return avg_test_loss
+        return None
 
     def update_average(self, loss, avg_loss):
         """Update running average of the loss.
diff --git a/speechbrain/dataio/dataio.py b/speechbrain/dataio/dataio.py
index 4d55bbdb8..9d5033bd2 100644
--- a/speechbrain/dataio/dataio.py
+++ b/speechbrain/dataio/dataio.py
@@ -213,6 +213,7 @@ def read_audio(waveforms_obj):
     >>> loaded.allclose(dummywav.squeeze(0),atol=1e-4) # replace with eq with sox_io backend
     True
     """
+
     if isinstance(waveforms_obj, str):
         audio, _ = torchaudio.load(waveforms_obj)
     else:
@@ -730,10 +731,10 @@ def length_to_mask(length, max_len=None, dtype=None, device=None):
     assert len(length.shape) == 1
 
     if max_len is None:
-        max_len = length.max().long().item()  # using arange to generate mask
+        max_len = torch.max(length, dim=0).values.long()  # using arange to generate mask
     mask = torch.arange(
         max_len, device=length.device, dtype=length.dtype
-    ).expand(len(length), max_len) < length.unsqueeze(1)
+    ).expand(length.shape[0], max_len) < length.unsqueeze(1)
 
     if dtype is None:
         dtype = length.dtype
diff --git a/speechbrain/dataio/dataset.py b/speechbrain/dataio/dataset.py
index 095399c24..2287e9d59 100644
--- a/speechbrain/dataio/dataset.py
+++ b/speechbrain/dataio/dataset.py
@@ -160,8 +160,10 @@ class DynamicItemDataset(Dataset):
         return len(self.data_ids)
 
     def __getitem__(self, index):
+
         data_id = self.data_ids[index]
         data_point = self.data[data_id]
+
         return self.pipeline.compute_outputs({"id": data_id, **data_point})
 
     def add_dynamic_item(self, func, takes=None, provides=None):
diff --git a/speechbrain/decoders/seq2seq.py b/speechbrain/decoders/seq2seq.py
index da339c363..6bf2b6a18 100644
--- a/speechbrain/decoders/seq2seq.py
+++ b/speechbrain/decoders/seq2seq.py
@@ -8,7 +8,7 @@ Authors
  * Sung-Lin Yeh 2020
 """
 import torch
-
+from tqdm.contrib import tqdm
 import speechbrain as sb
 from speechbrain.decoders.ctc import CTCPrefixScorer
 
@@ -685,8 +685,10 @@ class S2SBeamSearcher(S2SBaseSearcher):
 
         return topk_hyps, topk_scores, topk_lengths, topk_log_probs
 
-    def forward(self, enc_states, wav_len):  # noqa: C901
+    def forward(self, decoder_sess_om, enc_states, wav_len, ctc=True):  # noqa: C901
         """Applies beamsearch and returns the predicted tokens."""
+        if not ctc:
+          self.ctc_weight = 0
         enc_lens = torch.round(enc_states.shape[1] * wav_len).int()
         device = enc_states.device
         batch_size = enc_states.shape[0]
@@ -760,13 +762,13 @@ class S2SBeamSearcher(S2SBaseSearcher):
         # This variable will be used when using_max_attn_shift=True
         prev_attn_peak = torch.zeros(batch_size * self.beam_size, device=device)
 
-        for t in range(max_decode_steps):
+        for t in tqdm(range(max_decode_steps)):
             # terminate condition
             if self._check_full_beams(hyps_and_scores, self.beam_size):
                 break
 
             log_probs, memory, attn = self.forward_step(
-                inp_tokens, memory, enc_states, enc_lens
+                decoder_sess_om, inp_tokens, memory, enc_states, enc_lens
             )
             log_probs = self.att_weight * log_probs
 
@@ -819,7 +821,7 @@ class S2SBeamSearcher(S2SBaseSearcher):
                     g, ctc_memory, ctc_candidates, attn
                 )
                 log_probs = log_probs + self.ctc_weight * ctc_log_probs
-
+            
             scores = sequence_scores.unsqueeze(1).expand(-1, vocab_size)
             scores = scores + log_probs
 
@@ -1345,10 +1347,10 @@ class S2STransformerBeamSearch(S2SBeamSearcher):
         memory = torch.index_select(memory, dim=0, index=index)
         return memory
 
-    def forward_step(self, inp_tokens, memory, enc_states, enc_lens):
+    def forward_step(self, decoder_sess_om, inp_tokens, memory, enc_states, enc_lens):
         """Performs a step in the implemented beamsearcher."""
         memory = _update_mem(inp_tokens, memory)
-        pred, attn = self.model.decode(memory, enc_states)
+        pred, attn = self.model.decode(decoder_sess_om, memory, enc_states)
         prob_dist = self.softmax(self.fc(pred) / self.temperature)
         return prob_dist[:, -1, :], memory, attn
 
diff --git a/speechbrain/lobes/models/convolution.py b/speechbrain/lobes/models/convolution.py
index 7f5432708..35d27efe1 100644
--- a/speechbrain/lobes/models/convolution.py
+++ b/speechbrain/lobes/models/convolution.py
@@ -6,7 +6,7 @@ Authors
 import torch
 from speechbrain.nnet.CNN import Conv2d
 from speechbrain.nnet.containers import Sequential
-from speechbrain.nnet.normalization import LayerNorm
+from speechbrain.nnet.normalization import LayerNorm, BatchNorm2d
 
 
 class ConvolutionFrontEnd(Sequential):
@@ -57,7 +57,7 @@ class ConvolutionFrontEnd(Sequential):
         residuals=[True, True, True],
         conv_module=Conv2d,
         activation=torch.nn.LeakyReLU,
-        norm=LayerNorm,
+        norm=BatchNorm2d,
         dropout=0.1,
         conv_bias=True,
         padding="same",
diff --git a/speechbrain/lobes/models/transformer/Transformer.py b/speechbrain/lobes/models/transformer/Transformer.py
index 3913493bc..472967ec8 100644
--- a/speechbrain/lobes/models/transformer/Transformer.py
+++ b/speechbrain/lobes/models/transformer/Transformer.py
@@ -451,6 +451,7 @@ class TransformerEncoder(nn.Module):
         src_mask: Optional[torch.Tensor] = None,
         src_key_padding_mask: Optional[torch.Tensor] = None,
         pos_embs: Optional[torch.Tensor] = None,
+        attn_results: Optional[bool] = True
     ):
         """
         Arguments
@@ -483,8 +484,9 @@ class TransformerEncoder(nn.Module):
 
                 attention_lst.append(attention)
         output = self.norm(output)
-        return output, attention_lst
-
+        if attn_results:
+            return output, attention_lst
+        return output
 
 class TransformerDecoderLayer(nn.Module):
     """This class implements the self-attention decoder layer.
@@ -726,6 +728,7 @@ class TransformerDecoder(nn.Module):
         memory_key_padding_mask=None,
         pos_embs_tgt=None,
         pos_embs_src=None,
+        attn_results=True
     ):
         """
         Arguments
@@ -760,8 +763,9 @@ class TransformerDecoder(nn.Module):
             multihead_attns.append(multihead_attn)
         output = self.norm(output)
 
-        return output, self_attns, multihead_attns
-
+        if attn_results:
+            return output, self_attns, multihead_attns
+        return output
 
 class NormalizedEmbedding(nn.Module):
     """This class implements the normalized embedding layer for the transformer.
@@ -828,6 +832,17 @@ def get_key_padding_mask(padded_input, pad_idx):
 
     return key_padded_mask.detach()
 
+def triu_onnx(x, diagonal=0):
+    assert len(x.shape) == 2
+    m, l = x.shape
+    mask = torch.arange(l, device=x.device).expand(m, l)
+    # print('Transformer::839:mask', mask.shape)
+    arange = torch.arange(m, device=x.device)
+    arange = arange.unsqueeze(-1)
+    if diagonal:
+        arange = arange + diagonal
+    mask = mask >= arange
+    return x.masked_fill(mask==0, 0)
 
 def get_lookahead_mask(padded_input):
     """Creates a binary mask for each sequence which maskes future frames.
@@ -847,7 +862,7 @@ def get_lookahead_mask(padded_input):
     """
     seq_len = padded_input.shape[1]
     mask = (
-        torch.triu(torch.ones((seq_len, seq_len), device=padded_input.device))
+        triu_onnx(torch.ones((seq_len, seq_len), device=padded_input.device))
         == 1
     ).transpose(0, 1)
     mask = (
diff --git a/speechbrain/lobes/models/transformer/TransformerASR.py b/speechbrain/lobes/models/transformer/TransformerASR.py
index 4eddcba46..56de8c604 100644
--- a/speechbrain/lobes/models/transformer/TransformerASR.py
+++ b/speechbrain/lobes/models/transformer/TransformerASR.py
@@ -18,6 +18,11 @@ from speechbrain.lobes.models.transformer.Transformer import (
 from speechbrain.nnet.activations import Swish
 from speechbrain.dataio.dataio import length_to_mask
 
+import time
+import onnxruntime as rt
+from ais_bench.infer.interface import InferSession
+from sklearn.metrics.pairwise import cosine_similarity
+
 
 class TransformerASR(TransformerInterface):
     """This is an implementation of transformer model for ASR.
@@ -139,7 +144,7 @@ class TransformerASR(TransformerInterface):
 
         # reset parameters using xavier_normal_
         self._init_params()
-
+        
     def forward(self, src, tgt, wav_len=None, pad_idx=0):
         """
         Arguments
@@ -181,32 +186,32 @@ class TransformerASR(TransformerInterface):
             pos_embs=pos_embs_encoder,
         )
 
-        tgt = self.custom_tgt_module(tgt)
-
-        # Add positional encoding to the target before feeding the decoder.
-        if self.attention_type == "RelPosMHAXL":
-            # use standard sinusoidal pos encoding in decoder
-            tgt = tgt + self.positional_encoding_decoder(tgt)
-            pos_embs_encoder = None  # self.positional_encoding(src)
-            pos_embs_target = None
-        elif self.positional_encoding_type == "fixed_abs_sine":
-            tgt = tgt + self.positional_encoding(tgt)
-            pos_embs_target = None
-            pos_embs_encoder = None
-
-        decoder_out, _, _ = self.decoder(
-            tgt=tgt,
-            memory=encoder_out,
-            memory_mask=src_mask,
-            tgt_mask=tgt_mask,
-            tgt_key_padding_mask=tgt_key_padding_mask,
-            memory_key_padding_mask=src_key_padding_mask,
-            pos_embs_tgt=pos_embs_target,
-            pos_embs_src=pos_embs_encoder,
-        )
-
-        return encoder_out, decoder_out
-
+        # tgt = self.custom_tgt_module(tgt)
+
+        # # Add positional encoding to the target before feeding the decoder.
+        # if self.attention_type == "RelPosMHAXL":
+        #     # use standard sinusoidal pos encoding in decoder
+        #     tgt = tgt + self.positional_encoding_decoder(tgt)
+        #     pos_embs_encoder = None  # self.positional_encoding(src)
+        #     pos_embs_target = None
+        # elif self.positional_encoding_type == "fixed_abs_sine":
+        #     tgt = tgt + self.positional_encoding(tgt)
+        #     pos_embs_target = None
+        #     pos_embs_encoder = None
+
+        # decoder_out, _, _ = self.decoder(
+        #     tgt=tgt,
+        #     memory=encoder_out,
+        #     memory_mask=src_mask,
+        #     tgt_mask=tgt_mask,
+        #     tgt_key_padding_mask=tgt_key_padding_mask,
+        #     memory_key_padding_mask=src_key_padding_mask,
+        #     pos_embs_tgt=pos_embs_target,
+        #     pos_embs_src=pos_embs_encoder,
+        # )
+        
+        return encoder_out
+        
     def make_masks(self, src, tgt, wav_len=None, pad_idx=0):
         """This method generates the masks for training the transformer model.
 
@@ -231,7 +236,7 @@ class TransformerASR(TransformerInterface):
         return src_key_padding_mask, tgt_key_padding_mask, src_mask, tgt_mask
 
     @torch.no_grad()
-    def decode(self, tgt, encoder_out, enc_len=None):
+    def decode(self, decoder_sess_om, tgt, encoder_out, enc_len=None):
         """This method implements a decoding step for the transformer model.
 
         Arguments
@@ -259,15 +264,27 @@ class TransformerASR(TransformerInterface):
             pos_embs_target = None
             pos_embs_encoder = None
 
-        prediction, self_attns, multihead_attns = self.decoder(
-            tgt,
-            encoder_out,
-            tgt_mask=tgt_mask,
-            memory_key_padding_mask=src_key_padding_mask,
-            pos_embs_tgt=pos_embs_target,
-            pos_embs_src=pos_embs_encoder,
-        )
-        return prediction, multihead_attns[-1]
+        decoder_start = time.time()
+        # prediction, self_attns, multihead_attns = self.decoder(
+        #     tgt,
+        #     encoder_out,
+        #     tgt_mask=tgt_mask,
+        #     memory_key_padding_mask=src_key_padding_mask,
+        #     pos_embs_tgt=pos_embs_target,
+        #     pos_embs_src=pos_embs_encoder,
+        # )
+
+        tgt_np = tgt.cpu().numpy()
+        encoder_out_np = encoder_out.cpu().numpy()
+        decoder_mask_np = tgt_mask.cpu().numpy()
+
+        # om infer
+        outputSizes = 10000000
+        result = decoder_sess_om.infer([tgt_np, encoder_out_np, decoder_mask_np], 'dymshape', custom_sizes=outputSizes)
+        prediction = torch.from_numpy(result[0])
+
+        # return prediction, multihead_attns[-1]
+        return prediction, None
 
     def encode(self, src, wav_len=None):
         """
diff --git a/templates/speech_recognition/LM/train.py b/templates/speech_recognition/LM/train.py
index add47eeca..c52aa0269 100755
--- a/templates/speech_recognition/LM/train.py
+++ b/templates/speech_recognition/LM/train.py
@@ -25,7 +25,7 @@ logger = logging.getLogger(__name__)
 
 
 # Brain class for language model training
-class LM(sb.core.Brain):
+class LM(sb.core_mult.Brain):
     """Class that manages the training loop. See speechbrain.core.Brain."""
 
     def compute_forward(self, batch, stage):
diff --git a/tests/unittests/test_core.py b/tests/unittests/test_core.py
index 4c4fa8247..43fb1e1a0 100644
--- a/tests/unittests/test_core.py
+++ b/tests/unittests/test_core.py
@@ -1,5 +1,5 @@
 def test_parse_arguments():
-    from speechbrain.core import parse_arguments
+    from ASR_Transformer_for_Pytorch.speechbrain.speechbrain.core import parse_arguments
 
     filename, run_opts, overrides = parse_arguments(
         ["params.yaml", "--device=cpu", "--seed=3", "--data_folder", "TIMIT"]
@@ -11,7 +11,7 @@ def test_parse_arguments():
 
 def test_brain(device):
     import torch
-    from speechbrain.core import Brain, Stage
+    from ASR_Transformer_for_Pytorch.speechbrain.speechbrain.core import Brain, Stage
     from torch.optim import SGD
 
     model = torch.nn.Linear(in_features=10, out_features=10, device=device)
diff --git a/tests/unittests/test_profiling.py b/tests/unittests/test_profiling.py
index d3efe0880..388c5513b 100644
--- a/tests/unittests/test_profiling.py
+++ b/tests/unittests/test_profiling.py
@@ -1,7 +1,7 @@
 def test_profile_class(device):
     import torch
     from torch.optim import SGD
-    from speechbrain.core import Brain
+    from ASR_Transformer_for_Pytorch.speechbrain.speechbrain.core import Brain
     from speechbrain.utils.profiling import profile
 
     @profile
@@ -185,7 +185,7 @@ def test_scheduler(device):
     import torch
     from pytest import raises
     from torch.optim import SGD
-    from speechbrain.core import Brain
+    from ASR_Transformer_for_Pytorch.speechbrain.speechbrain.core import Brain
     from speechbrain.utils.profiling import profile, schedule
 
     @schedule
@@ -431,7 +431,7 @@ def test_scheduler(device):
 def test_tracer(device):
     import torch
     from torch.optim import SGD
-    from speechbrain.core import Brain
+    from ASR_Transformer_for_Pytorch.speechbrain.speechbrain.core import Brain
     from speechbrain.utils.profiling import profile, export
 
     @export
@@ -484,7 +484,7 @@ def test_tracer(device):
 def test_aggregated_traces(device):
     import torch
     from torch.optim import SGD
-    from speechbrain.core import Brain
+    from ASR_Transformer_for_Pytorch.speechbrain.speechbrain.core import Brain
     from speechbrain.utils.profiling import profile
 
     @profile
@@ -654,7 +654,7 @@ def test_profile_details(device):
 
     # from copy import deepcopy
     from torch.optim import SGD
-    from speechbrain.core import Brain
+    from ASR_Transformer_for_Pytorch.speechbrain.speechbrain.core import Brain
     from speechbrain.utils.profiling import (
         profile_analyst,
         profile_optimiser,
