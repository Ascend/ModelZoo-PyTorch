diff --git a/espnet/asr/pytorch_backend/recog.py b/espnet/asr/pytorch_backend/recog.py
index 6c6d4ce11..73567f6c9 100644
--- a/espnet/asr/pytorch_backend/recog.py
+++ b/espnet/asr/pytorch_backend/recog.py
@@ -168,7 +168,21 @@ def recog_v2(args):
             logging.info("(%d/%d) decoding " + name, idx, len(js.keys()))
             batch = [(name, js[name])]
             feat = load_inputs_and_targets(batch)[0][0]
-            enc = model.encode(torch.as_tensor(feat).to(device=device, dtype=dtype))
+            #enc = model.encode(torch.as_tensor(feat).to(device=device, dtype=dtype))
+            input_tensor = torch.as_tensor(feat)
+
+            print("input_tensor.shape:", input_tensor.shape)
+            torch.onnx.export(model.encoder,
+                              (input_tensor),
+                              "encoder.onnx",
+                              input_names=['input'],
+                              dynamic_axes={'input': [0]},
+                              export_params=True,
+                              opset_version=11,
+                              do_constant_folding=True,
+                              verbose=True
+                              )
+            exit()
             nbest_hyps = beam_search(
                 x=enc, maxlenratio=args.maxlenratio, minlenratio=args.minlenratio
             )
diff --git a/espnet/nets/pytorch_backend/conformer/encoder.py b/espnet/nets/pytorch_backend/conformer/encoder.py
index 515cf7e3f..b0e86b0d5 100644
--- a/espnet/nets/pytorch_backend/conformer/encoder.py
+++ b/espnet/nets/pytorch_backend/conformer/encoder.py
@@ -237,7 +237,7 @@ class Encoder(torch.nn.Module):
                 conditioning_layer_dim, attention_dim
             )
 
-    def forward(self, xs, masks):
+    def forward(self, xs, masks=None):
         """Encode input sequence.
 
         Args:
@@ -249,6 +249,7 @@ class Encoder(torch.nn.Module):
             torch.Tensor: Mask tensor (#batch, time).
 
         """
+        xs = xs.unsqueeze(0)
         if isinstance(self.embed, (Conv2dSubsampling, VGG2L)):
             xs, masks = self.embed(xs, masks)
         else:
@@ -290,7 +291,8 @@ class Encoder(torch.nn.Module):
 
         if self.normalize_before:
             xs = self.after_norm(xs)
-
+        #
+        return xs
         if self.intermediate_layers is not None:
             return xs, masks, intermediate_outputs
         return xs, masks
diff --git a/espnet/nets/pytorch_backend/transformer/attention.py b/espnet/nets/pytorch_backend/transformer/attention.py
index 8d8b68089..41af79fa3 100644
--- a/espnet/nets/pytorch_backend/transformer/attention.py
+++ b/espnet/nets/pytorch_backend/transformer/attention.py
@@ -53,13 +53,15 @@ class MultiHeadedAttention(nn.Module):
         """
         n_batch = query.size(0)
         q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
+        q2 = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
         k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)
         v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)
         q = q.transpose(1, 2)  # (batch, head, time1, d_k)
+        q2 = q2.transpose(1, 2)  # (batch, head, time1, d_k)
         k = k.transpose(1, 2)  # (batch, head, time2, d_k)
         v = v.transpose(1, 2)  # (batch, head, time2, d_k)
 
-        return q, k, v
+        return q, q2, k, v
 
     def forward_attention(self, value, scores, mask):
         """Compute attention context vector.
@@ -109,7 +111,7 @@ class MultiHeadedAttention(nn.Module):
             torch.Tensor: Output tensor (#batch, time1, d_model).
 
         """
-        q, k, v = self.forward_qkv(query, key, value)
+        q, q2, k, v = self.forward_qkv(query, key, value)
         scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
         return self.forward_attention(v, scores, mask)
 
@@ -152,10 +154,10 @@ class LegacyRelPositionMultiHeadedAttention(MultiHeadedAttention):
             torch.Tensor: Output tensor.
 
         """
-        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)
+        zero_pad = torch.zeros((1, 4, x.size(2), 1), device=x.device, dtype=x.dtype)
         x_padded = torch.cat([zero_pad, x], dim=-1)
 
-        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))
+        x_padded = x_padded.view(1, 4, x.size(3) + 1, x.size(2))
         x = x_padded[:, :, 1:].view_as(x)
 
         if self.zero_triu:
@@ -179,9 +181,9 @@ class LegacyRelPositionMultiHeadedAttention(MultiHeadedAttention):
             torch.Tensor: Output tensor (#batch, time1, d_model).
 
         """
-        q, k, v = self.forward_qkv(query, key, value)
+        q, q2, k, v = self.forward_qkv(query, key, value)
         q = q.transpose(1, 2)  # (batch, time1, head, d_k)
-
+        q2 = q2.transpose(1, 2)
         n_batch_pos = pos_emb.size(0)
         p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)
         p = p.transpose(1, 2)  # (batch, head, time1, d_k)
@@ -189,7 +191,7 @@ class LegacyRelPositionMultiHeadedAttention(MultiHeadedAttention):
         # (batch, head, time1, d_k)
         q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)
         # (batch, head, time1, d_k)
-        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)
+        q_with_bias_v = (q2 + self.pos_bias_v).transpose(1, 2)
 
         # compute attention score
         # first compute matrix a and matrix c
@@ -200,11 +202,12 @@ class LegacyRelPositionMultiHeadedAttention(MultiHeadedAttention):
         # compute matrix b and matrix d
         # (batch, head, time1, time1)
         matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))
+        matrix_bd = matrix_bd/ math.sqrt(
+            self.d_k)
         matrix_bd = self.rel_shift(matrix_bd)
-
-        scores = (matrix_ac + matrix_bd) / math.sqrt(
-            self.d_k
-        )  # (batch, head, time1, time2)
+        matrix_ac = matrix_ac/ math.sqrt(
+            self.d_k)
+        scores = matrix_ac + matrix_bd  # (batch, head, time1, time2)
 
         return self.forward_attention(v, scores, mask)
 
@@ -278,7 +281,7 @@ class RelPositionMultiHeadedAttention(MultiHeadedAttention):
             torch.Tensor: Output tensor (#batch, time1, d_model).
 
         """
-        q, k, v = self.forward_qkv(query, key, value)
+        q, q2, k, v = self.forward_qkv(query, key, value)
         q = q.transpose(1, 2)  # (batch, time1, head, d_k)
 
         n_batch_pos = pos_emb.size(0)
diff --git a/espnet/nets/pytorch_backend/transformer/subsampling.py b/espnet/nets/pytorch_backend/transformer/subsampling.py
index a69bc0944..2ccfd99fa 100644
--- a/espnet/nets/pytorch_backend/transformer/subsampling.py
+++ b/espnet/nets/pytorch_backend/transformer/subsampling.py
@@ -60,6 +60,7 @@ class Conv2dSubsampling(torch.nn.Module):
             torch.nn.ReLU(),
             torch.nn.Conv2d(odim, odim, 3, 2),
             torch.nn.ReLU(),
+            torch.nn.ZeroPad2d((0,12,0,0)),
         )
         self.out = torch.nn.Sequential(
             torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),
@@ -83,7 +84,10 @@ class Conv2dSubsampling(torch.nn.Module):
         x = x.unsqueeze(1)  # (b, c, t, f)
         x = self.conv(x)
         b, c, t, f = x.size()
-        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))
+        f = 20
+
+        x = x.transpose(1, 2)[:,:,:,:20]
+        x = self.out(x.contiguous().reshape(b, t, c * f))
         if x_mask is None:
             return x, None
         return x, x_mask[:, :, :-2:2][:, :, :-2:2]
