diff --git a/wenet/transformer/asr_model.py b/wenet/transformer/asr_model.py
index 73990fa..bb3e10f 100644
--- a/wenet/transformer/asr_model.py
+++ b/wenet/transformer/asr_model.py
@@ -158,6 +158,7 @@ class ASRModel(torch.nn.Module):
         decoding_chunk_size: int = -1,
         num_decoding_left_chunks: int = -1,
         simulate_streaming: bool = False,
+        encoder_model=None
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         # Let's assume B = batch_size
         # 1. Encoder
@@ -165,7 +166,8 @@ class ASRModel(torch.nn.Module):
             encoder_out, encoder_mask = self.encoder.forward_chunk_by_chunk(
                 speech,
                 decoding_chunk_size=decoding_chunk_size,
-                num_decoding_left_chunks=num_decoding_left_chunks
+                num_decoding_left_chunks=num_decoding_left_chunks,
+                encoder_model=encoder_model
             )  # (B, maxlen, encoder_dim)
         else:
             encoder_out, encoder_mask = self.encoder(
@@ -443,6 +445,57 @@ class ASRModel(torch.nn.Module):
                                                simulate_streaming)
         return hyps[0][0]
 
+    def get_encoder_flash_data(
+        self,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
+        beam_size: int,
+        decoding_chunk_size: int = -1,
+        num_decoding_left_chunks: int = -1,
+        ctc_weight: float = 0.0,
+        simulate_streaming: bool = False,
+        reverse_weight: float = 0.0,
+        encoder_model=None
+    ) -> List[int]:
+        """ Apply attention rescoring decoding, CTC prefix beam search
+            is applied first to get nbest, then we resoring the nbest on
+            attention decoder with corresponding encoder out
+
+        Args:
+            speech (torch.Tensor): (batch, max_len, feat_dim)
+            speech_length (torch.Tensor): (batch, )
+            beam_size (int): beam size for beam search
+            decoding_chunk_size (int): decoding chunk for dynamic chunk
+                trained model.
+                <0: for decoding, use full chunk.
+                >0: for decoding, use fixed chunk size as set.
+                0: used for training, it's prohibited here
+            simulate_streaming (bool): whether do encoder forward in a
+                streaming fashion
+            reverse_weight (float): right to left decoder weight
+            ctc_weight (float): ctc score weight
+
+        Returns:
+            List[int]: Attention rescoring result
+        """
+        assert speech.shape[0] == speech_lengths.shape[0]
+        assert decoding_chunk_size != 0
+        if reverse_weight > 0.0:
+            # decoder should be a bitransformer decoder if reverse_weight > 0.0
+            assert hasattr(self.decoder, 'right_decoder')
+        device = speech.device
+        batch_size = speech.shape[0]
+        # For attention rescoring we only support batch_size=1
+        assert batch_size == 1
+        # encoder_out: (1, maxlen, encoder_dim), len(hyps) = beam_size
+
+        encoder_out, encoder_mask = self._forward_encoder(
+            speech, speech_lengths, decoding_chunk_size,
+            num_decoding_left_chunks,simulate_streaming,
+            encoder_model=encoder_model)  # (B, maxlen, encoder_dim)
+        return encoder_out, encoder_mask
+
+
     def attention_rescoring(
         self,
         speech: torch.Tensor,
diff --git a/wenet/transformer/encoder.py b/wenet/transformer/encoder.py
index e342ed4..a105abf 100644
--- a/wenet/transformer/encoder.py
+++ b/wenet/transformer/encoder.py
@@ -26,7 +26,9 @@ from wenet.utils.common import get_activation
 from wenet.utils.mask import make_pad_mask
 from wenet.utils.mask import add_optional_chunk_mask
 
-
+import acl
+from wenet.transformer.acl_net import Net
+import numpy as np
 class BaseEncoder(torch.nn.Module):
     def __init__(
         self,
@@ -254,6 +256,7 @@ class BaseEncoder(torch.nn.Module):
         xs: torch.Tensor,
         decoding_chunk_size: int,
         num_decoding_left_chunks: int = -1,
+        encoder_model=None
     ) -> Tuple[torch.Tensor, torch.Tensor]:
         """ Forward input chunk by chunk with chunk_size like a streaming
             fashion
@@ -295,19 +298,25 @@ class BaseEncoder(torch.nn.Module):
         outputs = []
         offset = 0
         required_cache_size = decoding_chunk_size * num_decoding_left_chunks
-
+        subsampling_cache_om = torch.zeros(1, 1, 256, requires_grad=False)
+        elayers_output_cache_om = torch.zeros(12, 1, 1, 256, requires_grad=False)
+        conformer_cnn_cache_om = torch.zeros(12, 1, 256, 7, requires_grad=False)
         # Feed forward overlap input step by step
         for cur in range(0, num_frames - context + 1, stride):
             end = min(cur + decoding_window, num_frames)
-            chunk_xs = xs[:, cur:end, :]
-            (y, subsampling_cache, elayers_output_cache,
-             conformer_cnn_cache) = self.forward_chunk(chunk_xs, offset,
-                                                       required_cache_size,
-                                                       subsampling_cache,
-                                                       elayers_output_cache,
-                                                       conformer_cnn_cache)
+            chunk_xs = xs[:, cur:num_frames, :]
+            if offset > 0:
+                offset = offset - 1
+            offset = offset + 1
+            encoder_output, exe_time = encoder_model(
+                [chunk_xs.cpu().numpy(), np.array(offset), subsampling_cache_om.cpu().numpy(), \
+                 elayers_output_cache_om.cpu().numpy(), conformer_cnn_cache_om.cpu().numpy()])
+            y, subsampling_cache_om, elayers_output_cache_om, conformer_cnn_cache_om = \
+                torch.from_numpy(encoder_output[0][:, 1:, :]), torch.from_numpy(encoder_output[1]), \
+                torch.from_numpy(encoder_output[2]), torch.from_numpy(encoder_output[3])
             outputs.append(y)
             offset += y.size(1)
+            break
         ys = torch.cat(outputs, 1)
         masks = torch.ones(1, ys.size(1), device=ys.device, dtype=torch.bool)
         masks = masks.unsqueeze(1)
