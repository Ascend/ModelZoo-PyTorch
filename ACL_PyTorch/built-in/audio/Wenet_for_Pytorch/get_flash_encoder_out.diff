diff --git a/wenet/transformer/asr_model.py b/wenet/transformer/asr_model.py
index 73990fa..cbb1dc9 100644
--- a/wenet/transformer/asr_model.py
+++ b/wenet/transformer/asr_model.py
@@ -443,6 +443,58 @@ class ASRModel(torch.nn.Module):
                                                simulate_streaming)
         return hyps[0][0]
 
+    def get_encoder_flash_data(
+        self,
+        speech: torch.Tensor,
+        speech_lengths: torch.Tensor,
+        beam_size: int,
+        decoding_chunk_size: int = -1,
+        num_decoding_left_chunks: int = -1,
+        ctc_weight: float = 0.0,
+        simulate_streaming: bool = False,
+        reverse_weight: float = 0.0,
+    ) -> List[int]:
+        """ Apply attention rescoring decoding, CTC prefix beam search
+            is applied first to get nbest, then we resoring the nbest on
+            attention decoder with corresponding encoder out
+
+        Args:
+            speech (torch.Tensor): (batch, max_len, feat_dim)
+            speech_length (torch.Tensor): (batch, )
+            beam_size (int): beam size for beam search
+            decoding_chunk_size (int): decoding chunk for dynamic chunk
+                trained model.
+                <0: for decoding, use full chunk.
+                >0: for decoding, use fixed chunk size as set.
+                0: used for training, it's prohibited here
+            simulate_streaming (bool): whether do encoder forward in a
+                streaming fashion
+            reverse_weight (float): right to left decoder weight
+            ctc_weight (float): ctc score weight
+
+        Returns:
+            List[int]: Attention rescoring result
+        """
+        assert speech.shape[0] == speech_lengths.shape[0]
+        assert decoding_chunk_size != 0
+        if reverse_weight > 0.0:
+            # decoder should be a bitransformer decoder if reverse_weight > 0.0
+            assert hasattr(self.decoder, 'right_decoder')
+        device = speech.device
+        batch_size = speech.shape[0]
+        # For attention rescoring we only support batch_size=1
+        assert batch_size == 1
+        # encoder_out: (1, maxlen, encoder_dim), len(hyps) = beam_size
+        hyps, encoder_out = self._ctc_prefix_beam_search(
+            speech, speech_lengths, beam_size, decoding_chunk_size,
+            num_decoding_left_chunks, simulate_streaming)
+
+        encoder_out, encoder_mask = self._forward_encoder(
+            speech, speech_lengths, decoding_chunk_size,
+            num_decoding_left_chunks,simulate_streaming)  # (B, maxlen, encoder_dim)
+        return encoder_out, encoder_mask
+
+
     def attention_rescoring(
         self,
         speech: torch.Tensor,
diff --git a/wenet/transformer/encoder.py b/wenet/transformer/encoder.py
index e342ed4..c1724bd 100644
--- a/wenet/transformer/encoder.py
+++ b/wenet/transformer/encoder.py
@@ -26,7 +26,9 @@ from wenet.utils.common import get_activation
 from wenet.utils.mask import make_pad_mask
 from wenet.utils.mask import add_optional_chunk_mask
 
-
+import acl
+from wenet.transformer.acl_net import Net
+import numpy as np
 class BaseEncoder(torch.nn.Module):
     def __init__(
         self,
@@ -117,6 +119,11 @@ class BaseEncoder(torch.nn.Module):
         self.use_dynamic_chunk = use_dynamic_chunk
         self.use_dynamic_left_chunk = use_dynamic_left_chunk
 
+        device_id = 0
+        self.decoder_output_data_shape = 4233000
+        self.encoder_model = Net(model_path="model.om", \
+                                 output_data_shape = self.decoder_output_data_shape, device_id=device_id, )
+
     def output_size(self) -> int:
         return self._output_size
 
@@ -295,17 +302,22 @@ class BaseEncoder(torch.nn.Module):
         outputs = []
         offset = 0
         required_cache_size = decoding_chunk_size * num_decoding_left_chunks
-
+        subsampling_cache_om = torch.zeros(1, 1, 256, requires_grad=False)
+        elayers_output_cache_om = torch.zeros(12, 1, 1, 256, requires_grad=False)
+        conformer_cnn_cache_om = torch.zeros(12, 1, 256, 7, requires_grad=False)
         # Feed forward overlap input step by step
         for cur in range(0, num_frames - context + 1, stride):
             end = min(cur + decoding_window, num_frames)
             chunk_xs = xs[:, cur:end, :]
-            (y, subsampling_cache, elayers_output_cache,
-             conformer_cnn_cache) = self.forward_chunk(chunk_xs, offset,
-                                                       required_cache_size,
-                                                       subsampling_cache,
-                                                       elayers_output_cache,
-                                                       conformer_cnn_cache)
+            if offset > 0:
+                offset = offset - 1
+            offset = offset + 1
+            encoder_output, exe_time = self.encoder_model(
+                [chunk_xs.cpu().numpy(), np.array(offset), subsampling_cache_om.cpu().numpy(), \
+                 elayers_output_cache_om.cpu().numpy(), conformer_cnn_cache_om.cpu().numpy()])
+            y, subsampling_cache_om, elayers_output_cache_om, conformer_cnn_cache_om = \
+                torch.from_numpy(encoder_output[0][:, 1:, :]), torch.from_numpy(encoder_output[1]), \
+                torch.from_numpy(encoder_output[2]), torch.from_numpy(encoder_output[3])
             outputs.append(y)
             offset += y.size(1)
         ys = torch.cat(outputs, 1)
