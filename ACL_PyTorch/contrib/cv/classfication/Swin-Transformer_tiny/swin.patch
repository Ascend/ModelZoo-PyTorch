diff --git a/config.py b/config.py
index d9c85c3..74a027e 100644
--- a/config.py
+++ b/config.py
@@ -228,11 +228,10 @@ def update_config(config, args):
         config.EVAL_MODE = True
     if args.throughput:
         config.THROUGHPUT_MODE = True
-
     # set local rank for distributed training
-    config.LOCAL_RANK = args.local_rank
 
     # output folder
+    config.BIN_PATH= args.bin_path
     config.OUTPUT = os.path.join(config.OUTPUT, config.MODEL.NAME, config.TAG)
 
     config.freeze()
diff --git a/main.py b/main.py
index 18e9ad6..1cb8cae 100644
--- a/main.py
+++ b/main.py
@@ -35,7 +35,7 @@ except ImportError:
 
 def parse_option():
     parser = argparse.ArgumentParser('Swin Transformer training and evaluation script', add_help=False)
-    parser.add_argument('--cfg', type=str, required=True, metavar="FILE", help='path to config file', )
+    parser.add_argument('--cfg', type=str, metavar="FILE", help='path to config file', )
     parser.add_argument(
         "--opts",
         help="Modify config options by adding 'KEY VALUE' pairs. ",
@@ -62,9 +62,7 @@ def parse_option():
     parser.add_argument('--tag', help='tag of experiment')
     parser.add_argument('--eval', action='store_true', help='Perform evaluation only')
     parser.add_argument('--throughput', action='store_true', help='Test throughput only')
-
-    # distributed training
-    parser.add_argument("--local_rank", type=int, required=True, help='local rank for DistributedDataParallel')
+    parser.add_argument('--bin_path', default='./swin_bin', help='store bin data')
 
     args, unparsed = parser.parse_known_args()
 
@@ -344,4 +342,4 @@ if __name__ == '__main__':
     # print config
     logger.info(config.dump())
 
-    main(config)
+    main(config)
\ No newline at end of file
diff --git a/models/swin_transformer.py b/models/swin_transformer.py
index cfeb0f2..cfd4ad1 100644
--- a/models/swin_transformer.py
+++ b/models/swin_transformer.py
@@ -40,12 +40,16 @@ def window_partition(x, window_size):
         windows: (num_windows*B, window_size, window_size, C)
     """
     B, H, W, C = x.shape
+    B = int(B)
+    H = int(H)
+    W = int(W)
+    C = int(C)
     x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
     windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
     return windows
 
 
-def window_reverse(windows, window_size, H, W):
+def window_reverse(windows, window_size, H, W, B):
     """
     Args:
         windows: (num_windows*B, window_size, window_size, C)
@@ -56,7 +60,6 @@ def window_reverse(windows, window_size, H, W):
     Returns:
         x: (B, H, W, C)
     """
-    B = int(windows.shape[0] / (H * W / window_size / window_size))
     x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
     x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
     return x
@@ -110,6 +113,18 @@ class WindowAttention(nn.Module):
         trunc_normal_(self.relative_position_bias_table, std=.02)
         self.softmax = nn.Softmax(dim=-1)
 
+    def bmm_replace(self, q, k):
+        B, num_heads, N, D1 = q.shape
+        B = int(B)
+        num_heads = int(num_heads)
+        N = int(N)
+        D1 = int(D1)
+        D2 = int(k.shape[3])
+        q_r = q.reshape(B * num_heads, N, D1)
+        k_r = k.reshape(B * num_heads, D1, D2)
+        attn = torch.bmm(q_r, k_r)
+        return attn.reshape(B, num_heads, N, D2)
+
     def forward(self, x, mask=None):
         """
         Args:
@@ -117,19 +132,21 @@ class WindowAttention(nn.Module):
             mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
         """
         B_, N, C = x.shape
+        B_ = int(B_)
+        N = int(N)
+        C = int(C)
         qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
         q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)
-
-        q = q * self.scale
+        q = self.scale * q
         attn = (q @ k.transpose(-2, -1))
-
-        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
-            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
+        relative_index=self.relative_position_index.view(-1)
+        relative_position_bias = self.relative_position_bias_table.index_select(0,relative_index.long())
+        relative_position_bias = relative_position_bias.view(self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
         relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
         attn = attn + relative_position_bias.unsqueeze(0)
 
         if mask is not None:
-            nW = mask.shape[0]
+            nW = int(mask.shape[0])
             attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
             attn = attn.view(-1, self.num_heads, N, N)
             attn = self.softmax(attn)
@@ -137,7 +154,6 @@ class WindowAttention(nn.Module):
             attn = self.softmax(attn)
 
         attn = self.attn_drop(attn)
-
         x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
         x = self.proj(x)
         x = self.proj_drop(x)
@@ -193,7 +209,6 @@ class SwinTransformerBlock(nn.Module):
             # if window size is larger than input resolution, we don't partition windows
             self.shift_size = 0
             self.window_size = min(self.input_resolution)
-        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"
 
         self.norm1 = norm_layer(dim)
         self.attn = WindowAttention(
@@ -230,10 +245,32 @@ class SwinTransformerBlock(nn.Module):
 
         self.register_buffer("attn_mask", attn_mask)
 
+    def roll_re2(self, x, shifts):
+        if shifts > 0:
+            block_a = x[:, -shifts:, -shifts:, :]  # block A
+            block_b = x[:, 0:-shifts, -shifts:, :]  # block B
+            block_c = x[:, -shifts:, 0:-shifts, :]  # block C
+            block_d = x[:, :-shifts, :-shifts, :]  # block D
+            patch_ab = torch.cat((block_a, block_b), dim=1)
+            patch_cd = torch.cat((block_c, block_d), dim=1)
+            patch = torch.cat((patch_ab, patch_cd), dim=2)
+        else:
+            shifts = shifts * -1
+            block_a = x[:, :shifts, :shifts, :]
+            block_b = x[:, shifts:, :shifts, :]
+            block_c = x[:, :shifts, shifts:, :]
+            block_d = x[:, shifts:, shifts:, :]
+            patch_ba = torch.cat((block_b, block_a), dim=1)
+            patch_dc = torch.cat((block_d, block_c), dim=1)
+            patch = torch.cat((patch_dc, patch_ba), dim=2)
+        return patch
+
     def forward(self, x):
         H, W = self.input_resolution
         B, L, C = x.shape
-        assert L == H * W, "input feature has wrong size"
+        B = int(B)
+        L = int(L)
+        C = int(C)
 
         shortcut = x
         x = self.norm1(x)
@@ -241,7 +278,7 @@ class SwinTransformerBlock(nn.Module):
 
         # cyclic shift
         if self.shift_size > 0:
-            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
+            shifted_x = self.roll_re2(x,-self.shift_size)
         else:
             shifted_x = x
 
@@ -254,11 +291,11 @@ class SwinTransformerBlock(nn.Module):
 
         # merge windows
         attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
-        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
+        shifted_x = window_reverse(attn_windows, self.window_size, H, W, B)  # B H' W' C
 
         # reverse cyclic shift
         if self.shift_size > 0:
-            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
+            x = self.roll_re2(shifted_x, self.shift_size)
         else:
             x = shifted_x
         x = x.view(B, H * W, C)
@@ -310,8 +347,9 @@ class PatchMerging(nn.Module):
         """
         H, W = self.input_resolution
         B, L, C = x.shape
-        assert L == H * W, "input feature has wrong size"
-        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."
+        B = int(B)
+        L = int(L)
+        C = int(C)
 
         x = x.view(B, H, W, C)
 
@@ -438,10 +476,6 @@ class PatchEmbed(nn.Module):
             self.norm = None
 
     def forward(self, x):
-        B, C, H, W = x.shape
-        # FIXME look at relaxing size constraints
-        assert H == self.img_size[0] and W == self.img_size[1], \
-            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
         x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
         if self.norm is not None:
             x = self.norm(x)
@@ -571,6 +605,7 @@ class SwinTransformer(nn.Module):
         return x
 
     def forward(self, x):
+        #print(x.shape)
         x = self.forward_features(x)
         x = self.head(x)
         return x
diff --git a/utils.py b/utils.py
index fd2df67..885597a 100644
--- a/utils.py
+++ b/utils.py
@@ -38,6 +38,8 @@ def load_checkpoint(config, model, optimizer, lr_scheduler, logger):
         if 'max_accuracy' in checkpoint:
             max_accuracy = checkpoint['max_accuracy']
 
+    if config.ONNX_MODE:
+        return checkpoint
     del checkpoint
     torch.cuda.empty_cache()
     return max_accuracy
