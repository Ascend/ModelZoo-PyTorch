diff --git a/detectron2/data/dataset_mapper.py b/detectron2/data/dataset_mapper.py
index a8714f7..655c620 100644
--- a/detectron2/data/dataset_mapper.py
+++ b/detectron2/data/dataset_mapper.py
@@ -4,6 +4,7 @@ import logging
 import numpy as np
 from typing import List, Optional, Union
 import torch
+from torch.nn import functional as F
 
 from detectron2.config import configurable
 
@@ -162,6 +163,7 @@ class DatasetMapper:
 
         aug_input = T.AugInput(image, sem_seg=sem_seg_gt)
         transforms = self.augmentations(aug_input)
+        print(self.augmentations,transforms)
         image, sem_seg_gt = aug_input.image, aug_input.sem_seg
 
         image_shape = image.shape[:2]  # h, w
@@ -169,6 +171,20 @@ class DatasetMapper:
         # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
         # Therefore it's important to use torch.Tensor.
         dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))
+
+        size_divisibility = 32
+        pad_value = 0
+        pixel_mean = torch.Tensor([103.53, 116.28, 123.675]).view(-1, 1, 1)
+        pixel_std = torch.Tensor([1.0, 1.0, 1.0]).view(-1, 1, 1)
+        images = (dataset_dict["image"] - pixel_mean) / pixel_std
+        dataset_dict["image_size"] = tuple(images.shape[-2:])
+        batch_shape = (3, 1344, 1344)
+        padding_size = [0, batch_shape[-1] - images.shape[-1],
+                        0, batch_shape[-2] - images.shape[-2]]
+        padded = F.pad(images, padding_size, value=pad_value)
+        batched_imgs = padded.unsqueeze_(0)
+        dataset_dict["image_preprocess"] = batched_imgs.contiguous()
+
         if sem_seg_gt is not None:
             dataset_dict["sem_seg"] = torch.as_tensor(sem_seg_gt.astype("long"))
 
diff --git a/detectron2/layers/roi_align.py b/detectron2/layers/roi_align.py
index 163462e..0940a6a 100644
--- a/detectron2/layers/roi_align.py
+++ b/detectron2/layers/roi_align.py
@@ -38,7 +38,7 @@ class ROIAlign(nn.Module):
         self.output_size = output_size
         self.spatial_scale = spatial_scale
         self.sampling_ratio = sampling_ratio
-        self.aligned = aligned
+        self.aligned = True
 
         from torchvision import __version__
 
diff --git a/detectron2/modeling/meta_arch/rcnn.py b/detectron2/modeling/meta_arch/rcnn.py
index 7b45363..4d227ee 100644
--- a/detectron2/modeling/meta_arch/rcnn.py
+++ b/detectron2/modeling/meta_arch/rcnn.py
@@ -199,6 +199,9 @@ class GeneralizedRCNN(nn.Module):
         images = self.preprocess_image(batched_inputs)
         features = self.backbone(images.tensor)
 
+        #from torchvision import utils as vutils
+        #vutils.save_image(images.tensor, 'test.jpg')
+        print(features['p2'].shape)
         if detected_instances is None:
             if self.proposal_generator is not None:
                 proposals, _ = self.proposal_generator(images, features, None)
@@ -221,10 +224,14 @@ class GeneralizedRCNN(nn.Module):
         """
         Normalize, pad and batch the input images.
         """
-        images = [x["image"].to(self.device) for x in batched_inputs]
+        '''images = [x["image"].to(self.device) for x in batched_inputs]
         images = [(x - self.pixel_mean) / self.pixel_std for x in images]
         images = ImageList.from_tensors(images, self.backbone.size_divisibility)
-        return images
+        return images'''
+        images = [x["image_preprocess"].to(device=self.device) for x in batched_inputs]
+        images = torch.cat(images, dim=0)
+        image_sizes = [x["image_size"] for x in batched_inputs]
+        return ImageList(images, image_sizes)
 
     @staticmethod
     def _postprocess(instances, batched_inputs: List[Dict[str, torch.Tensor]], image_sizes):
