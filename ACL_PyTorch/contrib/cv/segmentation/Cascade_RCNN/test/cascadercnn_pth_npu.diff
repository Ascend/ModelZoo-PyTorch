diff --git a/detectron2/data/dataset_mapper.py b/detectron2/data/dataset_mapper.py
index 0e7785185b2a710a9f3b8c6577367bc860340b98..1962a7a5bda9ccdc268c62f1c5d1bb615088a622 100644
--- a/detectron2/data/dataset_mapper.py
+++ b/detectron2/data/dataset_mapper.py
@@ -4,7 +4,7 @@ import logging
 import numpy as np
 from typing import List, Optional, Union
 import torch
-
+from torch.nn import functional as F
 from detectron2.config import configurable
 
 from . import detection_utils as utils
@@ -133,6 +133,7 @@ class DatasetMapper:
 
         aug_input = T.AugInput(image, sem_seg=sem_seg_gt)
         transforms = self.augmentations(aug_input)
+        print(self.augmentations,transforms)
         image, sem_seg_gt = aug_input.image, aug_input.sem_seg
 
         image_shape = image.shape[:2]  # h, w
@@ -140,6 +141,19 @@ class DatasetMapper:
         # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
         # Therefore it's important to use torch.Tensor.
         dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))
+        size_divisibility = 32
+        pad_value = 0
+        pixel_mean = torch.Tensor([103.53, 116.28, 123.675]).view(-1, 1, 1)
+        pixel_std = torch.Tensor([1.0, 1.0, 1.0]).view(-1, 1, 1)
+        images = (dataset_dict["image"] - pixel_mean) / pixel_std
+        dataset_dict["image_size"] = tuple(images.shape[-2:])
+        batch_shape = (3, 1344, 1344)
+        padding_size = [0, batch_shape[-1] - images.shape[-1],
+                        0, batch_shape[-2] - images.shape[-2]]
+        padded = F.pad(images, padding_size, value=pad_value)
+        batched_imgs = padded.unsqueeze_(0)
+        dataset_dict["image_preprocess"] = batched_imgs.contiguous()
+        
         if sem_seg_gt is not None:
             dataset_dict["sem_seg"] = torch.as_tensor(sem_seg_gt.astype("long"))
 
diff --git a/detectron2/data/datasets/builtin.py b/detectron2/data/datasets/builtin.py
index aa1d8628992e96804b763ee9ccbb6a38881382c7..4b24576d1cfd87095d2766f67d897b0492888d5c 100644
--- a/detectron2/data/datasets/builtin.py
+++ b/detectron2/data/datasets/builtin.py
@@ -255,7 +255,7 @@ def register_all_ade20k(root):
 # Internally at fb, we register them elsewhere
 if __name__.endswith(".builtin"):
     # Assume pre-defined datasets live in `./datasets`.
-    _root = os.getenv("DETECTRON2_DATASETS", "datasets")
+    _root = os.getenv("DETECTRON2_DATASETS", "/root/datasets/")
     register_all_coco(_root)
     register_all_lvis(_root)
     register_all_cityscapes(_root)
diff --git a/detectron2/layers/roi_align.py b/detectron2/layers/roi_align.py
index 163462e1f194e1e4100da92d76d9516f7cc22e35..0940a6a2e0e19590aeaf4d836c4ee1785a12f671 100644
--- a/detectron2/layers/roi_align.py
+++ b/detectron2/layers/roi_align.py
@@ -38,7 +38,7 @@ class ROIAlign(nn.Module):
         self.output_size = output_size
         self.spatial_scale = spatial_scale
         self.sampling_ratio = sampling_ratio
-        self.aligned = aligned
+        self.aligned = True
 
         from torchvision import __version__
 
diff --git a/detectron2/modeling/meta_arch/rcnn.py b/detectron2/modeling/meta_arch/rcnn.py
index 7b45363e6eba306c519b5deeca2bc38d6535cec8..74f61918a3327fd3a606465d908875fe40a2a147 100644
--- a/detectron2/modeling/meta_arch/rcnn.py
+++ b/detectron2/modeling/meta_arch/rcnn.py
@@ -152,7 +152,9 @@ class GeneralizedRCNN(nn.Module):
             gt_instances = None
 
         features = self.backbone(images.tensor)
-
+        #from torchvision import utils as vutils
+        #vutils.save_image(images.tensor, 'test.jpg')
+        print(features['p2'].shape)
         if self.proposal_generator is not None:
             proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)
         else:
@@ -221,10 +223,16 @@ class GeneralizedRCNN(nn.Module):
         """
         Normalize, pad and batch the input images.
         """
+        '''
         images = [x["image"].to(self.device) for x in batched_inputs]
         images = [(x - self.pixel_mean) / self.pixel_std for x in images]
         images = ImageList.from_tensors(images, self.backbone.size_divisibility)
         return images
+        '''
+        images = [x["image_preprocess"].to(device=self.device) for x in batched_inputs]
+        images = torch.cat(images, dim=0)
+        image_sizes = [x["image_size"] for x in batched_inputs]
+        return ImageList(images, image_sizes)
 
     @staticmethod
     def _postprocess(instances, batched_inputs: List[Dict[str, torch.Tensor]], image_sizes):
diff --git a/detectron2/modeling/postprocessing.py b/detectron2/modeling/postprocessing.py
index 1a3d287eeb6c2cb3070f1aa7157b006e9aa857f5..e113ecf712d0350911f854420957d61b4f2846bf 100644
--- a/detectron2/modeling/postprocessing.py
+++ b/detectron2/modeling/postprocessing.py
@@ -52,7 +52,7 @@ def detector_postprocess(
     else:
         output_boxes = None
     assert output_boxes is not None, "Predictions must contain boxes!"
-
+    print(scale_x, scale_y)
     output_boxes.scale(scale_x, scale_y)
     output_boxes.clip(results.image_size)
 
