diff --git a/detectron2/data/dataset_mapper.py b/detectron2/data/dataset_mapper.py
index 0e77851..3599741 100644
--- a/detectron2/data/dataset_mapper.py
+++ b/detectron2/data/dataset_mapper.py
@@ -9,7 +9,7 @@ from detectron2.config import configurable
 
 from . import detection_utils as utils
 from . import transforms as T
-
+from torch.nn import functional as F
 """
 This file contains the default mapping that's applied to "dataset dicts".
 """
@@ -140,6 +140,18 @@ class DatasetMapper:
         # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
         # Therefore it's important to use torch.Tensor.
         dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))
+        size_divisibility = 32
+        pad_value = 0
+        pixel_mean = torch.Tensor([103.53, 116.28, 123.675]).view(-1, 1, 1)
+        pixel_std = torch.Tensor([1.0, 1.0, 1.0]).view(-1, 1, 1)
+        images = (dataset_dict["image"] - pixel_mean) / pixel_std
+        dataset_dict["image_size"] = tuple(images.shape[-2:])
+        batch_shape = (3, 1344, 1344)
+        padding_size = [0, batch_shape[-1] - images.shape[-1],
+                        0, batch_shape[-2] - images.shape[-2]]
+        padded = F.pad(images, padding_size, value=pad_value)
+        batched_imgs = padded.unsqueeze_(0)
+        dataset_dict["image_preprocess"] = batched_imgs.contiguous()
         if sem_seg_gt is not None:
             dataset_dict["sem_seg"] = torch.as_tensor(sem_seg_gt.astype("long"))
 
diff --git a/detectron2/evaluation/evaluator.py b/detectron2/evaluation/evaluator.py
index 60cc981..8eef7af 100644
--- a/detectron2/evaluation/evaluator.py
+++ b/detectron2/evaluation/evaluator.py
@@ -10,7 +10,7 @@ from torch import nn
 
 from detectron2.utils.comm import get_world_size, is_main_process
 from detectron2.utils.logger import log_every_n_seconds
-
+from detectron2.modeling.postprocessing import detector_postprocess
 
 class DatasetEvaluator:
     """
@@ -155,12 +155,20 @@ def inference_on_dataset(
                 total_eval_time = 0
 
             start_compute_time = time.perf_counter()
-            outputs = model(inputs)
+            results, batched_inputs, images = model(inputs)
             if torch.cuda.is_available():
                 torch.cuda.synchronize()
             total_compute_time += time.perf_counter() - start_compute_time
 
             start_eval_time = time.perf_counter()
+            outputs = []
+            for results_per_image, input_per_image, image_size in zip(
+                results, batched_inputs, images.image_sizes
+            ):
+                height = input_per_image.get("height", image_size[0])
+                width = input_per_image.get("width", image_size[1])
+                r = detector_postprocess(results_per_image, height, width)
+                outputs.append({"instances": r})
             evaluator.process(inputs, outputs)
             total_eval_time += time.perf_counter() - start_eval_time
 
diff --git a/detectron2/modeling/meta_arch/retinanet.py b/detectron2/modeling/meta_arch/retinanet.py
index 81992a3..af8b7d8 100644
--- a/detectron2/modeling/meta_arch/retinanet.py
+++ b/detectron2/modeling/meta_arch/retinanet.py
@@ -279,15 +279,7 @@ class RetinaNet(nn.Module):
             results = self.inference(anchors, pred_logits, pred_anchor_deltas, images.image_sizes)
             if torch.jit.is_scripting():
                 return results
-            processed_results = []
-            for results_per_image, input_per_image, image_size in zip(
-                results, batched_inputs, images.image_sizes
-            ):
-                height = input_per_image.get("height", image_size[0])
-                width = input_per_image.get("width", image_size[1])
-                r = detector_postprocess(results_per_image, height, width)
-                processed_results.append({"instances": r})
-            return processed_results
+            return results, batched_inputs, images
 
     def losses(self, anchors, pred_logits, gt_labels, pred_anchor_deltas, gt_boxes):
         """
@@ -489,10 +481,10 @@ class RetinaNet(nn.Module):
         """
         Normalize, pad and batch the input images.
         """
-        images = [x["image"].to(self.device) for x in batched_inputs]
-        images = [(x - self.pixel_mean) / self.pixel_std for x in images]
-        images = ImageList.from_tensors(images, self.backbone.size_divisibility)
-        return images
+        images = [x["image_preprocess"].to(device=self.device) for x in batched_inputs]
+        images = torch.cat(images, dim=0)
+        image_sizes = [x["image_size"] for x in batched_inputs]
+        return ImageList(images, image_sizes)
 
 
 class RetinaNetHead(nn.Module):
diff --git a/detectron2/structures/image_list.py b/detectron2/structures/image_list.py
index 26e6e49..a52e734 100644
--- a/detectron2/structures/image_list.py
+++ b/detectron2/structures/image_list.py
@@ -57,6 +57,7 @@ class ImageList(object):
             Tensor: an image of shape (H, W) or (C_1, ..., C_K, H, W) where K >= 1
         """
         size = self.image_sizes[idx]
+        return self.tensor[idx, ..., : , : ]
         return self.tensor[idx, ..., : size[0], : size[1]]
 
     @torch.jit.unused
@@ -111,9 +112,14 @@ class ImageList(object):
         if len(tensors) == 1:
             # This seems slightly (2%) faster.
             # TODO: check whether it's faster for multiple images as well
-            image_size = image_sizes[0]
-            padding_size = [0, max_size[-1] - image_size[1], 0, max_size[-2] - image_size[0]]
-            batched_imgs = F.pad(tensors[0], padding_size, value=pad_value).unsqueeze_(0)
+            # image_size = image_sizes[0]
+            # padding_size = [0, max_size[-1] - image_size[1], 0, max_size[-2] - image_size[0]]
+            # batched_imgs = F.pad(tensors[0], padding_size, value=pad_value).unsqueeze_(0)
+            max_size = [1344, 1344]
+            batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + max_size
+            batched_imgs = tensors[0].new_full(batch_shape, pad_value)
+            for img, pad_img in zip(tensors, batched_imgs):
+                pad_img[..., : img.shape[-2], : img.shape[-1]].copy_(img)
         else:
             # max_size can be a tensor in tracing mode, therefore convert to list
             batch_shape = [len(tensors)] + list(tensors[0].shape[:-2]) + list(max_size)
diff --git a/tools/train_net.py b/tools/train_net.py
index 564cbb5..2fdd7d5 100755
--- a/tools/train_net.py
+++ b/tools/train_net.py
@@ -20,7 +20,7 @@ import logging
 import os
 from collections import OrderedDict
 import torch
-
+print(torch.cuda.is_available())
 import detectron2.utils.comm as comm
 from detectron2.checkpoint import DetectionCheckpointer
 from detectron2.config import get_cfg
