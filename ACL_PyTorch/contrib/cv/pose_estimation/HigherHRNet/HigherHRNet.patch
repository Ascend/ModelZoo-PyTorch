diff --git a/lib/core/group.py b/lib/core/group.py
index f09be8f..03e2ca6 100644
--- a/lib/core/group.py
+++ b/lib/core/group.py
@@ -162,7 +162,7 @@ class HeatmapParser(object):
         )
 
         x = ind % w
-        y = (ind / w).long()
+        y = (ind // w).long()
 
         ind_k = torch.stack((x, y), dim=3)
 
@@ -201,7 +201,7 @@ class HeatmapParser(object):
         :param det: numpy.ndarray of size (17, 128, 128)
         :param tag: numpy.ndarray of size (17, 128, 128) if not flip
         :param keypoints: numpy.ndarray of size (17, 4) if not flip, last dim is (x, y, det score, tag score)
-        :return: 
+        :return:
         """
         if len(tag.shape) == 3:
             # tag shape: (17, 128, 128, 1)
diff --git a/lib/core/inference.py b/lib/core/inference.py
index fbc427e..49aa73d 100644
--- a/lib/core/inference.py
+++ b/lib/core/inference.py
@@ -12,7 +12,7 @@ from __future__ import print_function
 
 import torch
 
-from dataset.transforms import FLIP_CONFIG
+from lib.dataset.transforms.build import FLIP_CONFIG
 
 
 def get_outputs(
diff --git a/lib/dataset/COCODataset.py b/lib/dataset/COCODataset.py
index 265c54a..793dff7 100644
--- a/lib/dataset/COCODataset.py
+++ b/lib/dataset/COCODataset.py
@@ -21,7 +21,7 @@ import numpy as np
 from torch.utils.data import Dataset
 
 from pycocotools.cocoeval import COCOeval
-from utils import zipreader
+from lib.utils import zipreader
 
 logger = logging.getLogger(__name__)
 
@@ -163,9 +163,9 @@ class CocoDataset(Dataset):
         :param cfg: cfg dictionary
         :param preds: prediction
         :param output_dir: output directory
-        :param args: 
-        :param kwargs: 
-        :return: 
+        :param args:
+        :param kwargs:
+        :return:
         '''
         res_folder = os.path.join(output_dir, 'results')
         if not os.path.exists(res_folder):
diff --git a/lib/dataset/COCOKeypoints.py b/lib/dataset/COCOKeypoints.py
index 9957720..0a2b562 100644
--- a/lib/dataset/COCOKeypoints.py
+++ b/lib/dataset/COCOKeypoints.py
@@ -14,8 +14,8 @@ import logging
 import numpy as np
 
 import pycocotools
-from .COCODataset import CocoDataset
-from .target_generators import HeatmapGenerator
+from lib.dataset.COCODataset import CocoDataset
+from lib.dataset.target_generators.target_generators import HeatmapGenerator
 
 
 logger = logging.getLogger(__name__)
diff --git a/lib/dataset/__init__.py b/lib/dataset/__init__.py
index 1d32d71..e1fa8c2 100644
--- a/lib/dataset/__init__.py
+++ b/lib/dataset/__init__.py
@@ -5,7 +5,6 @@
 # ------------------------------------------------------------------------------
 
 from .COCOKeypoints import CocoKeypoints as coco
-from .CrowdPoseKeypoints import CrowdPoseKeypoints as crowd_pose
 from .build import make_dataloader
 from .build import make_test_dataloader
 
diff --git a/lib/dataset/build.py b/lib/dataset/build.py
index 5b45ce3..f443b56 100644
--- a/lib/dataset/build.py
+++ b/lib/dataset/build.py
@@ -11,14 +11,11 @@ from __future__ import print_function
 
 import torch.utils.data
 
-from .COCODataset import CocoDataset as coco
-from .COCOKeypoints import CocoKeypoints as coco_kpt
-from .CrowdPoseDataset import CrowdPoseDataset as crowd_pose
-from .CrowdPoseKeypoints import CrowdPoseKeypoints as crowd_pose_kpt
-from .transforms import build_transforms
-from .target_generators import HeatmapGenerator
-from .target_generators import ScaleAwareHeatmapGenerator
-from .target_generators import JointsGenerator
+from lib.dataset.COCODataset import CocoDataset as coco
+from lib.dataset.transforms.build import build_transforms
+from lib.dataset.target_generators.target_generators import HeatmapGenerator
+from lib.dataset.target_generators.target_generators import ScaleAwareHeatmapGenerator
+from lib.dataset.target_generators.target_generators import JointsGenerator
 
 
 def build_dataset(cfg, is_train):
diff --git a/lib/models/__init__.py b/lib/models/__init__.py
index 11df676..a98ab6f 100644
--- a/lib/models/__init__.py
+++ b/lib/models/__init__.py
@@ -8,4 +8,3 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import models.pose_higher_hrnet
diff --git a/lib/utils/transforms.py b/lib/utils/transforms.py
index 8f366d9..e705ea0 100644
--- a/lib/utils/transforms.py
+++ b/lib/utils/transforms.py
@@ -152,33 +152,67 @@ def resize(image, input_size):
     return image_resized, center, scale
 
 
-def get_multi_scale_size(image, input_size, current_scale, min_scale):
+def get_nearest_boader(value, value_list):
+    value_list = sorted(value_list)
+    if value_list[0] > value:
+        print("warning:{}->{}".format(value, value_list[0]))
+        return value_list[0]
+    if value_list[-1] < value:
+        print("warning:{}->{}".format(value, value_list[-1]))
+        return value_list[-1]
+    left = 0
+    right = len(value_list)
+    while left < right:
+        mid = (right - left) // 2 + left
+        if value_list[mid] == value:
+            return value
+        elif value_list[mid] < value:
+            left = mid + 1
+        elif value_list[mid] > value:
+            right = mid
+
+    if left + 1 < len(value_list):
+        if abs(value_list[left] - value) > abs(value_list[left + 1] - value):
+            return value_list[left + 1]
+    return value_list[left]
+
+
+def get_multi_scale_size(image, input_size, current_scale, min_scale, scale_list):
     h, w, _ = image.shape
     center = np.array([int(w / 2.0 + 0.5), int(h / 2.0 + 0.5)])
 
     # calculate the size for min_scale
-    min_input_size = int((min_scale * input_size + 63)//64 * 64)
+    min_input_size = int((min_scale * input_size + 63) // 64 * 64)
+
     if w < h:
         w_resized = int(min_input_size * current_scale / min_scale)
-        h_resized = int(
-            int((min_input_size/w*h+63)//64*64)*current_scale/min_scale
+        assert w_resized == 512
+        h_resized_ori = int(
+            int((min_input_size / w * h + 63) // 64 * 64) * current_scale / min_scale
         )
+        # change h_resized to nearest value in scale_list
+        h_resized = get_nearest_boader(h_resized_ori, scale_list)
+
         scale_w = w / 200.0
         scale_h = h_resized / w_resized * w / 200.0
     else:
         h_resized = int(min_input_size * current_scale / min_scale)
-        w_resized = int(
-            int((min_input_size/h*w+63)//64*64)*current_scale/min_scale
+        assert h_resized == 512
+        w_resized_ori = int(
+            int((min_input_size / h * w + 63) // 64 * 64) * current_scale / min_scale
         )
+        # change h_resized to nearest value in scale_list
+        w_resized = get_nearest_boader(w_resized_ori, scale_list)
+
         scale_h = h / 200.0
         scale_w = w_resized / h_resized * h / 200.0
 
     return (w_resized, h_resized), center, np.array([scale_w, scale_h])
 
 
-def resize_align_multi_scale(image, input_size, current_scale, min_scale):
+def resize_align_multi_scale(image, input_size, current_scale, min_scale, scale_list):
     size_resized, center, scale = get_multi_scale_size(
-        image, input_size, current_scale, min_scale
+        image, input_size, current_scale, min_scale, scale_list
     )
     trans = get_affine_transform(center, scale, 0, size_resized)
 
@@ -186,7 +220,6 @@ def resize_align_multi_scale(image, input_size, current_scale, min_scale):
         image,
         trans,
         size_resized
-        # (int(w_resized), int(h_resized))
     )
 
     return image_resized, center, scale
diff --git a/lib/utils/vis.py b/lib/utils/vis.py
index 69a1f77..a6ad0bb 100755
--- a/lib/utils/vis.py
+++ b/lib/utils/vis.py
@@ -15,7 +15,7 @@ import cv2
 import numpy as np
 import torchvision
 
-from dataset import VIS_CONFIG
+from lib.dataset._init_ import VIS_CONFIG
 
 
 def add_joints(image, joints, color, dataset='COCO'):
