diff --git a/fairseq/models/transformer/transformer_decoder.py b/fairseq/models/transformer/transformer_decoder.py
index 1a0f978..f1e73c0 100644
--- a/fairseq/models/transformer/transformer_decoder.py
+++ b/fairseq/models/transformer/transformer_decoder.py
@@ -188,6 +188,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         prev_output_tokens,
         encoder_out: Optional[Dict[str, List[Tensor]]] = None,
         incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,
+        incremental_state_step: Tensor = None,
+        incremental_state_sentence: Tensor = None,
+        incremental_state_mask: Tensor = None,
         features_only: bool = False,
         full_context_alignment: bool = False,
         alignment_layer: Optional[int] = None,
@@ -213,6 +216,22 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
                 - the decoder's output of shape `(batch, tgt_len, vocab)`
                 - a dictionary with any model-specific outputs
         """
+        # modification for onnx export
+        # beam_size = 5, num_heads = 16, head_dim = 64
+        if torch.onnx.is_in_onnx_export() and incremental_state:
+            beams = 5 # batchsize * beamsize
+            i = 0
+            j = 0
+            for k,v in incremental_state.items():
+                if isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    v['prev_key_padding_mask'] = incremental_state_mask[i//2 * beams : (i//2+1) * beams, :]
+                    v['prev_key'] = incremental_state_sentence[i*beams : (i+1)*beams, :, :, :]
+                    v['prev_value'] = incremental_state_sentence[(i+1)*beams : (i+2)*beams, :, :, :]
+                    i += 2
+                else:
+                    v['prev_key'] = incremental_state_step[j*beams : (j+1)*beams, :, :, :]
+                    v['prev_value'] = incremental_state_step[(j+1)*beams : (j+2)*beams, :, :, :]
+                    j += 2
 
         x, extra = self.extract_features(
             prev_output_tokens,
@@ -367,7 +386,16 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         if self.project_out_dim is not None:
             x = self.project_out_dim(x)
 
-        return x, {"attn": [attn], "inner_states": inner_states}
+        # return x, {"attn": [attn], "inner_states": inner_states}
+
+        is_step_list = []
+        for k,v in incremental_state.items():
+            if not isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                is_step_list.append(v['prev_key'][:, :, -1:, :])
+                is_step_list.append(v['prev_value'][:, :, -1:, :])
+        
+        is_step = torch.cat(is_step_list, 0)
+        return x, {"attn": [attn], "is_step": is_step}
 
     def output_layer(self, features):
         """Project features to the vocabulary size."""
@@ -385,15 +413,29 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
 
     def buffered_future_mask(self, tensor):
         dim = tensor.size(0)
+        # for onnx export
+        def triu_onnx(x, diagonal):
+            m = x.shape[0]
+            n = x.shape[1]
+            arange = torch.arange(n, device=x.device)
+            mask = arange.expand(m, n)
+            mask_maker = torch.arange(m, device=x.device).unsqueeze(-1)
+            if diagonal:
+                mask_maker = mask_maker + diagonal
+            mask = mask >= mask_maker
+            return mask * x
+
         # self._future_mask.device != tensor.device is not working in TorchScript. This is a workaround.
         if (
             self._future_mask.size(0) == 0
             or (not self._future_mask.device == tensor.device)
             or self._future_mask.size(0) < dim
         ):
-            self._future_mask = torch.triu(
-                utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1
-            )
+            # self._future_mask = torch.triu(
+            #     utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1
+            # )
+            self._future_mask = triu_onnx(utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1)
+            
         self._future_mask = self._future_mask.to(tensor)
         return self._future_mask[:dim, :dim]
 
diff --git a/fairseq/sequence_generator.py b/fairseq/sequence_generator.py
index 13f9907..2d230c5 100644
--- a/fairseq/sequence_generator.py
+++ b/fairseq/sequence_generator.py
@@ -16,6 +16,9 @@ from fairseq.data import data_utils
 from fairseq.models import FairseqIncrementalDecoder
 from fairseq.ngram_repeat_block import NGramRepeatBlock
 
+from acl_infer import AclNet, init_acl, release_acl
+import acl
+
 
 class SequenceGenerator(nn.Module):
     def __init__(
@@ -767,6 +770,28 @@ class EnsembleModel(nn.Module):
             for m in models
         ):
             self.has_incremental = True
+        
+        # acl
+        self.first_step_output = []
+        self.is_init = False
+        self.is_sentence = None
+        self.is_mask = None
+        init_acl(0)
+        output_data_shape = None
+        self.encoder_net = AclNet(
+            model_path="m2m_encoder.om",
+            device_id=0
+        )
+        self.decoder_net = AclNet(
+            model_path="m2m_decoder.om",
+            output_data_shape=output_data_shape,
+            device_id=0,
+            pin_list=[2,3]
+        )
+        self.decoder_first = AclNet(
+            model_path="m2m_decoder_first_step.om",
+            device_id=0
+        )
 
     def forward(self):
         pass
@@ -798,7 +823,32 @@ class EnsembleModel(nn.Module):
     def forward_encoder(self, net_input: Dict[str, Tensor]):
         if not self.has_encoder():
             return None
-        return [model.encoder.forward_torchscript(net_input) for model in self.models]
+        # return [model.encoder.forward_torchscript(net_input) for model in self.models]
+
+        """export encoder onnx model"""
+        # torch.onnx.export(
+        #     self.models[0].encoder,
+        #     (net_input['src_tokens']),
+        #     "m2m_encoder.onnx",
+        #     opset_version=11,
+        #     do_constant_folding=False,
+        #     verbose=False
+        # )
+
+        src_tokens = net_input['src_tokens'].numpy()
+        output, exe_time = self.encoder_net([src_tokens], first_step=True)
+
+        res = {
+            "encoder_out": [torch.from_numpy(output[0])], # T X B X C
+            "encoder_padding_mask": [torch.from_numpy(output[1])], # B X T
+            "encoder_embedding": [torch.from_numpy(output[2])], # B X T X C
+            "encoder_states": [],
+            "fc_results": [],
+            "src_tokens": [],
+            "src_lengths": [net_input['src_lengths']],
+        }
+
+        return [res]
 
     @torch.jit.export
     def forward_decoder(
@@ -808,6 +858,60 @@ class EnsembleModel(nn.Module):
         incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],
         temperature: float = 1.0,
     ):
+
+        """method for acl infer"""
+        def is_sentence_and_mask(incremental_state: Dict[str, Dict[str, Optional[Tensor]]]):
+            is_sentence_list = []
+            is_mask_list = []
+            for k,v in incremental_state_items():
+                if isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    is_mask_list.append(v['prev_key_padding_mask'])
+                    is_sentence_list.append(v['prev_key'])
+                    is_sentence_list.append(v['prev_value'])
+                
+            is_sentence = torch.cat(is_sentence_list, 0)
+            is_mask = torch.cat(is_mask_list, 0).to(torch.bool)
+
+            return is_sentence, is_mask
+        
+        def incremental_dict_to_tensor(incremental_state: Dict[str, Dict[str, Optional[Tensor]]]):
+            is_step_list = []
+            for k,v in incremental_state.items():
+                if not isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    is_step_list.append(v['prev_key'])
+                    is_step_list.append(v['[rev_value'])
+                
+            is_step = torch.cat(is_step_list, 0)
+
+            return is_step
+        
+        def incremental_tensor_to_dict(
+            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],
+            is_step: Tensor,
+        ):
+            # beam_size = 5, num_heads = 16, head_dim = 64
+            beams = 5 # batchsize *beamsize
+            i = 0
+            for k,v in incremental_state.items():
+                if not isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    v['prev_key'] = torch.cat([v['prev_key'], is_step[i*beams : (i+1)*beams, :, :, :]], 2)
+                    v['prev_value'] = torch.cat([v['prev_value'], is_step[(i+1)*beams : (i+2)*beams, :, :, :]], 2)
+                    i += 2
+            
+            return incremental_state
+
+        """export first step decoder onnx model"""
+        # torch.onnx.export(
+        #     self.models[0].decoder,
+        #     (tokens,
+        #     {'encoder_out': encoder_outs[0], 'incremental_state': incremental_states[0]}),
+        #     "m2m_decoder_first_step.onnx",
+        #     opset_version=11,
+        #     do_constant_folding=False,
+        #     verbose=True
+        # )
+        # return
+
         log_probs = []
         avg_attn: Optional[Tensor] = None
         encoder_out: Optional[Dict[str, List[Tensor]]] = None
@@ -816,11 +920,84 @@ class EnsembleModel(nn.Module):
                 encoder_out = encoder_outs[i]
             # decode each model
             if self.has_incremental_states():
-                decoder_out = model.decoder.forward(
-                    tokens,
-                    encoder_out=encoder_out,
-                    incremental_state=incremental_states[i],
-                )
+                # decoder_out = model.decoder.forward(
+                #     tokens,
+                #     encoder_out=encoder_out,
+                #     incremental_state=incremental_states[i],
+                # )
+
+                # first step
+                if not incremental_states[i]:
+                    encoder_out_tensor = encoder_out['encoder_out'][0].numpy()
+                    encoder_padding_mask = encoder_out['encoder_padding_mask'][0].numpy()
+                    self.first_step_output, exe_time = self.decoder_first([tokens.numpy(), encoder_out_tensor, encoder_padding_mask], first_step=True)
+                    self.is_init = False
+
+                    decoder_out = (
+                        torch.from_numpy(self.first_step_output[0]),
+                        {'attn': torch.from_numpy(self.first_step_output[1])}
+                    )
+                else:
+                    if not self.is_init:
+                        j = 2
+                        for k,v in incremental_states[0].items():
+                            v["prev_key"] = torch.from_numpy(self.first_step_output[j])
+                            j += 1
+                            v["prev_value"] = torch.from_numpy(self.first_step_output[j])
+                            j += 1
+                            if (j-1)% 5 == 0:
+                                v["prev_key_padding_mask"] = torch.from_numpy(self.first_step_output[j])
+                                j += 1
+                            else:
+                                v["prev_key_padding_mask"] = None
+                            
+                        self.is_init = True
+                        self.is_sentence, self.is_mask = is_sentence_and_mask(incremental_states[0])
+                        self.is_sentence = slef.is_sentence.to(torch.float16).numpy()
+                        self.is_mask = self.is_mask.numpy()
+
+                    """export decoder onnx model"""
+                    # is_step = incremental_dict_to_tensor(incremental_states[0])
+                    # is_sentence, is_mask = is_sentence_and_mask(incremental_states[0])
+                    # is_mask = is_mask.to(torch.bool)
+
+                    # torch.onnx.export(
+                    #     self.models[0].decoder,
+                    #     (tokens,
+                    #     {
+                    #         'encoder_out': encoder_outs[0],
+                    #         'incremental_state': incremental_states[0],
+                    #         'incremental_state_step': is_step,
+                    #         'incremental_state_sentence': is_sentece,
+                    #         'incremental_state_mask': is_mask,
+                    #     }),
+                    #     "m2m_decoder.onnx",
+                    #     opset_version=11,
+                    #     do_constant_folding=False,
+                    #     verbose=True
+                    # )
+                    # return
+
+                    is_step = incremental_dict_to_tensor(incremental_states[i]).to(torch.float16).numpy()
+                    if tokens.shape[1] == 2:
+                        first_step = True
+                    else:
+                        first_step = False
+                    
+                    om_input = [tokens.numpy(), is_step, self.is_sentence, self.is_mask]
+                    dims = {'dimCount': 6, 'name': '', 'dims': [5, tokens.shape[1], 120, 16, tokens.shape[1]-1, 64]}
+                    output, exe_time = self.decoder_net(om_input, dims=dims, first_step=first_step)
+
+                    incremental_states[i] = incremental_tensor_to_dict(
+                        incremental_states[i],
+                        torch.from_numpy(output[2])
+                    )
+
+                    decoder_out = (
+                        torch.from_numpy(output[0]),
+                        {'attn': torch.from_numpy(output[1])}
+                    )
+
             else:
                 if hasattr(model, "decoder"):
                     decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)
diff --git a/fairseq_cli/generate.py b/fairseq_cli/generate.py
index b875783..58bd029 100644
--- a/fairseq_cli/generate.py
+++ b/fairseq_cli/generate.py
@@ -23,7 +23,7 @@ from fairseq import checkpoint_utils, options, scoring, tasks, utils
 from fairseq.dataclass.utils import convert_namespace_to_omegaconf
 from fairseq.logging import progress_bar
 from fairseq.logging.meters import StopwatchMeter, TimeMeter
-
+import torch.nn.functional as F
 
 def main(cfg: DictConfig):
 
@@ -185,6 +185,13 @@ def _main(cfg: DictConfig, output_file):
     has_target = True
     wps_meter = TimeMeter()
     for sample in progress:
+        # preprocess
+        length = sample["net_input"]["src_tokens"].shape[1]
+        if length > 90:
+            continue
+        sample["net_input"]["src_tokens"] = F.pad(sample["net_input"]["src_tokens"], (90 - length, 0), 'constant', 1)
+
+
         sample = utils.move_to_cuda(sample) if use_cuda else sample
         if "net_input" not in sample:
             continue
@@ -376,6 +383,14 @@ def _main(cfg: DictConfig, output_file):
             num_sentences / gen_timer.sum,
             1.0 / gen_timer.avg,
         )
+    print(
+        "Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)".format(
+            num_sentences,
+            gen_timer.n,
+            gen_timer.sum,
+            num_sentences / gen_timer.sum,
+            1.0 / gen_timer.avg,
+        )
     )
     if has_target:
         if cfg.bpe and not cfg.generation.sacrebleu:
