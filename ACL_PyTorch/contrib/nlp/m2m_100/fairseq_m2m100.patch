diff --git a/fairseq/models/transformer/transformer_decoder.py b/fairseq/models/transformer/transformer_decoder.py
index 1a0f978b..e1997b06 100644
--- a/fairseq/models/transformer/transformer_decoder.py
+++ b/fairseq/models/transformer/transformer_decoder.py
@@ -188,6 +188,9 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         prev_output_tokens,
         encoder_out: Optional[Dict[str, List[Tensor]]] = None,
         incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,
+        incremental_state_step: Tensor = None,
+        incremental_state_sentence: Tensor = None,
+        incremental_state_mask: Tensor = None,
         features_only: bool = False,
         full_context_alignment: bool = False,
         alignment_layer: Optional[int] = None,
@@ -214,6 +217,23 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
                 - a dictionary with any model-specific outputs
         """
 
+        # modification for onnx export
+        # beam_size = 5, num_heads = 16, head_dim = 64
+        if torch.onnx.is_in_onnx_export() and incremental_state:
+            beams = 5 # batchsize * beamsize
+            i = 0
+            j = 0
+            for k,v in incremental_state.items():
+                if isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    v['prev_key_padding_mask'] = incremental_state_mask[i//2 * beams : (i//2+1) * beams, :]
+                    v['prev_key'] = incremental_state_sentence[i*beams : (i+1)*beams, :, :, :]
+                    v['prev_value'] = incremental_state_sentence[(i+1)*beams : (i+2)*beams, :, :, :]
+                    i += 2
+                else:
+                    v['prev_key'] = incremental_state_step[j*beams : (j+1)*beams, :, :, :]
+                    v['prev_value'] = incremental_state_step[(j+1)*beams : (j+2)*beams, :, :, :]
+                    j += 2
+
         x, extra = self.extract_features(
             prev_output_tokens,
             encoder_out=encoder_out,
@@ -367,7 +387,23 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
         if self.project_out_dim is not None:
             x = self.project_out_dim(x)
 
-        return x, {"attn": [attn], "inner_states": inner_states}
+        is_step_list = []
+        is_step_list2 = []
+        prev_key_padding_mask_list = []
+        for k,v in incremental_state.items():
+            if not isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                is_step_list.append(v['prev_key'][:, :, -1:, :])
+                is_step_list.append(v['prev_value'][:, :, -1:, :])
+            else:
+                is_step_list2.append(v['prev_key'][:, :, :, :])
+                is_step_list2.append(v['prev_value'][:, :, :, :])
+                prev_key_padding_mask_list.append(v['prev_key_padding_mask'])
+
+        is_step = torch.cat(is_step_list, 0)
+        is_step2 = torch.cat(is_step_list2, 0)
+        prev_key_padding_mask = torch.cat(prev_key_padding_mask_list, 0)
+
+        return x, {"attn": [attn], "is_step": is_step, "is_step2": is_step2, "prev_key_padding_mask": prev_key_padding_mask}
 
     def output_layer(self, features):
         """Project features to the vocabulary size."""
@@ -385,13 +421,28 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
 
     def buffered_future_mask(self, tensor):
         dim = tensor.size(0)
+        # for onnx export
+        def triu_onnx(x, diagonal):
+            m = x.shape[0]
+            n = x.shape[1]
+            arange = torch.arange(n, device=x.device)
+            mask = arange.expand(m, n)
+            mask_maker = torch.arange(m, device=x.device).unsqueeze(-1)
+            if diagonal:
+                mask_maker = mask_maker + diagonal
+            mask = mask >= mask_maker
+            return mask * x
+
         # self._future_mask.device != tensor.device is not working in TorchScript. This is a workaround.
         if (
             self._future_mask.size(0) == 0
             or (not self._future_mask.device == tensor.device)
             or self._future_mask.size(0) < dim
         ):
-            self._future_mask = torch.triu(
+            # self._future_mask = torch.triu(
+            #     utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1
+            #  )
+            self._future_mask = triu_onnx(
                 utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1
             )
         self._future_mask = self._future_mask.to(tensor)
diff --git a/fairseq/models/transformer/transformer_encoder.py b/fairseq/models/transformer/transformer_encoder.py
index c887c5af..e1aed3b7 100644
--- a/fairseq/models/transformer/transformer_encoder.py
+++ b/fairseq/models/transformer/transformer_encoder.py
@@ -320,7 +320,7 @@ class TransformerEncoderBase(FairseqEncoder):
         # The empty list is equivalent to None.
         src_lengths = (
             src_tokens.ne(self.padding_idx)
-            .sum(dim=1, dtype=torch.int32)
+            .sum(dim=1)
             .reshape(-1, 1)
             .contiguous()
         )
diff --git a/fairseq/sequence_generator.py b/fairseq/sequence_generator.py
index 13f99078..242b6503 100644
--- a/fairseq/sequence_generator.py
+++ b/fairseq/sequence_generator.py
@@ -3,6 +3,7 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
+import os
 import math
 import sys
 from typing import Dict, List, Optional
@@ -15,6 +16,7 @@ from fairseq import search, utils
 from fairseq.data import data_utils
 from fairseq.models import FairseqIncrementalDecoder
 from fairseq.ngram_repeat_block import NGramRepeatBlock
+from ais_bench.infer.interface import InferSession
 
 
 class SequenceGenerator(nn.Module):
@@ -768,6 +770,18 @@ class EnsembleModel(nn.Module):
         ):
             self.has_incremental = True
 
+        self.export_onnx_mode = os.getenv('EXPORT_ONNX_MODE')
+        # acl
+        if self.export_onnx_mode is None:
+            self.first_step_output = []
+            self.is_init = False
+            self.is_sentence = None
+            self.is_mask = None
+            output_data_shape = None
+            self.encoder_net = InferSession(0, os.getenv('ENCODER_OM')).infer
+            self.decoder_first = InferSession(0, os.getenv('DECODER_FIRST_OM')).infer
+            self.decoder_net = InferSession(0, os.getenv('DECODER_OM')).infer
+
     def forward(self):
         pass
 
@@ -798,7 +812,36 @@ class EnsembleModel(nn.Module):
     def forward_encoder(self, net_input: Dict[str, Tensor]):
         if not self.has_encoder():
             return None
-        return [model.encoder.forward_torchscript(net_input) for model in self.models]
+
+        # export encoder onnx model
+        if self.export_onnx_mode == "encoder":
+            torch.onnx.export(
+                self.models[0].encoder,
+                (net_input['src_tokens']),
+                "m2m_encoder.onnx",
+                opset_version=11,
+                do_constant_folding=False,
+                verbose=False,
+            )
+            return
+        if self.export_onnx_mode in ["first_step_decoder", "decoder"]:
+            out = [model.encoder.forward_torchscript(net_input) for model in self.models]
+            return out
+
+        src_tokens = net_input['src_tokens'].numpy()
+        output = self.encoder_net([src_tokens])
+
+        res = {
+            "encoder_out": [torch.from_numpy(output[0])], # T X B X C
+            "encoder_padding_mask": [torch.from_numpy(output[1])], # B X T
+            "encoder_embedding": [torch.from_numpy(output[2])], # B X T X C
+            "encoder_states": [],
+            "fc_results": [],
+            "src_tokens": [],
+            "src_lengths": [net_input['src_lengths']],
+        }
+
+        return [res]
 
     @torch.jit.export
     def forward_decoder(
@@ -808,6 +851,65 @@ class EnsembleModel(nn.Module):
         incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]],
         temperature: float = 1.0,
     ):
+        # method for acl infer
+        def is_sentence_and_mask(incremental_state: Dict[str, Dict[str, Optional[Tensor]]]):
+            is_sentence_list = []
+            is_mask_list = []
+            for k,v in incremental_state.items():
+                if isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    is_mask_list.append(v['prev_key_padding_mask'])
+                    is_sentence_list.append(v['prev_key'])
+                    is_sentence_list.append(v['prev_value'])
+
+            is_sentence = torch.cat(is_sentence_list, 0)
+            is_mask = torch.cat(is_mask_list, 0).to(torch.bool)
+
+            return is_sentence, is_mask
+
+        def incremental_dict_to_tensor(incremental_state: Dict[str, Dict[str, Optional[Tensor]]]):
+            is_step_list = []
+            for k,v in incremental_state.items():
+                if not isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    is_step_list.append(v['prev_key'])
+                    is_step_list.append(v['prev_value'])
+
+            is_step = torch.cat(is_step_list, 0)
+
+            return is_step
+
+        def incremental_tensor_to_dict(
+            incremental_state: Dict[str, Dict[str, Optional[Tensor]]],
+            is_step: Tensor,
+        ):
+            # beam_size = 5, num_heads = 16, head_dim = 64
+            beams = 5 # batchsize * beamsize
+            i = 0
+            for k,v in incremental_state.items():
+                if not isinstance(v['prev_key_padding_mask'], torch.Tensor):
+                    v['prev_key'] = torch.cat(
+                        [v['prev_key'], is_step[i*beams : (i+1)*beams, :, :, :]], 2
+                    )
+                    v['prev_value'] = torch.cat(
+                        [v['prev_value'], is_step[(i+1)*beams : (i+2)*beams, :, :, :]], 2
+                    )
+                    i += 2
+            return incremental_state
+
+        # export first step decoder onnx model
+        if self.export_onnx_mode == "first_step_decoder":
+            torch.onnx.export(
+                self.models[0].decoder,
+                (tokens,
+                {'encoder_out': encoder_outs[0], 'incremental_state': incremental_states[0]}),
+                output_names=[
+                    "logits", "attention", "prev_key_values", "prev_key_values2", "prev_key_padding_mask"],
+                f="m2m_decoder_first_step.onnx",
+                opset_version=11,
+                do_constant_folding=False,
+                verbose=False,
+            )
+            return
+
         log_probs = []
         avg_attn: Optional[Tensor] = None
         encoder_out: Optional[Dict[str, List[Tensor]]] = None
@@ -816,11 +918,98 @@ class EnsembleModel(nn.Module):
                 encoder_out = encoder_outs[i]
             # decode each model
             if self.has_incremental_states():
-                decoder_out = model.decoder.forward(
-                    tokens,
-                    encoder_out=encoder_out,
-                    incremental_state=incremental_states[i],
-                )
+                # export decoder onnx model
+                if self.export_onnx_mode == "decoder":
+                    decoder_out = model.decoder.forward(
+                        tokens,
+                        encoder_out=encoder_out,
+                        incremental_state=incremental_states[i],
+                    )
+                    is_step = incremental_dict_to_tensor(incremental_states[0])
+                    is_sentence, is_mask = is_sentence_and_mask(incremental_states[0])
+                    is_mask = is_mask.to(torch.bool)
+                    torch.onnx.export(
+                        self.models[0].decoder,
+                        (tokens,
+                        {
+                            'encoder_out': encoder_outs[0],
+                            'incremental_state': incremental_states[0],
+                            'incremental_state_step': is_step,
+                            'incremental_state_sentence': is_sentence,
+                            'incremental_state_mask': is_mask,
+                        }),
+                        "m2m_decoder.onnx",
+                        opset_version=11,
+                        do_constant_folding=False,
+                        verbose=False,
+                    )
+                    return
+
+                # first step
+                if not incremental_states[i]:
+                    encoder_out_tensor = encoder_out['encoder_out'][0].numpy()
+                    encoder_padding_mask = encoder_out['encoder_padding_mask'][0].numpy()
+                    self.first_step_output = self.decoder_first(
+                        [tokens.numpy(), encoder_out_tensor, encoder_padding_mask])
+                    self.is_init = False
+
+                    decoder_out = (
+                        torch.from_numpy(self.first_step_output[0]),
+                        {'attn': torch.from_numpy(self.first_step_output[1])}
+                    )
+                else:
+                    if not self.is_init:
+                        m = 0
+                        n = 0
+                        a = 0
+                        beams = 5
+                        prev_key_values = self.first_step_output[2]
+                        prev_key_values2 = self.first_step_output[3]
+                        prev_key_padding_mask = self.first_step_output[4]
+                        prev_key_values = prev_key_values.reshape(
+                            (-1, beams, prev_key_values.shape[1], prev_key_values.shape[2], prev_key_values.shape[3]))
+                        prev_key_values2 = prev_key_values2.reshape(
+                            (-1, beams, prev_key_values2.shape[1], prev_key_values2.shape[2], prev_key_values2.shape[3]))
+                        prev_key_padding_mask = prev_key_padding_mask.reshape((-1, beams, prev_key_padding_mask.shape[1]))
+
+                        for k,v in incremental_states[0].items():
+                            if a % 2 == 0:
+                                v["prev_key"] = torch.from_numpy(prev_key_values[m])
+                                v["prev_value"] = torch.from_numpy(prev_key_values[m + 1])
+                                v["prev_key_padding_mask"] = None
+                                m += 2
+                            else:
+                                v["prev_key"] = torch.from_numpy(prev_key_values2[n])
+                                v["prev_value"] = torch.from_numpy(prev_key_values2[n + 1])
+                                v["prev_key_padding_mask"] = torch.from_numpy(prev_key_padding_mask[(a - 1) // 2])
+                                n += 2
+                            a += 1
+
+                        self.is_init = True
+                        self.is_sentence, self.is_mask = is_sentence_and_mask(incremental_states[0])
+                        self.is_sentence = self.is_sentence.to(torch.float16).numpy()
+                        self.is_mask = self.is_mask.numpy()
+
+                    is_step = incremental_dict_to_tensor(incremental_states[i]).to(torch.float16).numpy()
+                    if tokens.shape[1] == 2:
+                        first_step = True
+                    else:
+                        first_step = False
+
+                    om_input = [tokens.numpy(), is_step, self.is_sentence, self.is_mask]
+                    dims = {'dimCount': 6, 'name': '', 'dims': [5, tokens.shape[1], 120, 16, tokens.shape[1]-1, 64]}
+                    output = self.decoder_net(om_input, 'dymdims')
+
+                    incremental_states[i] = incremental_tensor_to_dict(
+                        incremental_states[i],
+                        torch.from_numpy(output[2])
+                    )
+
+                    decoder_out = (
+                        torch.from_numpy(output[0]),
+                        {'attn': torch.from_numpy(output[1])}
+                    )
+
             else:
                 if hasattr(model, "decoder"):
                     decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)
diff --git a/fairseq_cli/generate.py b/fairseq_cli/generate.py
index b8757835..1b0f7f27 100644
--- a/fairseq_cli/generate.py
+++ b/fairseq_cli/generate.py
@@ -17,6 +17,7 @@ from itertools import chain
 
 import numpy as np
 import torch
+import torch.nn.functional as F
 from omegaconf import DictConfig
 
 from fairseq import checkpoint_utils, options, scoring, tasks, utils
@@ -185,6 +186,12 @@ def _main(cfg: DictConfig, output_file):
     has_target = True
     wps_meter = TimeMeter()
     for sample in progress:
+        # preprocess
+        length = sample["net_input"]["src_tokens"].shape[1]
+        if length > 90:
+            continue
+        sample["net_input"]["src_tokens"] = F.pad(sample["net_input"]["src_tokens"], (90 - length, 0), 'constant', 1)
+
         sample = utils.move_to_cuda(sample) if use_cuda else sample
         if "net_input" not in sample:
             continue
@@ -377,6 +384,15 @@ def _main(cfg: DictConfig, output_file):
             1.0 / gen_timer.avg,
         )
     )
+    print(
+        "Translated {:,} sentences ({:,} tokens) in {:.1f}s ({:.2f} sentences/s, {:.2f} tokens/s)".format(
+            num_sentences,
+            gen_timer.n,
+            gen_timer.sum,
+            num_sentences / gen_timer.sum,
+            1.0 / gen_timer.avg,
+        )
+    )
     if has_target:
         if cfg.bpe and not cfg.generation.sacrebleu:
             if cfg.common_eval.post_process:
