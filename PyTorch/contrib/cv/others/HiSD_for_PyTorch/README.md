# HiSD

Official pytorch implementation of paper "[Image-to-image Translation via Hierarchical Style Disentanglement](https://arxiv.org/abs/2103.01456)".


HiSD is the SOTA image-to-image translation method for both ****Scalability**** for multiple labels and **Controllable Diversity** with impressive disentanglement.

The styles to manipolate each tag in our method can be not only generated by random noise but also extracted from images!

#### 1. 下载与准备数据集
We recommend you to download CelebA-HQ from [CelebAMask-HQ](https://github.com/switchablenorms/CelebAMask-HQ).
Anyway you shound get the dataset folder like:

````
dataset
 - CelebA_img
   - 0.jpg
   - 1.jpg
   - 2.jpg
   - ...
 - CelebAMask_drop
   - CelebAMask-HQ-attribute-anno.txt
````

 #### 2. 数据集处理与划分
`CelebA_data_path`和`CelebA_laebl_path`请都使用绝对路径。

 ````shell
bash test/prepare_data.sh --CelebA_data_path=xxx/dataset/CelebA_img --CelebA_laebl_path=xxx/dataset/CelebAMask_drop/CelebAMask-HQ-attribute-anno.txt
 ````
In our paper, we use fisrt 3000 as test set and remaining 27000 for training.
Then you will get several ".txt" files in the "datasets/" and "datasets_test/" dir , each of them consists of lines of the absolute path of image and its tag-irrelevant conditions (Age and Gender by default).
And the script also will generate 3000 test images. The File structure is as following:

````
 HiSD-[root dir]
 - datasets
   - HairColor_black.txt
   - Eyeglasses_without.txt
   - ...
 - datasets_test
   - HairColor_black.txt
   - Eyeglasses_without.txt
   - ...
 - test_imgs
   -381.jpg
   -385.jpg
   - ...
````


### 3. 训练
- For 1p performance：

 ````shell
bash test/train_performance_1p.sh
 ````

- For 1p full：

 ````shell
bash test/train_full_1p.sh
 ````
The checkpoints are in the "outputs/" dir.

### 4. 训练结果展示

**表 2**  训练结果展示表

|  NAME  |  FPS  | Performance(FID) | AMP_Type |
| :----: | :---: | :--------------: | :------: |
| 1p-GPU | 13.38 |      81.69       |    O1    |
| 1p-NPU | 13.3  |      85.20       |    O1    |

# 公网地址说明

代码涉及公网地址参考 public_address_statement.md