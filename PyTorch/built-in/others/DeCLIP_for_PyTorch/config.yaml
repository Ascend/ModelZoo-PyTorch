#protobuf:                                                                                        #
model:
    type: declip_vitb32
    kwargs:
        image_encode:
            embed_dim: 512
        text_encode:
            bpe_path: './dataset/bpe_simple_vocab_16e6.txt.gz'
            text_encode_type: Transformer #Transformer,Bert,GPT2
            text_model_utils:
                random: False
                freeze: False
            embed_dim: 512
        clip:
            use_allgather: True
            text_mask_type: MLM
            return_nn_bank: True
            feature_dim: 512

dist:
    sync: False

grad_clip:
    type: logit_scale_param_value
    value: 3
    max_value: 6

clip_simsiam_loss_weight:
    clip_loss: 0.4
    nn_text: 0.2
    simsiam_loss: 0.2
    masking_language: 0.2

optimizer:
    type: AdamW
    kwargs:
        lr: 0.0001  # 5e-4
        weight_decay: 0.1
        betas: [0.9, 0.98]
        amsgrad: False
        eps: 0.00000001
    #fp16_normal_bn: True
    fp16_normal_bn: True
    fp16_normal_ln: True
    pconfig:
        bn_w:
            weight_decay: 0
        bn_b:
            weight_decay: 0
        ln_w:
            weight_decay: 0
        ln_b:
            weight_decay: 0
        bias:
            weight_decay: 0
        logit_scale:
            # lr: 0.0001  # not useful
            weight_decay: 0


lr_scheduler:
    type: Cosine
    kwargs:
        base_lr: 0.0001
        warmup_lr: 0.001  # 3e-3
        min_lr: 0.0
        warmup_steps: 2500
        max_iter: 128001



label_smooth: 0.0
ema:
    enable: False
    kwargs:
        decay: 0.999
data:
    type: clip
    read_from: fs
#read_from: fs
    use_dali: False
    batch_size: 128
    num_workers: 20
    pin_memory: True
    input_size: 224
    test_resize: 256

    train:
        root_dir: [
                    './dataset/yfcc15m_clean_open_data/',
                   ]
        meta_file: [
                   './dataset/yfcc15m_clean_open_data.json',
                    ]
        image_reader:
            type: pil
        sampler:
            type: distributed_iteration
        transforms:
            type: MOCOV2
        image_text_two_view: True
        fseek: True
        use_ranked: False

    test:
      - type: clip
        read_from: fs
        use_dali: True
        batch_size: 128
        num_workers: 4
        pin_memory: False
        input_size: 224
        test_resize: 256
        test:
            root_dir: './dataset/imagenet_val/'
            meta_file: './dataset/val_official.json'
            # you can change it to imagenet_info relative path, file already in gitlab
            image_reader:
                type: pil
            sampler:
                type: distributed
            transforms:
                type: ONECROP
            evaluator:
                type: imagenet
                kwargs:
                    topk: [1, 5]
            #label_texts_ensemble: 'simple'
            label_texts_ensemble: 'prompt80'

saver:
    print_freq: 1
    val_freq: 2000
    save_freq: 1000
    save_many: False
    pretrain:
        auto_resume: True
