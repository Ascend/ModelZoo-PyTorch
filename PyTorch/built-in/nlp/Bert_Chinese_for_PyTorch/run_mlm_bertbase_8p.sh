source env.sh
python3 -m torch.distributed.launch --nproc_per_node 8 run_mlm.py \
        --model_type bert \
        --config_name ./bert-base-chinese/config.json \
        --tokenizer_name ./bert-base-chinese \
        --max_seq_length 512 \
        --train_file ./train_huawei.txt \
        --eval_metric_path ./accuracy.py \
        --line_by_line \
        --pad_to_max_length \
        --remove_unused_columns false \
        --save_steps 5000 \
        --num_train_epochs 3 \
        --overwrite_output_dir \
        --per_device_train_batch_size 32 \
        --per_device_eval_batch_size 32 \
        --do_train \
        --do_eval \
        --eval_accumulation_steps 100 \
        --fp16 \
        --fp16_opt_level O2 \
        --loss_scale 8192 \
        --use_combine_grad \
        --optim adamw_apex_fused_npu \
        --distributed_process_group_timeout 5400 \
        --output_dir ./output