diff --git a/yolox/evaluators/coco_evaluator.py b/yolox/evaluators/coco_evaluator.py
index 96eb56a..64827fc 100644
--- a/yolox/evaluators/coco_evaluator.py
+++ b/yolox/evaluators/coco_evaluator.py
@@ -164,33 +164,34 @@ class COCOEvaluator:
                 data_list.append(pred_data)
         return data_list
 
-    def evaluate_prediction(self, data_dict, statistics):
+    def evaluate_prediction(self, data_dict, statistics=None):
         if not is_main_process():
             return 0, 0, None
 
         logger.info("Evaluate in main process...")
 
         annType = ["segm", "bbox", "keypoints"]
+        
+        if statistics is not None:
+            inference_time = statistics[0].item()
+            nms_time = statistics[1].item()
+            n_samples = statistics[2].item()
+
+            a_infer_time = 1000 * inference_time / (n_samples * self.dataloader.batch_size)
+            a_nms_time = 1000 * nms_time / (n_samples * self.dataloader.batch_size)
+    
+            time_info = ", ".join(
+                [
+                    "Average {} time: {:.2f} ms".format(k, v)
+                    for k, v in zip(
+                        ["forward", "NMS", "inference"],
+                        [a_infer_time, a_nms_time, (a_infer_time + a_nms_time)],
+                    )
+                ]
+            )
 
-        inference_time = statistics[0].item()
-        nms_time = statistics[1].item()
-        n_samples = statistics[2].item()
-
-        a_infer_time = 1000 * inference_time / (n_samples * self.dataloader.batch_size)
-        a_nms_time = 1000 * nms_time / (n_samples * self.dataloader.batch_size)
-
-        time_info = ", ".join(
-            [
-                "Average {} time: {:.2f} ms".format(k, v)
-                for k, v in zip(
-                    ["forward", "NMS", "inference"],
-                    [a_infer_time, a_nms_time, (a_infer_time + a_nms_time)],
-                )
-            ]
-        )
-
-        info = time_info + "\n"
-
+            info = time_info + "\n"
+        info = "\n"
         # Evaluate the Dt (detection) json comparing with the ground truth
         if len(data_dict) > 0:
             cocoGt = self.dataloader.dataset.coco
@@ -216,6 +217,6 @@ class COCOEvaluator:
             with contextlib.redirect_stdout(redirect_string):
                 cocoEval.summarize()
             info += redirect_string.getvalue()
-            return cocoEval.stats[0], cocoEval.stats[1], info
+            return info
         else:
             return 0, 0, info
