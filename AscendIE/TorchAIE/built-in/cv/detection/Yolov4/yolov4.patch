diff -Nur a/pytorch-YOLOv4/demo_pytorch2onnx.py b/pytorch-YOLOv4/demo_pytorch2onnx.py
--- a/pytorch-YOLOv4/demo_pytorch2onnx.py	2023-01-17 03:01:42.415330050 +0000
+++ b/pytorch-YOLOv4/demo_pytorch2onnx.py	2023-01-17 03:04:42.907332261 +0000
@@ -13,14 +13,14 @@
 
 
 def transform_to_onnx(weight_file, batch_size, n_classes, IN_IMAGE_H, IN_IMAGE_W):
-    
-    model = Yolov4(n_classes=n_classes, inference=True)
 
-    pretrained_dict = torch.load(weight_file, map_location=torch.device('cuda'))
+    model = Yolov4(n_classes=n_classes, inference=False)
+
+    pretrained_dict = torch.load(weight_file, map_location=torch.device('cpu'))
     model.load_state_dict(pretrained_dict)
 
     input_names = ["input"]
-    output_names = ['boxes', 'confs']
+    output_names = ['feature_map_1', 'feature_map_2', 'feature_map_3']
 
     dynamic = False
     if batch_size <= 0:
@@ -29,7 +29,10 @@
     if dynamic:
         x = torch.randn((1, 3, IN_IMAGE_H, IN_IMAGE_W), requires_grad=True)
         onnx_file_name = "yolov4_-1_3_{}_{}_dynamic.onnx".format(IN_IMAGE_H, IN_IMAGE_W)
-        dynamic_axes = {"input": {0: "batch_size"}, "boxes": {0: "batch_size"}, "confs": {0: "batch_size"}}
+        dynamic_axes = {"input": {0: "batch_size"},
+                        "feature_map_1": {0: "batch_size"},
+                        "feature_map_2": {0: "batch_size"},
+                        "feature_map_3": {0: "batch_size"}}
         # Export the model
         print('Export the onnx model ...')
         torch.onnx.export(model,
@@ -60,7 +63,7 @@
 
         print('Onnx model exporting done')
         return onnx_file_name
-    
+
 
 
 def main(weight_file, image_path, batch_size, n_classes, IN_IMAGE_H, IN_IMAGE_W):
@@ -73,19 +76,19 @@
         # Transform to onnx for demo
         onnx_path_demo = transform_to_onnx(weight_file, 1, n_classes, IN_IMAGE_H, IN_IMAGE_W)
 
-    session = onnxruntime.InferenceSession(onnx_path_demo)
-    # session = onnx.load(onnx_path)
-    print("The model expects input shape: ", session.get_inputs()[0].shape)
+    # session = onnxruntime.InferenceSession(onnx_path_demo)
+    # # session = onnx.load(onnx_path)
+    # print("The model expects input shape: ", session.get_inputs()[0].shape)
 
-    image_src = cv2.imread(image_path)
-    detect(session, image_src)
+    # image_src = cv2.imread(image_path)
+    # detect(session, image_src)
 
 
 
 if __name__ == '__main__':
     print("Converting to onnx and running demo ...")
     if len(sys.argv) == 7:
-        
+
         weight_file = sys.argv[1]
         image_path = sys.argv[2]
         batch_size = int(sys.argv[3])
diff -Nur a/pytorch-YOLOv4/models.py b/pytorch-YOLOv4/models.py
--- a/pytorch-YOLOv4/models.py	2023-01-17 03:01:42.415330050 +0000
+++ b/pytorch-YOLOv4/models.py	2023-01-17 03:04:24.191332032 +0000
@@ -419,10 +419,10 @@
         self.down4 = DownSample4()
         self.down5 = DownSample5()
         # neck
-        self.neck = Neck(inference)
+        self.neek = Neck(inference)
         # yolov4conv137
         if yolov4conv137weight:
-            _model = nn.Sequential(self.down1, self.down2, self.down3, self.down4, self.down5, self.neck)
+            _model = nn.Sequential(self.down1, self.down2, self.down3, self.down4, self.down5, self.neek)
             pretrained_dict = torch.load(yolov4conv137weight)
 
             model_dict = _model.state_dict()
@@ -443,7 +443,7 @@
         d4 = self.down4(d3)
         d5 = self.down5(d4)
 
-        x20, x13, x6 = self.neck(d5, d4, d3)
+        x20, x13, x6 = self.neek(d5, d4, d3)
 
         output = self.head(x20, x13, x6)
         return output
